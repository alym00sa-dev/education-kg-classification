finding_id:ID(EmpiricalFinding),direction,results_summary,measure,study_size:int,effect_size:float
finding_1,,"This study analyzed policies, guidelines, and resources from the top 100 U.S. universities regarding GenAI use. The majority of universities adopt an open but cautious approach, with 54.8% allowing instructors to decide on GenAI use in their courses. Primary concerns identified include ethical usage, accuracy, and data privacy. No universities completely banned GenAI tools.",,104,
finding_2,,"This is a commentary piece that does not present original empirical findings. The author argues that the academic AIED research community has long addressed pedagogy, cognition, and increasingly ethics and human rights, contrary to criticisms that conflate commercial AI in education applications with academic research.",,,
finding_3,Positive,"The Deliberative Democracy process facilitated a unique and structured co-production process that enabled diverse higher education stakeholders to integrate their situated knowledge through dialogue. Participants reported increased trust, commitment, and a sense of pride in co-producing ethical principles for AI-enabled educational technology, which subsequently informed a new university AI governance policy.","Semi-structured interviews assessing participant perceptions of the deliberative democracy process, engagement, trust, and satisfaction with co-produced principles",13,
finding_4,Negative,"The study found that ChatGPT 3.5 and Google Bard were unable to reliably provide accurate feedback on concurrent programming exercises. All experiments resulted in approximately 50% accuracy, with both tools showing high false positive rates when identifying synchronization errors such as deadlocks, race conditions, and starvation.","Accuracy, precision, recall, and F1-score comparing LLM-generated feedback against expert instructor ground-truth evaluations for detecting concurrency errors",52,
finding_5,Positive,"The systematic review synthesized 93 studies on text-based automated assessment systems in post-secondary education, identifying five main system types. Findings indicate that automatic grading systems were most common, and that these AI/NLP-based tools show promise for supporting assessment, feedback, and deeper learning, though most studies emphasized model performance metrics over direct learning impact.","Model performance metrics (accuracy, reliability, correlation with human scores), learning outcomes (pre-post test scores, recall)",93,
finding_6,Positive,"GPT-3 generated reading passages were evaluated by human expert judges and found to be comparable to original PIRLS passages in terms of adequacy, coherence, engagement, and readability for fourth graders. For some passages, the AI-generated content was rated as more adequate and engaging than the original human-written passages.","Human expert ratings on 4-point Likert scale assessing coherence, appropriateness for fourth graders, readability, engagement, and distraction",150,
finding_7,,"This is a commentary/manifesto paper that does not present empirical findings. It argues for the AIED community to take a more proactive role in responsible AI discussions, expand focus beyond mainstream education to include diverse learners, and engage with EdTech industry and policy.",,,
finding_8,Mixed,"This systematic literature review analyzed 107 papers on AI in education ethics from 2011-2022, finding that research interest has grown significantly since 2018, particularly after the COVID-19 pandemic. The review identified key ethical concerns including fairness, inclusion, autonomy, and agency, while noting significant gaps in addressing cultural differences, emotional monitoring, and capacity building for ethical AIED implementation.","Thematic analysis of published literature on AIED ethics across multiple dimensions including technologies, cultural considerations, emotion monitoring, capacity building, and regulatory frameworks",107,
finding_9,Positive,"The study found that combining machine learning-based clustering and automated scoring reduced the overall workload for grading short answer exams by 64% to 74% compared to fully manual grading. Clustering alone reduced grading time by 69%, and adding score suggestions further reduced time by 73%, while still allowing complete manual review of all classifier outputs to ensure fair grading.","Time spent on grading (seconds per answer), percentage of workload reduction, classifier accuracy",438,
finding_10,,,,,
finding_11,Positive,"AI-formed teams showed significantly higher relational well-being (Cohen's d = 0.55, p = 0.016) and social network growth (d = 0.47, p = 0.037) compared to randomly formed control groups. Participants in AI-formed teams reported feeling they could freely express themselves and showed significant improvement in activism and media creation skills.","Post-activity questionnaire measuring relational well-being, social network growth, team experience, and project skill development using Likert scales; video quality scores evaluated by committee",97,0.55
finding_12,Positive,"GPT-4 produced grades that were not statistically different from instructor grades when provided with both a model answer and a rubric. The LLM achieved an Intraclass Correlation Coefficient of 0.92, indicating excellent reliability that matched or exceeded peer grading reliability. LLM-generated rubrics performed comparably to instructor-provided rubrics for automated grading.","Grade agreement between LLM and instructor grades, Intraclass Correlation Coefficient (ICC), root mean square differences, Friedman test with post-hoc Conover's test",120,
finding_13,Positive,"The study found that students exhibited higher social presence through increased social and affective language during remote instruction in Spring 2020 compared to Fall 2019 and Winter 2020. Emergent COVID-19 related discourse involved sharing personal experiences with positive sentiments and expressing opinions on contemporary events, with students showing rapport and empathy towards peers.","LIWC linguistic variables (social processes, affective processes, cognitive processes, analytic thinking) and VADER sentiment scores",1441,0.13
finding_14,Mixed,"This systematic review of 257 articles on video-based learning (2016-2021) found that textual and visual features were the most explored characteristics, primarily used in recommendation systems and navigation tools. Interactive features like quizzes showed positive effects on learning outcomes, though results were not always statistically significant. The impact of video characteristics on learning effectiveness varied considerably across studies, with contradictory findings regarding production styles, instructor presence, and video pacing.","Learning outcome, cognitive load, satisfaction, mental effort, social presence, motivation, self-efficacy",257,
finding_15,Mixed,"The study found that emotional synchrony appeared most often in socially shared regulation of learning (SSRL) segments compared to other regulatory activities. However, the close mean values for emotional synchrony frequency among SSRL, Cognitive, and Task Execution interactions suggest the metric is not sufficient to clearly differentiate among these interactions, with large standard deviations complicating interpretation.","Emotional synchrony frequency (co-occurrence of shared emotions among group members) detected via AI facial expression recognition, aligned with qualitative video coding of regulatory activities",36,
finding_16,Mixed,"The study found limited use of ChatGPT by undergraduate students due to limited expertise and ethical concerns. The use of ChatGPT showed a negative association with the amount of literal source-text information included in students' written products, while the level of integration of conflicting information was not associated with ChatGPT interaction.","Argument-counterargument integration scores, matching percentage with source text, matching percentage with ChatGPT output, number of information categories transferred from source",27,
finding_17,,"This paper is a conceptual and reflective article rather than an empirical study. It discusses the inherent complexity and challenges in developing adaptive learning environments, focusing on modeling techniques for student, domain, and pedagogical models. The author draws on personal experience to outline trade-offs and nuances without presenting new empirical data or experimental results.",,,
finding_18,Positive,"Students in the adaptive condition using Bayesian Knowledge Tracing achieved significantly higher post-test scores (69%) compared to the randomized condition (54%) on scientific principles of stability and balance. The adaptive system led to better mastery of scientific concepts without sacrificing enjoyment, with over 80% of student groups reaching mastery in the adaptive condition versus only 16.7% in the random condition.","Paper pre/post-tests measuring prediction and explanation of scientific principles (height, base-width, symmetry, center of mass)",36,0.66
finding_19,Positive,"The CatBoost classifier achieved an AUC of 0.774 and average precision of 0.847 in predicting student dropout using pre-enrollment achievement measures. The study demonstrated that explainable AI tools (SHAP, LIME, PDP) can effectively identify at-risk students and provide interpretable explanations for personalized intervention planning.",AUC (Area Under ROC Curve) and Average Precision for dropout prediction accuracy,8508,
finding_20,No Effect,"The study found that conversational tutoring with high levels of learner control showed trends toward higher perceived ease-of-use and learning enjoyment compared to static tutoring when comparing means. However, after Bonferroni correction for multiple comparisons, no statistically significant differences were found in learning outcomes or learning experience measures between the treatment groups and control group.","Essay-quality assessment ability (pre-post test comparing student ratings to expert GRE ratings), perceived ease-of-use, learning enjoyment, and learner autonomy",96,
finding_21,,"This is a commentary/perspective paper that does not present original empirical findings. It synthesizes existing literature to argue that social-psychological and contextual factors fundamentally shape how educators perceive, trust, and use AI-powered education technologies, and calls for more research on these human factors.",,,
finding_22,Positive,"The Longformer-based large language models achieved strong predictive accuracy for automated summary scoring, explaining 82% of variance for content scores and 70% for wording scores when compared to human rater judgments. Post-hoc validation on professional summaries showed the models effectively discriminated between matched and unmatched source-summary pairs with large effect sizes (d = 1.56 for content, d = 0.938 for wording).",Correlation and explained variance (R²) between model-predicted scores and human rater scores on summary quality rubric,4690,
finding_23,Positive,"ChatGPT (GPT-3.5) achieved a mean grade of 7.3 on Dutch high school English reading comprehension exams, comparable to the student average of 6.99. GPT-4 significantly outperformed students with a mean grade of 8.3, placing it at approximately the 97th percentile of student performance.",Exam grades on a 1-10 scale based on official Dutch national exam scoring norms,,
finding_24,Mixed,"The systematic review found that most studies reported no strict tradeoff between fairness and accuracy in education algorithms. Common bias mitigation strategies included class balancing techniques, adversarial learning, and fairness through awareness/unawareness. The review identified that fairness metrics like ABROCA, group difference in performance, and disparity metrics are commonly used, but no one-size-fits-all solution exists for assessing fairness.","ABROCA, AUC-ROC, accuracy, F1 score, true positive rate, false positive rate, disparity metrics",12,
finding_25,Mixed,"The randomized controlled trial found that highly informative feedback had a positive effect on perceived helpfulness but a negative effect on motivation compared to basic feedback. Students who received highly informative feedback rated it as significantly more helpful for learning, but less motivating than the control group receiving only essential feedback.","Six-item survey measuring comprehensiveness, helpfulness, progress, reflection, regulation, and motivation on a 4-point scale",257,
finding_26,Positive,"The T5 model with dual contrastive fine-tuning and label-aware data augmentation achieved a 4% to 8% improvement in accuracy over baseline methods for recognizing epistemic and topic-based dialogue acts. The best-performing model achieved 76% accuracy and a Cohen's kappa of 0.65 for topic labels, demonstrating substantial agreement with human annotations.","Accuracy, Precision, Recall, F1-Score, Cohen's Kappa",72,
finding_27,Positive,"GPT-4 achieved human-level performance on multiple NLP tasks including multi-label classification, binary classification, extraction, and sentiment analysis for educational survey feedback. The zero-shot approach with GPT-4 yielded a Jaccard similarity coefficient of 92.97% on consensus rows, comparable to human inter-rater agreement of 81.24%, demonstrating that LLMs can effectively analyze qualitative survey data without requiring labeled training examples.","Jaccard similarity coefficient, precision, recall, F1 score, accuracy, Hamming loss",2500,
finding_28,Positive,"Learners who received facilitation through a pedagogical conversational agent (PCA) achieved significantly better collaborative learning performance and process quality than those in the control condition. The recurrence indicators (gaze, language, and facial expression) were useful for predicting the collaborative learning process when using the PCA, though no significant correlation was found between these indicators and learning performance.",Gain scores on pre-test/post-test free recall tests and ratings of collaborative learning process quality using Meier et al.'s scale,44,
finding_29,Mixed,"LLaVA-Docent demonstrated strengths in fostering user engagement through sequential questioning and limiting cognitive overload by asking one question at a time. However, it showed weaknesses compared to GPT-4 including repetitive questioning, less diverse feedback, difficulty progressing through all appreciation stages, and occasional generation of inaccurate information (hallucinations).",Qualitative analysis of dialogue quality using Anderson's critical stages framework and expert evaluation of conversation characteristics,360,
finding_30,Positive,"FieldTripOrganizer outperformed the baseline tour planner in both usability (SUS score 79.47 vs 34.44) and user experience. Teachers reported the application was helpful, motivated them to organize field trips, and reduced their anxiety during the design process.","System Usability Scale (SUS) questionnaire and custom post-task questionnaire measuring support, autonomy, anxiety, and motivation levels",18,
finding_31,,"This is a commentary article that does not present original empirical findings. It provides a critical analysis of AI in education from social, historical, economic, and political perspectives, arguing that AI cannot be viewed as a neutral technology with inevitable beneficial effects.",,,
finding_32,Mixed,"The review found that neural question generation (NQG) can produce fluent and relevant factoid-type questions, but few studies focus specifically on educational applications. Current NQG systems struggle with generating high-level questions useful for educational contexts, and there is limited research on multi-paragraph contexts and controllable question generation systems.","Automatic evaluation metrics (BLEU, METEOR, ROUGE-L) and human evaluation criteria (fluency, relevance, answerability, usefulness for learning)",224,
finding_33,Positive,"The study found that 73% of student posts contained at least one emoji, demonstrating high engagement with the emoji-based annotation system. The emoji heat map tool helped instructors identify sections of reading material that students found difficult or interesting, with instructors reporting it would save them time and help them be more effective in course planning and revision.","Emoji usage frequency, instructor satisfaction ratings (USE questionnaire), and qualitative feedback from user study",1800,
finding_34,,,,,
finding_35,Mixed,"Choice-based personalization in a Climate Science MOOC increased learners' overall course activity, self-reported understanding of local climate issues, and likelihood to change climate-related habits. However, no significant differences were found in assignment completion rate, assignment length, time-on-task, final exam scores, or likelihood to recommend the course.","Number of course events (clickstream), self-reported understanding of local/global issues, self-reported likelihood to change habits, assignment completion rate, essay length, time-on-task, final exam scores",219,0.021
finding_36,,"This is a commentary/opinion piece that does not present empirical findings. The paper discusses the knowledge traditions of the AIED community, critiques from other academic traditions, and suggests three areas for the community to address: balancing learning and computational sciences, clarifying connections between learning theory and education, and distinguishing academic AIED from the commercial sector.",,,
finding_37,,"This is a commentary/opinion piece that critically examines the concept of 'personalised learning' in AI education research. The author argues that claims about personalised learning solving educational problems are largely unarticulated and lack empirical grounding, and that the discourse serves neoliberal agendas rather than educational equality.",,,
finding_38,Positive,"Deep neural networks and LSTM models achieved F1 scores of 0.44 and 0.459 respectively for within-dataset mind wandering detection, performing above chance level. Cross-dataset prediction using BiLSTMs achieved an F1 score of 0.352, demonstrating generalizability from lab to in-the-wild settings across different tasks and cultural backgrounds.","F1 score, AUC-PR, precision, recall for mind wandering detection",150,
finding_39,Positive,"The systematic review found that supervised machine learning prediction models, particularly neural networks, Random Forest, and SVM, are effective for overcoming limitations of traditional item calibration methods. Linguistic features, especially syntactic and semantic features extracted using NLP techniques and word embeddings, play a major role in determining item difficulty levels.","RMSE, accuracy, precision, recall, F1 score, and Pearson's correlation coefficient were used to evaluate prediction model performance against ground truth difficulty labels from psychometric testing or expert judgment",55,
finding_40,Mixed,"Teachers with higher AI-EdTech self-efficacy and AI understanding perceived more benefits, fewer concerns, and reported more trust in AI-EdTech. Geographic and cultural differences in teachers' trust were found, but no demographic differences emerged based on age, gender, or level of education.","Survey responses measuring trust, perceived benefits, perceived concerns, AI-EdTech self-efficacy, AI understanding, and cultural values",508,
finding_41,,This is a preface/editorial introducing a special issue on K-12 AI education. It does not present original empirical research findings but rather summarizes the articles included in the special issue and calls for future research in K-12 AI education.,,,
finding_42,Positive,"The QBERT model achieved 90.76% accuracy in equivalent question recognition on the WTC corpus and successfully categorized over 8,600 questions into 90 themes and 5,930 communities. The model answered 463 questions with high confidence using Wikipedia Summary, demonstrating effective question processing capabilities for understanding visitor curiosity.","Classification accuracy, F1 score, cosine similarity thresholds for question taxonomy, equivalent question detection, and question answering tasks",8600,
finding_43,Mixed,"Most children overestimated the cognitive abilities and intelligence of conversational assistants while being uncertain about their feelings or agency. Children lacked accurate understanding of data privacy and security aspects, and the majority believed it was wrong to be rude to conversational assistants, suggesting they attributed some moral standing to these AI systems.","Questionnaire responses and interview analysis measuring children's perceptions of CA intelligence, anthropomorphism, agency, trust, privacy awareness, and interaction appropriateness",194,
finding_44,Positive,"Participants who received personalized examples through the RELEX system showed significantly better writing performance and higher quality scores compared to those who received non-adaptive feedback. The adaptive feedback groups also reported significantly higher perceived usefulness, behavioral intention, and perceived learning gain.","Quality score based on procedural writing criteria (structure, clarity, specificity) and predicted recipe stars using a fine-tuned language model",187,
finding_45,Positive,"The study demonstrated that both an interpretable mathematical model (IMM) and a machine learning algorithm (MLA) could successfully identify students' task-specific strategies for solving histogram problems based on gaze data. The IMM achieved 62% to 84% accuracy in identifying strategies, while the MLA achieved 71% to 88% accuracy with jackknife cross-validation, showing that automated strategy identification from eye-tracking data is feasible for future intelligent tutoring applications.",Strategy identification accuracy comparing IMM and MLA classifications against qualitative expert coding of correct versus incorrect histogram interpretation strategies,50,
finding_46,Mixed,"GPT-4 achieved approximately 65% accuracy on standard calculus problems and 20% on competition-level problems, showing improvement over GPT-3.5 but still below passing grades. For grading tasks, GPT-4's average scores aligned closely with human graders, though detailed accuracy remained suboptimal due to arithmetic errors, loss of coherence, and occasional hallucinations.",Percentage accuracy on calculus problem solutions and grader aptness index comparing AI-assigned scores to human grader scores,10,
finding_47,,This document is a correction notice addressing a misspelling of the author's name in a previously published article. It does not contain empirical findings.,,,
finding_48,Positive,"The GPT-4 model using a few-shot approach effectively identified correct/incorrect trainee responses with an average F1 score of 0.84 and AUC score of 0.85. Additionally, GPT-4 adeptly rephrased incorrect trainee responses into desired responses, achieving performance comparable to human experts in terms of accuracy while maintaining similar responsiveness.","F1 score, AUC score, accuracy and responsiveness ratings of rephrased responses",383,
finding_49,Mixed,"The study found that while most instructors reported moderate to high familiarity with GenAI concepts, their actual use of GenAI tools for direct instructional tasks remained limited. Trust and distrust in GenAI were found to be related yet distinct constructs that can co-exist, with significant differences in familiarity across different trust and distrust groups. Instructors teaching only undergraduate courses showed lower trust and higher distrust compared to those teaching both undergraduate and graduate courses.","Survey responses measuring trust and distrust scales, familiarity with GenAI, and current practices of GenAI use in teaching",178,
finding_50,,"This is a theoretical paper describing the design of a game-based learning environment (ARIN-561) for teaching AI concepts to high school students. The design has not yet been evaluated in any studies with students, so no empirical findings are reported.",,,
finding_51,Positive,"The study demonstrates that near-optimal individualized curricula can be computed using meta-heuristic approaches despite the NP-Complete complexity of the problem. The genetic algorithm consistently improved solution quality from initial populations, with some solutions achieving fitness values near zero (fully satisfying all constraints). The decay-aware heuristic in the exact solver outperformed the baseline solver in computational efficiency.","Fitness function comprising four metrics: credit fulfillment, objective requisite matching, course uniqueness, and prerequisite satisfaction",124740,
finding_52,,"This is a commentary/interview piece discussing ethical considerations, commercialization, and future directions for AI in Education. It does not present original empirical findings but rather reflects on the field's history, challenges with datafication, surveillance concerns, and the need for causal models over purely correlational approaches.",,,
finding_53,Positive,"The study developed a German-language cognitive presence classifier achieving up to 82% accuracy and a Cohen's κ of 0.76 using a multilayer perceptron. Including additional learning traces beyond linguistic features did not improve predictive ability. The classifier successfully distinguished between exploration, integration, and resolution phases of cognitive presence in online discussions.","Cohen's κ (interrater reliability), accuracy, F1 scores",15,0.76
finding_54,Positive,"Participants trained via ChatGPT demonstrated significantly improved learning outcomes compared to conventional training methods. This included shorter activation times, higher consistency, and higher accuracy across all examined ADAS functions, with the ChatGPT group achieving 80-100% accuracy compared to lower rates in the conventional group.",Reaction times to activate ADAS functions and accuracy percentages of correct function activation,86,
finding_55,Positive,The study demonstrates that a Transformer-based automated short-answer grading model for German can achieve comparable performance to English models and reduce teacher grading workload by 26-50% while maintaining acceptable agreement rates (≤15% disagreement) with human graders through a hybrid grading workflow. The ASYST tool achieved a usability score of 88.8 (above average) with 100% task completion effectiveness among expert teachers.,"Accuracy, F-Score, Precision, Recall, Software Usability Score (SUS), workload reduction percentage, grading agreement rates",6,
finding_56,Mixed,"The Random Forest model achieved the best performance with 60.27% accuracy, 59.46% precision, 60.27% recall, 59.51% F1-score, and 87% ROC AUC for predicting students' academic performance in video-conference-assisted online learning. SHAP analysis revealed that students' self-reported performance during video conferencing was the most critical predictor, with decreased performance contributing negatively to academic outcomes.","Self-reported academic performance classification (very degraded, decreased, stable, good, very good) predicted using machine learning models",361,
finding_57,,"This is a commentary article reviewing other papers in a special issue on K-12 AI Education. It does not present original empirical research but rather synthesizes and critiques the approaches, frameworks, and curricula proposed by other authors, calling for a future research agenda to provide empirical grounding for K-12 AI education design.",,,
finding_58,Mixed,"Deep learning models outperformed feature-based machine learning models in cross-validation experiments, achieving up to 87% accuracy for English and 74-75% for Basque and Spanish in 4-level classification. However, models did not generalize well to unseen test data, with accuracy dropping to 28-38% for 4-level classification, indicating that while within-corpus performance was promising, cross-corpus generalizability remains a significant challenge.",Classification accuracy and weighted F1 score for predicting educational grade level of science texts,,
finding_59,,,,,
finding_60,Positive,"The study found that sentence embedding models, particularly USE-T and USE-DAN, can reliably evaluate the novelty of open-ended ideas generated during cocreative problem-solving in project-based learning. These models showed high correlation with expert evaluations, demonstrating their potential for providing real-time feedback to enhance collaborative creativity in educational settings.","Correlation between sentence embedding model novelty scores and expert evaluations (Pearson and Spearman correlations, R-squared, RMSE)",25,
finding_61,Positive,"GPT-4 outperformed GPT-3.5 and human instructors in generating more readable feedback and feedback containing effective components such as feeding-up, feeding-forward, process level, and self-regulation level information. GPT-4 also demonstrated higher reliability of feedback compared to GPT-3.5, though both models showed limited agreement with human instructors on feedback polarity for most assessment aspects.","Readability scores (5-point scale), presence of effective feedback components based on Hattie and Timperley framework, precision/recall/F1 for feedback polarity agreement",103,
finding_62,,"This is a commentary paper that does not present empirical findings. It discusses the growing criticism and backlash against AI in education, identifies different stakeholder groups with varying interests in AIED, and calls for constructive engagement between proponents and critics of educational AI technologies.",,,
finding_63,Positive,"The Intelligent Explanation Generator produced explanations rated significantly higher by students compared to random and generic baseline explainers. Statistical tests showed P values below .005, indicating the intelligent explainer's superiority in perceived explanatory power across multiple topics.",User ratings of explanation quality on a 0-5 star scale,102,
finding_64,Mixed,"Machine learning models for scoring scientific argumentation showed substantial agreement with human scores on average (mean Cohen's kappa = 0.60), but performance varied significantly based on assessment construct characteristics. Higher levels of complexity, diversity, and structure in assessment items were associated with decreased ML model performance, with significant negative correlations found for all three characteristics.",Cohen's kappa agreement between machine-predicted and human-assigned scores,932,0.6
finding_65,,,,,
finding_66,Positive,"The study found that combining machine translation with multilingual transformer models outperformed each method individually for crosslingual content scoring. The best results were achieved by first training a model on the large English ASAP dataset and then fine-tuning on smaller amounts of training data in the target language, with performance approaching monolingual scoring levels for some language pairs.",Quadratically weighted kappa (QWK) for scoring accuracy,,
finding_67,Positive,"ChatGPT (GPT-4 base model) achieved 66.42% overall accuracy on the FE Environmental Exam dataset, improving to 75.37% with noninvasive prompt modifications. The study demonstrated significant improvements in mathematical capabilities across successive ChatGPT model iterations, with refined GPT-4 achieving 86.21% accuracy on math-heavy sections compared to 41.38% for GPT-3.5-legacy.",Accuracy percentage on FE Environmental Exam practice questions across different sections and model versions,134,
finding_68,Positive,The proposed hybrid framework combining ontology-based and machine-learning techniques generated 363 MCQ stems (126 Wh-type and 237 Cloze questions). Expert evaluation showed 74.38% of questions were rated useful and 97.24% were grammatically correct. Cohen's Kappa coefficients indicated moderate to substantial agreement among evaluators.,"Expert evaluation using 3-point Likert scale for usefulness and grammatical correctness, with inter-rater reliability measured by Cohen's Kappa coefficients",363,
