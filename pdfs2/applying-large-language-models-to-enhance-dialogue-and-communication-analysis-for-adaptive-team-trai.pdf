<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="twitter:card" content="summary"/><meta name="twitter:site" content="@researchsquare"/><meta name="twitter:creator" content="@researchsquare"/><meta property="og:url" content="https://www.researchsquare.com"/><meta property="og:type" content="website"/><meta property="og:image:alt" content="Research Square"/><meta property="og:image:width" content="1280"/><meta property="og:image:height" content="720"/><meta property="og:locale" content="en_US"/><title>Applying Large Language Models to Enhance Dialogue and Communication Analysis for Adaptive Team Training | Research Square</title><meta name="robots" content="index,follow"/><meta property="og:title" content="Applying Large Language Models to Enhance Dialogue and Communication Analysis for Adaptive Team Training"/><meta property="og:description" content="Adaptive training environments that can analyze team communication content and provide remediation to facilitate team coordination offer great potential for enhancing adaptive training systems for teams. Developing computational models that can perform robust team communication analytics based..."/><meta property="og:image" content=""/><meta name="viewport" content="width=device-width, initial-scale=1"/><meta name="apple-mobile-web-app-title" content="Research Square"/><meta name="application-name" content="Research Square"/><link rel="shortcut icon" type="image/x-icon" href="/static/favicons/favicon.ico"/><link rel="apple-touch-icon" sizes="180x180" href="/static/favicons/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/static/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/static/favicons/favicon-16x16.png"/><link rel="manifest" href="/static/favicons/site.webmanifest"/><link rel="mask-icon" href="/static/favicons/safari-pinned-tab.svg" color="#244261"/><meta name="msapplication-config" content="/static/favicons/browserconfig.xml"/><meta name="msapplication-TileColor" content="#ffffff"/><meta name="theme-color" content="#ffffff"/><link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin="anonymous"/><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"/><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="anonymous"/><link rel="preconnect" href="https://app.snipcart.com"/><link rel="preconnect" href="https://cdn.snipcart.com"/><link rel="canonical" href="&#x27;https://www.researchsquare.com/article/rs-4565500/v1"/><meta name="description" content="Adaptive training environments that can analyze team communication content and provide remediation to facilitate team coordination offer great potential for enhancing adaptive training systems for teams. Developing computational models that can perform robust team communication analytics based..."/><meta property="og:title" content="Applying Large Language Models to Enhance Dialogue and Communication Analysis for Adaptive Team Training"/><meta property="og:description" content="Adaptive training environments that can analyze team communication content and provide remediation to facilitate team coordination offer great potential for enhancing adaptive training systems for teams. Developing computational models that can perform robust team communication analytics based..."/><meta property="og:url" content="&#x27;https://www.researchsquare.com/article/rs-4565500/v1"/><meta property="og:type" content="article"/><meta name="twitter:title" content="Applying Large Language Models to Enhance Dialogue and Communication Analysis for Adaptive Team Training"/><meta name="twitter:description" content="Adaptive training environments that can analyze team communication content and provide remediation to facilitate team coordination offer great potential for enhancing adaptive training systems for teams. Developing computational models that can perform robust team communication analytics based..."/><meta name="citation_publication_date" content="2024-07-01"/><meta property="citation_author" content="Randall Spain"/><meta property="citation_author" content="Wookhee Min"/><meta property="citation_author" content="Vikram Kumaran"/><meta property="citation_author" content="Jay Pande"/><meta property="citation_author" content="Jason Saville"/><meta property="citation_author" content="James Lester"/><meta name="citation_title" content="Applying Large Language Models to Enhance Dialogue and Communication Analysis for Adaptive Team Training"/><meta name="citation_doi" content="10.21203/rs.3.rs-4565500/v1"/><meta name="citation_pdf_url" content="https://www.researchsquare.com/article/rs-4565500/latest.pdf"/><meta name="next-head-count" content="50"/><script async="" src="https://polyfill-fastly.io/v3/polyfill.min.js?flags=gated&amp;features=String.prototype.startsWith%2CString.prototype.endsWith%2CArray.prototype.forEach%2CSet%2CArray.prototype.includes%2CString.prototype.includes%2CArray.prototype.find%2CMap%2CObject.values"></script><link rel="prefetch stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous"/><link rel="preconnect" href="https://app.snipcart.com"/><link rel="preconnect" href="https://cdn.snipcart.com"/><link rel="prefetch stylesheet" href="https://cdn.snipcart.com/themes/v3.3.0/default/snipcart.css"/><script async="" src="https://cdn.snipcart.com/themes/v3.3.0/default/snipcart.js"></script><link data-next-font="" rel="preconnect" href="/" crossorigin="anonymous"/><script id="gtm-init" data-nscript="beforeInteractive">
                    (function() {
                        var accessVector = localStorage.getItem('access_vector') || '';
                        window.dataLayer = window.dataLayer || [];

                        if (accessVector) {
                        window.dataLayer.push({
                            user: {
                            profile: {
                                profileInfo: {
                                snid: accessVector
                                }
                            }
                            }
                        });
                        }
                    })();
                </script><script id="gtm-init" data-nscript="beforeInteractive">(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-K279D39R');</script><link rel="preload" href="/_next/static/css/097af2fce36b4545.css" as="style"/><link rel="stylesheet" href="/_next/static/css/097af2fce36b4545.css" data-n-g=""/><link rel="preload" href="/_next/static/css/74fc351921f6806e.css" as="style"/><link rel="stylesheet" href="/_next/static/css/74fc351921f6806e.css" data-n-p=""/><link rel="preload" href="/_next/static/css/5eb7067aaea39ce0.css" as="style"/><link rel="stylesheet" href="/_next/static/css/5eb7067aaea39ce0.css"/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script defer="" src="/_next/static/chunks/2b7b2d2a-fb8e248e4f64c09b.js"></script><script defer="" src="/_next/static/chunks/6086-d8b3d1ee0298789b.js"></script><script defer="" src="/_next/static/chunks/8719.7a9257edd8ce0cc0.js"></script><script src="/_next/static/chunks/webpack-0bfccabd01ff3222.js" defer=""></script><script src="/_next/static/chunks/framework-88f2b98ffb47976f.js" defer=""></script><script src="/_next/static/chunks/main-e365b9365853811f.js" defer=""></script><script src="/_next/static/chunks/pages/_app-aed4a9c8a0313d20.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-a8c9805be41684e2.js" defer=""></script><script src="/_next/static/chunks/d6e1aeb5-d41cf1c71f0906fb.js" defer=""></script><script src="/_next/static/chunks/2981-d6d616b40ecc010b.js" defer=""></script><script src="/_next/static/chunks/7206-4b589a2cfeb5b511.js" defer=""></script><script src="/_next/static/chunks/724-4454ccdce88563f0.js" defer=""></script><script src="/_next/static/chunks/7915-505fa2b3a25928a0.js" defer=""></script><script src="/_next/static/chunks/4197-895c5fe4fad6142f.js" defer=""></script><script src="/_next/static/chunks/722-3bf0cafff23e180b.js" defer=""></script><script src="/_next/static/chunks/5598-12e41c77dcb43113.js" defer=""></script><script src="/_next/static/chunks/7947-4632d0176d1c053d.js" defer=""></script><script src="/_next/static/chunks/1444-f7fa1162c6aed527.js" defer=""></script><script src="/_next/static/chunks/1265-b150e39bc366fdc6.js" defer=""></script><script src="/_next/static/chunks/pages/article/%5Bidentity%5D/%5B%5B...version%5D%5D-93e68a26a3c162c2.js" defer=""></script><script src="/_next/static/f22rFx76zg5YRmNXsU5vb/_buildManifest.js" defer=""></script><script src="/_next/static/f22rFx76zg5YRmNXsU5vb/_ssgManifest.js" defer=""></script><style id="__jsx-1810699180">.jsx-1810699180:export{white:#FFFFFF;grey10:#FAFAFA;grey20:#F2F2F3;grey30:#E5E6E8;grey40:#CCCFD2;grey50:#B2B7BC;grey60:#999FA6;grey65:#7F8790;grey70:#66707A;grey80:#33404E;grey90:#192837;black:#001122;g1:#F3FBF6;g2:#3AAE74;g3:#319767;g4:#268455;g5:#20734D;g6:#63b91a;b1:#F2F6FB;b2:#186EC4;b3:#175089;b4:#1B4064;b5:#15324E;y1:#FFFCEB;y2:#EBCC5F;orange:#E9BE5C;orange-2:#FB8C00;orange-3:#EF6C00;}#search-button.jsx-1810699180{pointer-events:auto;}#search-input.jsx-1810699180{font-size:14px;padding-top:1rem;padding-bottom:1rem;-webkit-letter-spacing:0.3px;-moz-letter-spacing:0.3px;-ms-letter-spacing:0.3px;letter-spacing:0.3px;}#search-input.jsx-1810699180::-webkit-input-placeholder{color:#66707A;}#search-input.jsx-1810699180::-moz-placeholder{color:#66707A;}#search-input.jsx-1810699180:-ms-input-placeholder{color:#66707A;}#search-input.jsx-1810699180::placeholder{color:#66707A;}#search-button.jsx-1810699180{padding-bottom:0.3rem;}#search-button.jsx-1810699180:disabled{pointer-events:none;}#search-icon.jsx-1810699180{color:#CCCFD2;}#search-icon.jsx-1810699180:hover{color:#66707A;}#search-spinner.jsx-1810699180{color:#66707A;}</style><style id="__jsx-3772613807">.lightbox-wrapper .yarl__slide{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;}.lightbox-wrapper .yarl__slide_wrapper{max-height:80%;overflow-y:auto;}.lightbox-wrapper .yarl__slide_description_container{width:100%;max-height:20%;overflow-y:auto;background:rgba(0,0,0,0.85);color:white;padding:1rem;box-sizing:border-box;}.lightbox-wrapper .yarl__slide_description,.lightbox-wrapper .yarl__slide_description *{-webkit-user-select:text !important;-moz-user-select:text !important;-ms-user-select:text !important;user-select:text !important;pointer-events:auto !important;}</style></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-K279D39R" height="0" width="0" style="display:none;visibility:hidden" /></noscript><div id="__next"><div class="no-scroll"><div class="original-css"><nav class="rs-navbar false
                    false
                " role="navigation" aria-label="navigation"><div class="rs-container rs-ph"><div class="left"><div class="hamburger hamburger--squeeze false false" role="navigation"><div class="hamburger-box"><div class="hamburger-inner"></div></div></div><a href="/"><img class="logo-rs" src="/static/img/logos/logo-rs.svg" alt="Research Square"/></a><form class="jsx-1810699180 b-pl-10"><div class="jsx-1810699180 control has-icons-right lg:tw-w-96 md:tw-w-52"><input type="text" placeholder="Search preprints" id="search-input" class="jsx-1810699180 input"/><button disabled="" id="search-button" type="submit" class="jsx-1810699180 icon is-small is-right rounded-r-md"><i id="search-icon" class="jsx-1810699180 fas fa-search"></i></button></div></form></div><div class="right"><div class="nav-link"><h6>Browse</h6><i class="fas fa-caret-down"></i><div class="nav-dropdown"><div class=""><a href="/browse"><div class="nav-link  "><h6 class="align" style="text-wrap:initial">Preprints</h6></div></a></div><div class=""><a href="/journals"><div class="nav-link  "><h6 class="align" style="text-wrap:initial"><span class="is-italic">In Review </span>Journals</h6></div></a></div><div class=""><a href="/coronavirus"><div class="nav-link  "><h6 class="align" style="text-wrap:initial">COVID-19 Preprints</h6></div></a></div><div class=""><a href="/browse?articleType=Video"><div class="nav-link  "><h6 class="align" style="text-wrap:initial">AJE Video Bytes</h6></div></a></div><div class="b-mb-15"></div></div></div><div class="nav-link"><h6>Research Tools</h6><i class="fas fa-caret-down"></i><div class="nav-dropdown"><div class=""><a href="/researchers/promotion"><div class="nav-link  "><h6 class="align" style="text-wrap:initial">Research Promotion</h6></div></a></div><div class=""><a href="/researchers/editing"><div class="nav-link  "><h6 class="align" style="text-wrap:initial">AJE Professional Editing</h6></div></a></div><div class=""><a href="/researchers/rubriq"><div class="nav-link  "><h6 class="align" style="text-wrap:initial">AJE Rubriq</h6></div></a></div><div class="b-mb-15"></div></div></div><div class="nav-link"><h6>About</h6><i class="fas fa-caret-down"></i><div class="nav-dropdown"><div class=""><a href="/researchers/preprints"><div class="nav-link  "><h6 class="align" style="text-wrap:initial">Preprint Platform</h6></div></a></div><div class=""><a href="/researchers/in-review"><div class="nav-link  "><h6 class="align" style="text-wrap:initial">In Review</h6></div></a></div><div class=""><a href="/legal/editorial"><div class="nav-link  "><h6 class="align" style="text-wrap:initial">Editorial Policies</h6></div></a></div><div class=""><a href="/about"><div class="nav-link  "><h6 class="align" style="text-wrap:initial">Our Team</h6></div></a></div><div class=""><a href="/advisory-board"><div class="nav-link  "><h6 class="align" style="text-wrap:initial">Advisory Board</h6></div></a></div><div class=""><a href="https://support.researchsquare.com" target="_blank" rel="noopener noreferrer" data-track="click_contact_us_button" data-track-context="Help Centre, Contact us link clicked from arrow links"><div class="nav-link  "><h6 class="align" style="text-wrap:initial">Help Center</h6></div></a></div><div class="b-mb-15"></div></div></div><div class=""><a href="/login"><div class="nav-link  "><h6 class="align" style="text-wrap:initial">Sign In</h6></div></a></div><div class="b-pl-10"><a class="button color-button-g3 " data-track="click_submit_a_preprint_button" data-track-context="Submit Preprint main button click" href="/submit"><i class="button-icon fas fa-plus"></i>Submit a Preprint</a></div></div></div></nav></div><div class="rs-container"><div class="tw-mt-12 "><div class=""><div class="tw-max-w-[1600px] tw-mx-auto"><div class="lg:tw-grid lg:tw-grid-cols-10 lg:tw-grid-flow-col"><div class="tw-min-h-screen tw-col-span-6"><div class="tw-flex tw-justify-center lg:tw-justify-start lg:tw-border-r-2 lg:tw-border-gray-50"><div class="tw-max-w-reading tw-w-full tw-overflow-x-scroll _rs-scrollbar lg:tw-pb-20 lg:tw-px-4 lg:tw-pt-2 xl:tw-px-8"><div class="tw-flex tw-pr-4 lg:tw-pr-0 tw-mt-4 lg:tw-hidden tw-pl-3 tw-justify-end"><div class="tw-pr-3"><div class="tw-relative tw-inline-block tw-text-left" data-headlessui-state=""><div><button class="tw-group tw-inline-flex tw-items-center tw-px-3 tw-py-2 tw-transition-all tw-border-2 tw-rounded-md tw-group tw-bg-white tw-border-gray-100 hover:tw-bg-gray-100 focus:tw-outline-none" id="headlessui-menu-button-:R2h6lm:" type="button" aria-haspopup="menu" aria-expanded="false" data-headlessui-state=""><svg xmlns="http://www.w3.org/2000/svg" class="tw-w-5 tw-h-5 tw-text-gray-600" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path d="M16.6187 6.07577C17.6476 6.07404 18.4714 6.44992 19.09 7.20342C19.6858 7.97982 19.9846 8.89392 19.9864 9.94571C19.9879 10.8146 19.8292 11.6266 19.5103 12.3816C19.1915 13.1367 18.7925 13.8234 18.3134 14.4415C17.8114 15.0597 17.2864 15.5979 16.7385 16.0562C16.1676 16.5373 15.6424 16.9383 15.1628 17.2592C14.6832 17.603 14.2949 17.8552 13.998 18.0158C13.6781 18.1763 13.5296 18.2566 13.5525 18.2566L12.7619 17.1947C12.9675 17.1029 13.2416 16.9195 13.5841 16.6446C13.9266 16.3925 14.2691 16.0832 14.6114 15.7168C14.9309 15.3504 15.2161 14.9498 15.4668 14.5149C15.7177 14.1029 15.8427 13.7026 15.8421 13.3139C15.8417 13.0852 15.7041 12.8568 15.4294 12.6286C15.1546 12.4233 14.857 12.1723 14.5363 11.8756C14.2158 11.6017 13.9179 11.2593 13.6429 10.8482C13.3678 10.4599 13.2298 9.97999 13.2289 9.40837C13.228 8.90534 13.3301 8.44786 13.5352 8.03595C13.7175 7.62407 13.9684 7.26924 14.288 6.97145C14.5848 6.69657 14.9388 6.47875 15.3501 6.318C15.7614 6.15725 16.1843 6.07651 16.6187 6.07577ZM7.29256 6.02144C7.81845 6.02055 8.29881 6.13406 8.73363 6.36198C9.16845 6.58989 9.54624 6.89793 9.86701 7.28609C10.1878 7.67426 10.44 8.1197 10.6238 8.62242C10.7848 9.14805 10.8657 9.7081 10.8667 10.3026C10.8687 11.4916 10.6191 12.5895 10.1178 13.5964C9.5936 14.6262 9.02346 15.5075 8.40734 16.2402C7.67713 17.1103 6.84389 17.9006 5.90763 18.611L4.91144 17.6866C5.11699 17.5491 5.36814 17.3314 5.66488 17.0337C5.93876 16.736 6.20111 16.3811 6.45192 15.9691C6.67992 15.5801 6.88501 15.1681 7.06719 14.7334C7.22651 14.2987 7.30583 13.8756 7.30513 13.464C7.30467 13.1896 7.13279 12.9612 6.78951 12.7789C6.44623 12.5966 6.06857 12.3686 5.65653 12.0949C5.2445 11.8212 4.86663 11.4674 4.52292 11.0336C4.17925 10.6226 4.00678 10.0398 4.00551 9.28526C4.00473 8.82796 4.09548 8.4048 4.27774 8.01579C4.46 7.62677 4.71094 7.28337 5.03055 6.98559C5.32729 6.68784 5.68129 6.44716 6.09255 6.26355C6.48099 6.10283 6.88099 6.02213 7.29256 6.02144Z" stroke="currentColor" fill="currentColor"></path></svg><p class="tw-antialiased tw-hidden tw-ml-2 tw-text-sm tw-font-bold tw-text-gray-700 group-hover:tw-underline md:tw-block lg:tw-hidden xl:tw-block">Cite</p></button></div></div></div><div class="last:tw-pr-0"><div class="tw-relative tw-inline-block tw-text-left" data-headlessui-state=""><div><button class="tw-group tw-inline-flex tw-items-center tw-px-3 tw-py-2 tw-transition-all tw-border-2 tw-rounded-md tw-group tw-bg-white tw-border-gray-100 hover:tw-bg-gray-100 focus:tw-outline-none" id="headlessui-menu-button-:R2p6lm:" type="button" aria-haspopup="menu" aria-expanded="false" data-headlessui-state=""><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" aria-hidden="true" class="tw-w-5 tw-h-5 tw-text-gray-600"><path stroke-linecap="round" stroke-linejoin="round" d="M8.684 13.342C8.886 12.938 9 12.482 9 12c0-.482-.114-.938-.316-1.342m0 2.684a3 3 0 110-2.684m0 2.684l6.632 3.316m-6.632-6l6.632-3.316m0 0a3 3 0 105.367-2.684 3 3 0 00-5.367 2.684zm0 9.316a3 3 0 105.368 2.684 3 3 0 00-5.368-2.684z"></path></svg><p class="tw-antialiased tw-hidden tw-ml-2 tw-text-sm tw-font-bold tw-text-gray-700 group-hover:tw-underline md:tw-block lg:tw-hidden xl:tw-block">Share</p></button></div></div></div><a href="https://www.researchsquare.com/article/rs-4565500/v1.pdf?c=1747685311000" target="_blank" rel="noopener noreferrer" class="tw-text-blue-600 hover:tw-text-blue-800 hover:tw-underline tw-inline-flex tw-items-center tw-px-3.5 tw-py-1.5 tw-transition-all tw-border-2 tw-rounded-md tw-group tw-border-blue-50 tw-bg-blue-50 hover:tw-bg-blue-100 hover:tw-border-blue-100 focus:tw-outline-none tw-text-blue-600 hover:tw-text-blue-800 hover:tw-underline tw-ml-3" data-track="content_download" data-track-context="article pdf" data-track-category="Published" data-track-content-type="Research Article" data-track-label="rs-4565500"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" aria-hidden="true" class="tw-w-5 tw-h-5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 10v6m0 0l-3-3m3 3l3-3m2 8H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"></path></svg><span class="tw-antialiased tw-ml-2 tw-text-sm tw-font-bold"><span class="tw-hidden xl:tw-contents">Download</span> PDF</span></a></div><div class="tw-py-4 tw-border-b-2 tw-border-gray-100"><div class="tw-px-4"><p class="tw-antialiased tw-text-left tw-text-sm tw-text-black tw-pb-2">Research Article</p><h1 class="tw-antialiased tw-font-bold tw-text-left tw-text-blue-900 tw-text-2xl sm:tw-text-3xl"><p>Applying Large Language Models to Enhance Dialogue and Communication Analysis for Adaptive Team Training</p></h1></div></div><div class="tw-px-4"><div class="tw-flex tw-items-start tw-justify-between"><button type="button" class="focus:tw-outline-none tw-group tw-min-w-full"><div class="tw-flex tw-items-start tw-justify-between"><div class="tw-py-3 tw-w-full"><p class="tw-antialiased tw-text-left tw-text-sm tw-text-black">Randall Spain, Wookhee Min, Vikram Kumaran, Jay Pande, Jason Saville, and 1 more</p></div><div class="tw-py-2 sm:tw-border-r-2 sm:tw-border-white"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" aria-hidden="true" class="tw-ml-4 tw-h-7 tw-w-7 tw-flex-shrink-0 tw-rounded-full tw-text-gray-500 group-hover:tw-bg-gray-200 group-hover:tw-text-gray-900 tw-transform tw-duration-300"><path fill-rule="evenodd" d="M5.293 7.293a1 1 0 011.414 0L10 10.586l3.293-3.293a1 1 0 111.414 1.414l-4 4a1 1 0 01-1.414 0l-4-4a1 1 0 010-1.414z" clip-rule="evenodd"></path></svg></div></div></button></div></div><div class="tw-py-3 md:tw-px-1.5 tw-border-gray-100 tw-bg-gray-50 lg:tw-bg-white lg:tw-border-0 lg:tw-px-0 lg:tw-py-0"><div class="tw-mx-2.5 lg:tw-mx-0 tw-border-2 tw-border-gray-100 tw-bg-white tw-rounded-lg lg:tw-mb-1 tw-max-w-xl lg:tw-max-w-none"><div class="tw-px-4"><button type="button" class="tw-min-w-full focus:tw-outline-none tw-group"><div class="tw-flex tw-items-center tw-justify-between"><div class="tw-w-full tw-py-2"><p class="tw-antialiased tw-text-left tw-text-sm tw-text-red-800 tw-font-bold">This is a preprint; it has not been peer reviewed by a journal.</p></div><div class="tw-rounded-xl tw-font-bold tw-px-3 tw-py-1 tw-bg-gray-100 tw-text-gray-700 tw-my-2 tw-flex tw-items-center tw-duration-300 tw-transform tw-bg-gray-100 group-hover:tw-bg-gray-200"><p class="tw-antialiased tw-text-left tw-text-sm tw-text-black tw-text-gray-800 group-hover:tw-text-gray-900 tw-flex"></p><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" aria-hidden="true" class="tw-text-gray-800 group-hover:tw-text-gray-900 tw-h-6 tw-w-6 tw-flex-shrink-0 tw-rounded-full tw-transform tw-duration-300"><path fill-rule="evenodd" d="M5.293 7.293a1 1 0 011.414 0L10 10.586l3.293-3.293a1 1 0 111.414 1.414l-4 4a1 1 0 01-1.414 0l-4-4a1 1 0 010-1.414z" clip-rule="evenodd"></path></svg></div></div></button></div><div class="tw-px-4"><div class="tw-border-t-2 tw-border-gray-100 tw-py-2"><p class="tw-antialiased tw-text-left tw-text-sm tw-text-black">https://doi.org/<!-- -->10.21203/rs.3.rs-4565500/v1</p><p class="tw-antialiased tw-text-left tw-text-sm tw-text-black">This work is licensed under a <!-- -->CC BY 4.0<!-- --> License</p></div></div></div><div class="tw-mx-2.5 lg:tw-mx-0 tw-border-2 tw-border-gray-100 tw-bg-white tw-rounded-lg lg:tw-hidden tw-max-w-xl lg:tw-max-w-none tw-mt-3 tw-mb-0"><div class="tw-px-4"><button type="button" class="tw-min-w-full focus:tw-outline-none tw-group"><div class="tw-flex tw-items-center tw-justify-between"><div class="tw-w-full tw-py-2"><div class="tw-flex tw-items-center"><h3 class="tw-antialiased tw-font-bold tw-text-left tw-text-blue-900 tw-text-base">Status:</h3><div class="tw-rounded-xl tw-font-bold tw-px-3 tw-py-1 tw-bg-green-600 tw-text-white tw-ml-2"><p class="tw-antialiased tw-text-left tw-text-sm tw-text-white">Published</p></div></div></div><div class="tw-rounded-xl tw-font-bold tw-px-3 tw-py-1 tw-bg-gray-100 tw-text-gray-700 tw-my-2 tw-flex tw-items-center tw-duration-300 tw-transform tw-bg-gray-100 group-hover:tw-bg-gray-200"><p class="tw-antialiased tw-text-left tw-text-sm tw-text-black tw-text-gray-800 group-hover:tw-text-gray-900 tw-flex"></p><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" aria-hidden="true" class="tw-text-gray-800 group-hover:tw-text-gray-900 tw-h-6 tw-w-6 tw-flex-shrink-0 tw-rounded-full tw-transform tw-duration-300"><path fill-rule="evenodd" d="M5.293 7.293a1 1 0 011.414 0L10 10.586l3.293-3.293a1 1 0 111.414 1.414l-4 4a1 1 0 01-1.414 0l-4-4a1 1 0 010-1.414z" clip-rule="evenodd"></path></svg></div></div><div class="tw-mb-3 tw-rounded-lg sm:tw-mx-0 tw-bg-white tw-border-2 tw-border-gray-100"><div class="tw-px-2.5 sm:tw-px-4"><img src="https://assets-eu.researchsquare.com/logos/journals/logo-international-journal-of-artificial-intelligence-in-education-m.svg" alt="International Journal of Artificial Intelligence in Education" class="tw-py-3 tw-w-80"/></div></div></button></div><div class="tw-px-4 tw-py-2 tw-border-t-2 tw-border-gray-100"><div class="tw-text-left"><h3 class="tw-antialiased tw-font-bold tw-text-left tw-text-blue-900 tw-text-base">Journal Publication</h3><p class="tw-antialiased tw-text-left tw-text-sm tw-text-black">published <!-- -->14 May, 2025</p></div><p class="tw-antialiased tw-text-left tw-text-sm tw-text-black tw-mt-3 tw-mb-1"><a href="https://doi.org/10.1007/s40593-025-00479-5" target="_blank" rel="noopener noreferrer" class="tw-text-blue-600 hover:tw-text-blue-800 hover:tw-underline" data-track="click_reading_the_published_version" data-track-context="International Journal of Artificial Intelligence in Education" data-track-category="Published" data-track-link-text="Read the published version in International Journal of Artificial Intelligence in Education" data-track-content-type="Research Article" data-track-label="Springer Link"><span class="tw-rounded-xl tw-font-bold tw-px-3 tw-py-1 tw-bg-blue-50 hover:tw-bg-blue-100 tw-transition tw-block tw-w-full tw-text-center">Read the published version in<!-- --> <span class="tw-italic">International Journal of Artificial Intelligence in Education</span>  →</span></a></p></div><div class="tw-px-4 tw-border-t-2 tw-border-gray-100"><button type="button" class="tw-min-w-full focus:tw-outline-none tw-group"><div class="tw-flex tw-items-center tw-justify-between"><div class="tw-w-full tw-py-2"><div class="tw-text-left"><h3 class="tw-antialiased tw-font-bold tw-text-left tw-text-blue-900 tw-text-base">Version <!-- -->1</h3><p class="tw-antialiased tw-text-left tw-text-sm tw-text-black tw-text-gray-700">posted<!-- --> </p></div></div><div class="tw-rounded-xl tw-font-bold tw-px-3 tw-py-1 tw-bg-gray-100 tw-text-gray-700 tw-my-2 tw-flex tw-items-center tw-duration-300 tw-transform tw-bg-gray-100 group-hover:tw-bg-gray-200"><p class="tw-antialiased tw-text-left tw-text-sm tw-text-black tw-text-gray-800 group-hover:tw-text-gray-900 tw-flex">11</p><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" aria-hidden="true" class="tw-text-gray-800 group-hover:tw-text-gray-900 tw-h-6 tw-w-6 tw-flex-shrink-0 tw-rounded-full tw-transform tw-duration-300 tw-ml-2"><path fill-rule="evenodd" d="M5.293 7.293a1 1 0 011.414 0L10 10.586l3.293-3.293a1 1 0 111.414 1.414l-4 4a1 1 0 01-1.414 0l-4-4a1 1 0 010-1.414z" clip-rule="evenodd"></path></svg></div></div></button><div class="tw-pb-1"><p class="tw-antialiased tw-text-left tw-text-sm tw-text-black"><span class="tw-rounded-xl tw-font-bold tw-px-3 tw-py-1 tw-bg-gray-100 tw-text-gray-700 tw-flex tw-justify-center tw-w-full tw-mb-2">You are reading this <!-- -->latest<!-- --> <!-- -->preprint<!-- --> version</span></p></div></div></div></div><div class="tw-border-b-2 tw-border-gray-100 tw-px-4"><div class="tw-flex tw-items-start tw-justify-between tw-min-w-full"><button type="button" class="focus:tw-outline-none tw-group tw-min-w-full"><div class="tw-flex tw-items-start tw-justify-between"><div class="tw-py-2 tw-w-full"><h2 class="tw-antialiased tw-font-bold tw-text-left tw-text-blue-900 tw-text-xl">Abstract</h2></div><div class="tw-py-2 sm:tw-border-r-2 sm:tw-border-white"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" aria-hidden="true" class="tw-ml-4 tw-h-7 tw-w-7 tw-flex-shrink-0 tw-rounded-full tw-text-gray-500 group-hover:tw-bg-gray-200 group-hover:tw-text-gray-900 tw-transform tw-duration-300 -tw-rotate-180"><path fill-rule="evenodd" d="M5.293 7.293a1 1 0 011.414 0L10 10.586l3.293-3.293a1 1 0 111.414 1.414l-4 4a1 1 0 01-1.414 0l-4-4a1 1 0 010-1.414z" clip-rule="evenodd"></path></svg></div></div></button></div><div class="tw-pb-1"><div class="tw-text-base tw-antialiased tw-leading-relaxed tw-text-left tw-text-black md:tw-leading-loose _fulltext-content"><span style="display:block"><div><p>Adaptive training environments that can analyze team communication content and provide remediation to facilitate team coordination offer great potential for enhancing adaptive training systems for teams. Developing computational models that can perform robust team communication analytics based on small datasets is challenging. Large language models (LLMs) offer significant potential to address these challenges and enhance dialogue act classification performance using zero-shot and few-shot learning. This paper evaluates LLMs against previous state-of-the-art methods, with an emphasis on dialogue act recognition performance and error analysis for identifying frequently misclassified instances. Results from a small team communication dataset indicate that zero-shot LLMs, particularly those utilizing GPT-4 and refined through robust prompt engineering, achieve significant classification performance improvements in dialogue act recognition compared to previous state-of-the-art transformer-based models fine-tuned with team communication data. Error analysis shows that the prompt refinements, especially those aimed at clarifying confusion between dialogue acts, result in superior recall rates for challenging dialogue act labels by effectively handling complex dialogue scenarios and ambiguities within communication data. Our transformer-based framework demonstrates its effectiveness in achieving high accuracy rates in dialogue act recognition with minimal training data, underscoring its potential to enhance team training programs by providing adaptive feedback. This approach paves the way for developing AI-enabled training systems that can adapt to the dynamic communication styles of different teams.</p></div></span></div><div class="tw-flex tw-flex-wrap tw-pb-3"><div class="tw-my-2 tw-mr-2"><p class="tw-antialiased tw-text-left tw-text-sm tw-text-black"><span class="tw-rounded-xl tw-font-bold tw-px-3 tw-py-1 tw-bg-gray-100 tw-text-gray-700">Natural language processing</span></p></div><div class="tw-my-2 tw-mr-2"><p class="tw-antialiased tw-text-left tw-text-sm tw-text-black"><span class="tw-rounded-xl tw-font-bold tw-px-3 tw-py-1 tw-bg-gray-100 tw-text-gray-700">Large language models</span></p></div><div class="tw-my-2 tw-mr-2"><p class="tw-antialiased tw-text-left tw-text-sm tw-text-black"><span class="tw-rounded-xl tw-font-bold tw-px-3 tw-py-1 tw-bg-gray-100 tw-text-gray-700">Team communication analysis</span></p></div><div class="tw-my-2 tw-mr-2"><p class="tw-antialiased tw-text-left tw-text-sm tw-text-black"><span class="tw-rounded-xl tw-font-bold tw-px-3 tw-py-1 tw-bg-gray-100 tw-text-gray-700">Dialogue act recognition</span></p></div></div></div></div><div class="tw-border-b-2 tw-border-gray-100 tw-px-4"><div class="tw-flex tw-items-start tw-justify-between tw-min-w-full"><button type="button" class="focus:tw-outline-none tw-group tw-min-w-full"><div class="tw-flex tw-items-start tw-justify-between"><div class="tw-py-2 tw-w-full"><h2 class="tw-antialiased tw-font-bold tw-text-left tw-text-blue-900 tw-text-xl">Figures</h2></div><div class="tw-py-2 sm:tw-border-r-2 sm:tw-border-white"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" aria-hidden="true" class="tw-ml-4 tw-h-7 tw-w-7 tw-flex-shrink-0 tw-rounded-full tw-text-gray-500 group-hover:tw-bg-gray-200 group-hover:tw-text-gray-900 tw-transform tw-duration-300 -tw-rotate-180"><path fill-rule="evenodd" d="M5.293 7.293a1 1 0 011.414 0L10 10.586l3.293-3.293a1 1 0 111.414 1.414l-4 4a1 1 0 01-1.414 0l-4-4a1 1 0 010-1.414z" clip-rule="evenodd"></path></svg></div></div></button></div><div class="tw-pb-1"><div class="tw-flex tw-flex-wrap tw-items-end tw-pt-2"><button class="tw-mb-4 tw-mr-4 focus:tw-outline-none tw-group" type="button" tabindex="0"><img alt="Figure 1" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="tw-mb-2 tw-border-2 tw-border-gray-100 tw-rounded-lg group-hover:tw-border-gray-200" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fassets-eu.researchsquare.com%2Ffiles%2Frs-4565500%2Fv1%2Fd928177ce6493f8dad72a910.jpg%3FmaxDims%3D150x150&amp;w=128&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fassets-eu.researchsquare.com%2Ffiles%2Frs-4565500%2Fv1%2Fd928177ce6493f8dad72a910.jpg%3FmaxDims%3D150x150&amp;w=256&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fassets-eu.researchsquare.com%2Ffiles%2Frs-4565500%2Fv1%2Fd928177ce6493f8dad72a910.jpg%3FmaxDims%3D150x150&amp;w=256&amp;q=75"/><p class="tw-antialiased tw-text-left tw-text-sm tw-text-black group-hover:tw-underline">Figure 1</p></button><button class="tw-mb-4 tw-mr-4 focus:tw-outline-none tw-group" type="button" tabindex="1"><img alt="Figure 2" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="tw-mb-2 tw-border-2 tw-border-gray-100 tw-rounded-lg group-hover:tw-border-gray-200" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fassets-eu.researchsquare.com%2Ffiles%2Frs-4565500%2Fv1%2F28e6b7341485c5137e1cd266.jpg%3FmaxDims%3D150x150&amp;w=128&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fassets-eu.researchsquare.com%2Ffiles%2Frs-4565500%2Fv1%2F28e6b7341485c5137e1cd266.jpg%3FmaxDims%3D150x150&amp;w=256&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fassets-eu.researchsquare.com%2Ffiles%2Frs-4565500%2Fv1%2F28e6b7341485c5137e1cd266.jpg%3FmaxDims%3D150x150&amp;w=256&amp;q=75"/><p class="tw-antialiased tw-text-left tw-text-sm tw-text-black group-hover:tw-underline">Figure 2</p></button></div></div></div><div class="tw-border-b-2 tw-border-gray-100 tw-px-4"><div class="tw-flex tw-items-start tw-justify-between tw-min-w-full"><button type="button" class="focus:tw-outline-none tw-group tw-min-w-full"><div class="tw-flex tw-items-start tw-justify-between"><div class="tw-py-2 tw-w-full"><h2 class="tw-antialiased tw-font-bold tw-text-left tw-text-blue-900 tw-text-xl">Introduction</h2></div><div class="tw-py-2 sm:tw-border-r-2 sm:tw-border-white"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" aria-hidden="true" class="tw-ml-4 tw-h-7 tw-w-7 tw-flex-shrink-0 tw-rounded-full tw-text-gray-500 group-hover:tw-bg-gray-200 group-hover:tw-text-gray-900 tw-transform tw-duration-300 -tw-rotate-180"><path fill-rule="evenodd" d="M5.293 7.293a1 1 0 011.414 0L10 10.586l3.293-3.293a1 1 0 111.414 1.414l-4 4a1 1 0 01-1.414 0l-4-4a1 1 0 010-1.414z" clip-rule="evenodd"></path></svg></div></div></button></div><div class="tw-pb-1"><div class="tw-text-base tw-antialiased tw-leading-relaxed tw-text-left tw-text-black md:tw-leading-loose _fulltext-content"><span style="display:block"><div><p>Effective communication among team members is essential in many domains, especially those characterized by complex, high-stakes, time-limited tasks such operating rooms, aircraft cockpits, command and control stations, and flight control centers. In these environments, team members must effectively communicate with one another to minimize errors, improve coordination, and facilitate team action processes required for effective performance. Conversely, ineffective communication can lead to disastrous consequences. For decades, team science researchers have investigated methods to support team communication analysis to better understand the behaviors that exemplify high-performing teams and to support team training (Marlow et al., 2018). As a result, speech-based communication data is playing an increasingly important role in the advancement of adaptive training systems for teams (Johnston et al., 2018). </p>
<p>Adaptive team training systems aim to improve both team performance outcomes and teamwork behaviors that facilitate team performance by providing tailored feedback based on patterns of their verbal and nonverbal interactions (Sottilaire et al., 2018). Because many teamwork and team decision-making behaviors can be assessed by monitoring a team’s verbal communication, a significant task for the AI-driven training research community is to develop speech recognition and natural language processing (NLP) capabilities that facilitate team performance measurement (Badrinath &amp; Balakrishnan, 2022; Willms et al., 2019). In particular, being able to leverage NLP capabilities to automatically classify and label spoken communication between team members would represent a significant step towards the development of training environments that can partially or fully automate the assessment of teamwork skills and team performance. Indeed, research has shown that speech patterns can provide meaningful insights into team performance (Baber et al., 2022; John et al., 2019; van den Oever &amp; Schraagen, 2021) and can be used to identify team communication behaviors associated with high functioning teams (Saville et al., 2022). Dialogue act recognition is the classification of utterances based on their communicative function or intent in a conversation (Stolcke et al., 2000). When dialogue act recognition is integrated into adaptive training environments, instructors and individuals can obtain timely insights on team actions through automatic analysis of team communication behaviors. A key challenge in this process is devising reliable and accurate dialogue act recognition models.</p>
<p>Researchers have investigated a broad range of supervised learning techniques for dialogue act recognition, including probabilistic graphical modeling approaches such as Bayesian networks (Grau et al., 2004) and conditional random fields (CRF; Chen et al., 2018), support vector machines (Surendran &amp; Levow, 2006), and neural networks (Bothe et al., 2018; Firdaus et al., 2021). While these techniques have often been evaluated with real-world dialogue corpora, a significant need for the team science research community is to develop dialogue act classifiers that can assist with team communication analysis when training data are limited. </p>
<p>In our previous research, we explored an approach that combines deep neural networks with probabilistic graphical models to recognize dialogue acts in team communication, using a hybrid NLP framework (Min et al., 2021). This framework combined ELMo word embeddings with CRF to facilitate dialogue act recognition and information flow classification (i.e., the directional flow of communication within the team) with team training transcripts. Subsequently, we devised a team communication analysis framework (Pande et al., 2023) that leveraged the adaptability and transfer learning capabilities of the Text-to-Text Transfer Transformer (T5) model, an open-source language model based on the Transformer architecture (Raffel et al., 2020). This enabled us to attain high accuracy in both dialogue act recognition and information flow classification, aiding in the analysis of team communication. </p>
<p>In this article, we extend this line of research to investigate how large language models (LLMs) can support accurate team dialogue act classification in situations where labeled training data is limited. LLMs have shown outstanding predictive performance in a wide range of downstream NLP tasks (e.g., text summarization, language understanding, translation), even with limited training data (i.e., few-shot learning) or even no training data (i.e., zero-shot learning). GPT-3.5 and GPT-4 have additional advantages over other LLMs, such as the ability for users to provide verbose prompts with detailed information about tasks and the use of human feedback during training to reduce undesirable outputs (OpenAI, 2023). Because of the flexibility and the rich representational capacity afforded by the transformer architecture, LLMs offer significant versatility while also being more generalizable than previous approaches to support dialogue act classification. </p>
<p>The motivation for investigating the dialogue act recognition performance of LLMs stems from the substantial challenges encountered in crafting robust team communication analysis models under the constraint of limited labeled training data. The process of transcribing and annotating team communications from training events is notably resource- and labor-intensive. Although various tools exist to aid researchers in this endeavor, the availability of publicly accessible labeled datasets for developing and testing approaches for these tasks remains limited. Moreover, the practice among team science researchers of devising unique dialogue act labeling schemes tailored to the specific demands of each team training domain can limit the generalizability of approaches. Consequently, there is an increasing need to develop flexible NLP-driven solutions capable of supporting team communication analytics—particularly dialogue act recognition—on a scalable level. To address these challenges, we investigate the capabilities of GPT-3.5 and GPT-4 in supporting dialogue act recognition using small-sized transcripts captured from a team training exercise. We explore how prompt engineering can be used to further enhance the accuracy of these LLMs and compare model classification performance to our previous work using CRF, bidirectional long short-term memory networks, and T5-based models evaluated with the same team communication dataset. We also investigate common classification errors produced by the LLMs. </p>
<p>In particular, we explore the following research questions: </p>
<p><strong>RQ1.</strong> Can adding contextual information through iterative stages of prompt engineering enhance the dialogue act recognition performance of the GPT models?</p>
<p><strong>RQ2.</strong> What is the accuracy of the top-performing GPT models for dialogue act recognition compared to the previous state-of-the-art approach based on T5, as well as other competitive baselines?</p>
<p><strong>RQ3.</strong> What types of classification errors are most prominent in the LLMs’ inferences, and how can the performance of these NLP models be enhanced based on error analysis findings? </p>
<p>In the following sections, we introduce a generalizable team communication analysis framework designed to analyze spoken team dialogue. This framework leverages NLP models examine dialogue acts among team members, with the ultimate objective of designing adaptive training environments that employ such analytics to enhance teamwork. To address RQ1, we investigate a prompt engineering process to iteratively optimize the performance of GPT-3.5 and GPT-4 models. This method is akin to the iterative training process of human coders, who revise and refine coding guidelines after reviewing and discussing utterances that generated the most disagreement post-labeling. For RQ2, we compare the accuracy of the highest-performing LLMs against our framework’s baseline models. Specifically, we evaluate LLMs’ performance compared to models investigated in prior work, such as CRF-based probabilistic graphical models (Min et al., 2021) and T5-based models (Pande et al., 2023) for the dialogue act recognition task. Finally, for RQ3, we present an error analysis of our team communication models. This analysis informs our discussion on the implications of our work for future research, particularly in further improving NLP models’ performance and choosing suitable NLP models for reliable, real-time analysis of team communication.</p>
<p><strong>Background</strong></p>
<p><strong><em>Natural Language Processing for Education</em></strong></p>
<p>The increasing availability of large-scale textual and speech datasets has fueled advances in NLP, propelling forward a wide array of applications ranging from text summarization to translation (see e.g., Raffel et al., 2020). NLP is emerging as a cornerstone technology within the rapidly evolving landscape of educational technology, revolutionizing AI-enabled learning and learning analytics. Recent work has explored NLP’s capacity to automate and refine assessment, such as essay scoring (Ramesh &amp; Sanampudi, 2022; Ruseti et al., 2024; Yamaura et al., 2023) and short answer grading (Egaña et al., 2023; Ouahrani &amp; Bennouar, 2024; Sawatski, Schlippe, &amp; Benner-Wickner, 2022). Beyond assessment, these technologies extend to predicting learning outcomes (Alnasyan et al., 2024; Emerson et al., 2022) and goals (Gupta et al., 2022), thereby paving the way for more tailored educational experiences. Furthermore, NLP has been instrumental in analyzing classroom discourse, providing insights into the dynamics of student-teacher interactions and the cognitive underpinnings of learning processes (Jensen et al., 2021; Park et al., 2022; Wang et al., 2023). More recently, LLMs have shown significant promise for supporting student assessment (Carpenter et al., in press; Owan et al., 2023), content and scenario generation (Moore et al., 2023; Kumaran et al., 2023), and feedback and explanation generation (Morris et al., 2024; Shahriar et al., 2024). While proprietary LLMs have often achieved the state-of-the-art performance for these downstream NLP tasks, open-source LLMs offer a practical alternative, particularly for implementing scalable team training environments across diverse sessions. These offer advantages like cost-effectiveness and addressing critical ethical concerns, including data privacy. Considering these factors of accuracy, practicality, scalability, and ethics, our research explores both proprietary and open-source LLMs to assess the performance of our team communication analysis framework.</p>
<p><strong><em>Team Communication</em></strong></p>
<p>Team tasks require team members to actively share and provide information with one another to achieve a shared goal (Garosi et al., 2020; Marlow et al., 2018). Research has consistently demonstrated that effective team communication is a key factor in enhancing team performance. It contributes to improved team coordination (Grimm et al., 2022), heightened team situation awareness (Gorman et al., 2017), and facilitates the development of high levels of team cognition (Gorman et al., 2020). Effective communication also strengthens team cohesion (Stucky et al., 2020). Research shows teams whose members readily share information among themselves can recognize and anticipate the needs of one another (Cannon-Bowers et al., 1993; Entin &amp; Serfaty, 1999), adapt to stressful situations (Butchibabu et al., 2016; Su et al., 2017), and function more cohesively as a unit (Gorman et al., 2020; Stucky et al., 2020). Moreover, meta-analytic findings indicate that it is not solely the quantity of communication among team members that impacts performance, but quality of a team’s communication behaviors (Marlow et al., 2018). Therefore, examining how teams communicate and function in dynamic, real-world environments is critical for gaining insights into the processes teams use to collaborate and coordinate and for advancing theories of team development. </p>
<p>To investigate communication behaviors within teams, researchers often develop and apply content labeling schemes to transcripts of team communication. These schemes enable researchers to classify individual utterances according to their intent. Common labeling schemes include classifying utterances into statements that correspond with requesting information, providing information, demonstrating closed loop communication, providing command statements, and requesting help (Bower et al., 1998; Robinson et al., 2023). By using these label outputs researchers may perform frequency or sequence analyses to analyze team communication behaviors and gather insights into team performance during simulated activities and training events. For instance, Saville et al., (2022) used labeled transcripts to examine communication behaviors across low and high functioning military teams who completed a stressful live training exercise. They found that high-performing teams shared more information than low-performing teams and gave fewer commands during periods of high demand. As another example, Garosi et al. (2022) examined verbal communication among surgical teams to identify communication patterns that contributed to poor surgical team performance. </p>
<p>In recent years, there has been an expansion of research investigating NLP-driven approaches for dialogue act classification labeling (Min et al., 2021; Pande et al., 2023). When integrated into team training systems, automatically recognized dialogue acts can provide insights that support additional downstream tasks essential for adaptive team training.</p>
<p><strong><em>AI-Driven Multi-Party Dialogue Analysis</em></strong></p>
<p>Multi-party dialogues, compared to dyadic conversations, are complex conversational interactions involving three or more participants in a dynamic exchange of information (Rahimi &amp; Litman, 2018; Ishizaki &amp; Kato, 1998). The complexity of analyzing team communication in multi-party paradigms arises not only from the number of participants but also from concurrent conversation threads and multi-threaded discourse (Shamekhi et al., 2018; Tan et al., 2019). Analysis of multi-party dialogue is further complicated by the variability in group dynamics, where each participant may contribute differently to the conversation, necessitating sophisticated computational models that can account for these variances to reliably parse and understand the discourse (Ganesh et al., 2023; Mayfield et al., 2012; Rahimi &amp; Litman, 2018). Recent approaches to modeling multi-party dialogues have emphasized the need for contextual awareness, drawing on advanced language models and machine learning techniques that can navigate the nuanced interplay of dialogue acts and thematic thread detection (Pande et al., 2023; Wahlster, 2023). </p>
<p>Multi-party dialogues in team training environments play a pivotal role in the social construction of knowledge, where the interplay among learners or trainees can either facilitate or hinder collective learning objectives (Dillenbourg et al., 2009; Jeong et al., 2019). For example, detecting disruptive talk within these environments, which can manifest as off-task behavior or negative socio-emotional engagement, presents a significant barrier to the efficacy of these educational interactions. To address this, research has investigated various computational approaches, ranging from classic <em>n</em>-grams and random forests to more sophisticated deep learning architectures such as long short-term memory networks, enhanced with recent advances in contextualized language representations such as BERT, and attention-based user-aware neural architecture (Carpenter et al., 2020; Park et al., 2021; 2022). Such work aims not only to detect disruptive elements but also to preserve the collaborative integrity of the learning experience, by ensuring that communication remains on-task and conducive to the shared learning objectives of the group.</p>
<p>In the context of team training, deep learning-based NLP models, such as a hybrid team communication analytics model combining ELMo embeddings with CRF (Min et al., 2021), have shown promise in identifying dialogue acts and information flow during team activities to support multi-party dialogue analysis. More recently, team communication analysis models based on the T5 architecture have demonstrated high accuracy in dialogue act recognition and information flow classification tasks. These models encode both the utterances of team members and their hierarchical roles within the team using natural language processing techniques. These models outperform simple majority class baselines and hybrid models trained on the same dataset (Pande et al., 2023), which comprised utterances from live team training exercises. These transformer-based models also exceeded benchmarks from prior work for generalizing to data from a different training event.</p>
<p> In this article, we extend upon our prior research (Min et al., 2021; Pande et al., 2023) to examine the dialogue act recognition performance of GPT models and how prompt engineering strategies can be used to iteratively enhance their accuracy. We compare the accuracy of the top-performing GPT models for dialogue act recognition to previous state-of-the-art approaches based on T5, as well as other competitive baselines. Finally, we examine the types of classification errors that are most prominent in the LLMs’ predictions and discuss how the performance of these NLP models can be enhanced based on error analysis findings.</p></div></span></div></div></div><div class="tw-border-b-2 tw-border-gray-100 tw-px-4"><div class="tw-flex tw-items-start tw-justify-between tw-min-w-full"><button type="button" class="focus:tw-outline-none tw-group tw-min-w-full"><div class="tw-flex tw-items-start tw-justify-between"><div class="tw-py-2 tw-w-full"><h2 class="tw-antialiased tw-font-bold tw-text-left tw-text-blue-900 tw-text-xl">Methods</h2></div><div class="tw-py-2 sm:tw-border-r-2 sm:tw-border-white"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" aria-hidden="true" class="tw-ml-4 tw-h-7 tw-w-7 tw-flex-shrink-0 tw-rounded-full tw-text-gray-500 group-hover:tw-bg-gray-200 group-hover:tw-text-gray-900 tw-transform tw-duration-300 -tw-rotate-180"><path fill-rule="evenodd" d="M5.293 7.293a1 1 0 011.414 0L10 10.586l3.293-3.293a1 1 0 111.414 1.414l-4 4a1 1 0 01-1.414 0l-4-4a1 1 0 010-1.414z" clip-rule="evenodd"></path></svg></div></div></button></div><div class="tw-pb-1"><div class="tw-text-base tw-antialiased tw-leading-relaxed tw-text-left tw-text-black md:tw-leading-loose _fulltext-content"><span style="display:block"><div><div id="Sec6" class="Section2"> <h2>Framework</h2> <p>Initial research into automating team communication analysis has shown promising results. This research aims to present a team communication analysis framework, illustrated in Fig. <span refid="Fig1" class="InternalRef">1</span>, that explores the advanced capabilities of transformer-based LLMs, with a particular focus on GPTs. It specifically addresses dialogue act recognition, especially in scenarios with limited training data. As shown in Fig. <span refid="Fig1" class="InternalRef">1</span>, speech uttered by team members is collected by the learning environment and converted into transcripts using automatic speech recognition. Then, the framework analyzes the transcripts to produce insights about each utterance, recognizing dialogue acts in the team communication. Next, these insights are passed to another component of the learning environment, the adaptation engine, which provides adaptive scaffolding to each team member (e.g., feedback about ways in which their communication has been effective or ineffective thus far in the activity, or hints about how to communicate more effectively going forward) to help team members improve their performance. The analysis from the framework can also help researchers make decisions about how to refine the learning environment to provide even more effective adaptive scaffolding. Finally, upon the conclusion of a learning activity, the framework can help team members better understand how they performed during the activity, using the insights associated with each utterance as supporting evidence for a summary of overall performance. This article focuses on evaluating the lower part of Fig. <span refid="Fig1" class="InternalRef">1</span> (i.e., the Team Communication Analysis Framework box), which incorporates the development, evaluation, and refinement of robust team communication analysis models, as well as detailed error analysis based on the model inference results.</p> <p>  </p> </div> <div id="Sec7" class="Section2"> <h2>Training Environment and Dataset</h2> <p>To develop and evaluate our framework, we used labeled transcripts compiled from a U.S. Army training event (Johnston et al., <span citationid="CR28" class="CitationRef">2019</span>). The transcripts contain utterances captured from squad members as they completed a 45-minute live training exercise. The training exercise was designed to assess decision-making and team development under stress. Each squad was composed of nine members, including a squad leader, two team leaders, and specialists, who each fulfilled specific roles within the squad. These roles influenced their interactions with one another and the actors in the exercise. The squad leader oversaw coordinating and overseeing the operations of the squad’s subordinate teams and reporting information up the chain of command to the platoon leader. The Alpha and Bravo team leaders reported to the squad leader and were responsible for directing activities for their respective fireteams. The alpha and bravo teams each included four to five team members.  In the exercise, squads were tasked with observing and maintaining overwatch of a village, interacting with key local leaders to gain intelligence on local gang movements and activity, and maintaining security. As the mission narrative unfolded, squad members were faced with critical events such as aiding villagers, responding to tactical threats, aiding team members, and administering casualty care to squad members. These events required trainees to coordinate actions, exchange information, and provide supportive behaviors, all while fulfilling distinct roles within a structured chain of command.</p> <p>The transcribed audio logs underwent a detailed annotation process by experts, utilizing a coding scheme that categorized utterances into 27 types of dialogue acts, alongside identifying the speaker’s role within the team. This granular classification aimed to elucidate the nature of information exchange within the squads, capturing what their communicative intent or function is (i.e., dialogue acts). The dialogue act labels capture what a speaker intends to communicate in an utterance, such as information about what they see, a suggestion about what to do next, or a question for another squad member. Each utterance was labeled with its corresponding dialogue act label. Inter-rater reliability for labeling dialogue acts demonstrated a Krippendorff’s alpha of 0.679, with a raw agreement was 0.715 and an expected agreement was 0.113 (Pande et al., <span citationid="CR41" class="CitationRef">2023</span>).</p> <p>After annotation, the dialogue act labels were consolidated into a more manageable and analytically useful set to enhance analysis and intervention and to improve recognition accuracy. This resulted in nine dialogue act labels: <span type="SmallCaps" class="SmallCaps" name="Emphasis">Acknowledgement</span>, <span type="SmallCaps" class="SmallCaps" name="Emphasis">Action request</span>, <span type="SmallCaps" class="SmallCaps" name="Emphasis">Action Statement</span>, <span type="SmallCaps" class="SmallCaps" name="Emphasis">Command</span>, <span type="SmallCaps" class="SmallCaps" name="Emphasis">Attention</span>, <span type="SmallCaps" class="SmallCaps" name="Emphasis">Greeting</span>, <span type="SmallCaps" class="SmallCaps" name="Emphasis">Provide information</span>, <span type="SmallCaps" class="SmallCaps" name="Emphasis">Request information</span>, and <span type="SmallCaps" class="SmallCaps" name="Emphasis">Other</span> (Table <span refid="Tab1" class="InternalRef">1</span>). The &quot;<span type="SmallCaps" class="SmallCaps" name="Emphasis">Other</span>&quot; category encompasses utterances that do not fit any defined dialogue acts or are non-meaningful, such as incomplete statements and expletives. Analysis of the 4,315 coded utterances revealed that providing information and issuing commands were the most prevalent forms of communication, highlighting the importance of clear, directive exchanges in team operations. Table <span refid="Tab2" class="InternalRef">2</span> presents some example utterances along with their labels.</p> <p> </p><div class="gridtable"><table float="Yes" id="Tab1" border="1"> <caption language="En"> <div class="CaptionNumber">Table 1</div> <div class="CaptionContent"> <p>Frequency of dialogue act labels.</p> </div> </caption> <colgroup cols="3"> <div align="left" class="colspec" colname="c1" colnum="1"></div> <div align="left" class="colspec" colname="c2" colnum="2"></div> <div align="char" char="." class="colspec" colname="c3" colnum="3"></div> <thead> <tr> <th align="left" colname="c1"> <p>Dialogue Act Label</p> </th> <th align="left" colname="c2"> <p>Definition</p> </th> <th align="left" colname="c3"> <p>Number of Occurrences</p> </th> </tr> </thead> <tbody> <tr> <td align="left" colname="c1"> <p><span type="SmallCaps" class="SmallCaps" name="Emphasis">Provide information</span></p> </td> <td align="left" colname="c2"> <p>Providing (solicited or unsolicited) information, including repeating back information</p> </td> <td align="char" char="." colname="c3"> <p>1109</p> </td> </tr> <tr> <td align="left" colname="c1"> <p><span type="SmallCaps" class="SmallCaps" name="Emphasis">Command</span></p> </td> <td align="left" colname="c2"> <p>Issuing a command, including denying or granting permission</p> </td> <td align="char" char="." colname="c3"> <p>805</p> </td> </tr> <tr> <td align="left" colname="c1"> <p><span type="SmallCaps" class="SmallCaps" name="Emphasis">Acknowledgement</span></p> </td> <td align="left" colname="c2"> <p>Signaling understanding of the previous utterance, including simple agreement or disagreement</p> </td> <td align="char" char="." colname="c3"> <p>762</p> </td> </tr> <tr> <td align="left" colname="c1"> <p><span type="SmallCaps" class="SmallCaps" name="Emphasis">Request information</span></p> </td> <td align="left" colname="c2"> <p>Requesting information or anything not covered by <span type="SmallCaps" class="SmallCaps" name="Emphasis">Action request</span></p> </td> <td align="char" char="." colname="c3"> <p>611</p> </td> </tr> <tr> <td align="left" colname="c1"> <p><span type="SmallCaps" class="SmallCaps" name="Emphasis">Other</span></p> </td> <td align="left" colname="c2"> <p>Any utterance not covered by another dialogue act label (e.g., miscellaneous)</p> </td> <td align="char" char="." colname="c3"> <p>601</p> </td> </tr> <tr> <td align="left" colname="c1"> <p><span type="SmallCaps" class="SmallCaps" name="Emphasis">Attention</span></p> </td> <td align="left" colname="c2"> <p>Getting the attention of another team member</p> </td> <td align="char" char="." colname="c3"> <p>203</p> </td> </tr> <tr> <td align="left" colname="c1"> <p><span type="SmallCaps" class="SmallCaps" name="Emphasis">Action Statement</span></p> </td> <td align="left" colname="c2"> <p>Committing to, suggesting, or confirming completion of an action</p> </td> <td align="char" char="." colname="c3"> <p>95</p> </td> </tr> <tr> <td align="left" colname="c1"> <p><span type="SmallCaps" class="SmallCaps" name="Emphasis">Greeting</span></p> </td> <td align="left" colname="c2"> <p>Uttering greetings or pleasantries</p> </td> <td align="char" char="." colname="c3"> <p>83</p> </td> </tr> <tr> <td align="left" colname="c1"> <p><span type="SmallCaps" class="SmallCaps" name="Emphasis">Action request</span></p> </td> <td align="left" colname="c2"> <p>Requesting permission, backup, or repeat of a previous utterance</p> </td> <td align="char" char="." colname="c3"> <p>46</p> </td> </tr> </tbody> </colgroup> </table></div> <p></p> <p> </p><div class="gridtable"><table float="Yes" id="Tab2" border="1"> <caption language="En"> <div class="CaptionNumber">Table 2</div> <div class="CaptionContent"> <p>Example utterances and their dialogue act labels.</p> </div> </caption> <colgroup cols="3"> <div align="left" class="colspec" colname="c1" colnum="1"></div> <div align="left" class="colspec" colname="c2" colnum="2"></div> <div align="left" class="colspec" colname="c3" colnum="3"></div> <thead> <tr> <th align="left" colname="c1"> <p>Speaker</p> </th> <th align="left" colname="c2"> <p>Example Utterances</p> </th> <th align="left" colname="c3"> <p>Dialogue Act</p> </th> </tr> </thead> <tbody> <tr> <td align="left" colname="c1"> <p>Squad Leader</p> </td> <td align="left" colname="c2"> <p>We’re moving into town now</p> </td> <td align="left" colname="c3"> <p><span type="SmallCaps" class="SmallCaps" name="Emphasis">Action statement</span></p> </td> </tr> <tr> <td align="left" colname="c1"> <p>Alpha Team Leader</p> </td> <td align="left" colname="c2"> <p>I don’t see any humans but I do see the black truck</p> </td> <td align="left" colname="c3"> <p><span type="SmallCaps" class="SmallCaps" name="Emphasis">Provide information</span></p> </td> </tr> <tr> <td align="left" colname="c1"> <p>Squad Leader</p> </td> <td align="left" colname="c2"> <p>Roger, where is it at?</p> </td> <td align="left" colname="c3"> <p><span type="SmallCaps" class="SmallCaps" name="Emphasis">Request information</span></p> </td> </tr> <tr> <td align="left" colname="c1"> <p>Bravo Team Leader</p> </td> <td align="left" colname="c2"> <p>Eyes up for wires on that bridge</p> </td> <td align="left" colname="c3"> <p><span type="SmallCaps" class="SmallCaps" name="Emphasis">Command</span></p> </td> </tr> <tr> <td align="left" colname="c1"> <p>Squad Leader</p> </td> <td align="left" colname="c2"> <p>Go ahead and shift security to bravo four</p> </td> <td align="left" colname="c3"> <p><span type="SmallCaps" class="SmallCaps" name="Emphasis">Command</span></p> </td> </tr> </tbody> </colgroup> </table></div> <p></p> <p>Prior studies that used this labeled data found that high-performing squads shared information more frequently and issued more commands to teammates compared to their lower-performing counterparts (Saville et al., 2021). Additionally, high-performing teams maintained consistent rates of information exchange regardless of scenario difficulty, while low-performing teams showed significant increases in commands and information exchange during stressful situations (Saville et al., <span citationid="CR52" class="CitationRef">2022</span>). These investigations not only confirm the critical role of communication in team performance but also demonstrate the value of using dialogue act analysis to investigate communication behaviors.  Thus, accurate classification of utterances according to dialogue act labels can support more rigorous assessment of team performance and richer adaptive team training systems.</p> </div> <div id="Sec8" class="Section2"> <h2>Baseline Models</h2> <p>To identify the highest-performing models for dialogue act recognition for the dataset, we evaluate four different machine learning methods with our team communication analysis framework. First, we describe three models grounded in our prior work, including CRF (Min et al., 2021), bidirectional long short-term memory networks (Min et al., 2021), and Text-To-Text Transfer Transformers (Pande et al., <span citationid="CR41" class="CitationRef">2023</span>). Then, we describe generative pre-trained transformers (GPTs), which are refined through an iterative process of prompt engineering and compared to the previous approaches.</p> </div> <div id="Sec9" class="Section2"> <h2>Baseline 1: Conditional Random Fields Models</h2> <p>In our prior work we investigated linear-chain CRFs models with deep learning-based inputs. These models combine probabilistic graphical models’ inference capacity and deep neural networks’ representation capacity for team analytics, aiming to categorize team communication into dialogue acts (Min et al., 2021). CRFs demonstrated notable utility in structured inference and sequence modeling, employing undirected graphical models for classifying multivariate data. They are particularly effective at capturing dependencies between predictive features and their associated class labels. While conventional representation techniques for natural language, such as bag-of-words, term frequency-inverse document frequency (TF-IDF), and GloVe (Pennington et al., <span citationid="CR44" class="CitationRef">2014</span>), use static feature vectors or word embeddings, these approaches often fall short in capturing the semantics and syntactics inherent in dialogue. To overcome this limitation, our research explored the use of ELMo embeddings (Peters et al., <span citationid="CR45" class="CitationRef">2018</span>), contextual representations generated by a bidirectional long short-term memory network, which were integrated into the CRFs as utterance-level input.</p> <p>To develop CRF models, we used a pre-trained ELMo model to generate an embedding of each utterance. This ELMo model was built with stacked bidirectional LSTMs trained on the 1 Billion Word Benchmark, approximately 800M tokens of news crawl data from WMT 2011.  The ELMo model provided a 1,024-dimensional embedding of each word in an utterance, and then we averaged each dimension over all words to obtain an utterance-level embedding. Due to the large dimensionality of these ELMo embeddings, we applied principal component analysis (PCA) to reduce the 1,024 dimensions down to one of 32, 64, or 128 dimensions, identifying the optimal reduced dimension through cross-validation. Finally, the PCA-transformed utterance features were concatenated with the speaker role-based features, which are represented using one-hot encoding, to collectively serve as the input features for the CRF models.</p> <p>We capped the input sequence length at 100 and limited training to 100 iterations for training CRF models. To determine the best-performing CRF models through cross-validation, we carried out hyperparameter optimization, focusing on three hyperparameters: the regularization parameter from the set {0.1, 0.5, 1}, the optimizer convergence tolerance from the set {0.01, 0.001}, and the reduced PCA dimensions from the set {32, 64, 128}. We employed PyStruct (Müller &amp; Behnke, <span citationid="CR37" class="CitationRef">2014</span>), a Python-based, off-the-shelf CRF modeling library, for training the models. The optimal hyperparameter values were identified through a cross-validation process per classification task. Utilizing the identified optimal hyperparameter values, the final models for dialogue act recognition were trained using the entire training set and subsequently evaluated using a held-out test set.</p> <div id="Sec10" class="Section3"> <h2>Baseline 2: Bi-directional Long Short-Term Memory Networks</h2> <p>Secondly, our investigation extended to two-layer bidirectional long short-term memory networks (BLSTMs) (Graves &amp; Schmidhuber, <span citationid="CR21" class="CitationRef">2005</span>) that utilized as input the same ELMo embeddings as the CRFs (Min et al., 2021). The choice to employ stacked BLSTMs over simpler, single-layer LSTMs was influenced by preliminary evidence indicating that stacked configurations, capable of capturing the bidirectional context of speaker utterances through their multi-layer architecture, were more effective in dialogue act classification. Specifically, we explored the capabilities of multi-task BLSTMs aiming to harness the benefits of models designed for effectively handling multiple, interrelated tasks, in addition to single-task BLSTMs (which handle dialogue act recognition task only). To achieve this goal, the multi-task models utilized labels for information flow in the communication data used for our dialogue act recognition. Information flow indicates the direction of information being passed (e.g., up or down the chain of command) during communication. Prior research suggests that the nuances in communication—specifically, the frequency and type of dialogue acts and information flows—correlate strongly with team performance levels (Saville et al., 2021; <span citationid="CR52" class="CitationRef">2022</span>). These findings underscore the critical role of information flow in understanding team dynamics and outcomes, which exhibits significant potential to model together with dialogue acts through multi-task modeling. For more details about the information flow labels, refer to our previous work (Min et al., 2021; Pande et al., <span citationid="CR41" class="CitationRef">2023</span>).</p> <p>Furthermore, we delved into two fusion techniques—early fusion and late fusion—to refine our modeling strategy based on different sets of input features (utterance-based and speaker role-based). In early fusion, PCA-reduced ELMo representations of the speaker’s utterance and one-hot-encoded speaker role representations were merged into a single vector, then fed into the BLSTMs. Late fusion, on the other hand, involves running two separate BLSTMs for the two distinct sets of input features, with their outputs later merged for classification in a softmax layer. These approaches were evaluated using the same PCA-reduced dimensionality options (32, 64, or 128) as those used in the CRF models.</p> <p>For the BLSTM layers, we opted for 32 hidden units and incorporated a dropout rate of 0.25 to regularize the models, employed the softmax function for output layer activations, and utilized the Adam optimizer (Kingma &amp; Ba, <span citationid="CR30" class="CitationRef">2014</span>). Echoing the configuration for CRF models, we capped the input sequence length at 100 and limited training to a maximum of 100 epochs. The models employed an early stopping mechanism triggered after 10 epochs (i.e., patience duration) without improvement in validation loss, calculated using 10% of the training data, to optimize the training process.</p> </div> </div> <div id="Sec11" class="Section2"> <h2>Baseline 3: Text-to-Text Transfer Transformer Models</h2> <p>Third, we explored the capabilities of T5 for dialogue act recognition (Pande et al., <span citationid="CR41" class="CitationRef">2023</span>). T5 is a text-to-text transfer Transformer, renowned for its superior performance across a range of NLP tasks such as machine translation, abstractive summarization, and comprehensive language understanding (Raffel et al., <span citationid="CR46" class="CitationRef">2020</span>). T5’s architecture employs multi-head self-attention mechanisms grounded in the Transformer architecture to intricately understand the relationships between tokens in a sequence, allowing it to consider the context of each token irrespective of its position. This is achieved through parallel computations across multiple heads, each specializing in different aspects of token relationships by projecting the sequence into a lower-dimensional space. Specifically, the model structure includes an encoder and a decoder, each with six layers featuring multi-head attention and a feedforward neural network. The encoder computes intermediate representations of inputs, and the decoder generates output tokens based on the encoder’s computations and previous outputs. The model was pre-trained in two stages: unsupervised learning on the Colossal Clean Crawled Corpus (C4) and supervised fine-tuning on NLP benchmarks, using a denoising objective for the initial unsupervised phase.</p> <p>We selected the “small” variant of T5, which comprises 60 million parameters, due to its efficient training capabilities, making it ideal for application in scenarios with limited resources. This model is capable of processing inputs up to 512 tokens in length, utilizing a vocabulary of 32,128 distinct tokens. In addition, our T5 model utilized eight heads in its multi-head attention sublayers, projecting sequences down to 64 dimensions in each head. We formatted inputs to include both the speaker role and utterance, using task-specific prefixes to guide the T5 model’s classification processes. For instance, an utterance by the bravo team leader was formatted as <em>b_tmldr: Keep an eye on everybody</em>. As each speaker has a unique task-specific prefix, T5 can be readily trained to tailor how it classifies utterances based on who is speaking. The training examples formatted with this method are used to fine-tune the pre-trained T5-small models to fit our purposes for dialogue act recognition.</p> <p>Through cross-validation, we determined the optimal hyperparameter values for each model, involving learning rate, warm-up steps, and batch size adjustments. Following the process utilized for training the CRF and BLSTM models, this method reserved one squad’s data as the same held-out test set, while utilizing the rest for training and validation, applying early stopping with a patience of 10 epochs to refine our training process. Our evaluation focused on achieving the highest macro-average classification accuracy across all data folds, aiming to enhance the precision of our dialogue analyses.</p> </div> <div id="Sec12" class="Section2"> <h2>Generative Pre-Trained Transformer Models</h2> <p>We investigate the capabilities of generative pre-trained Transformer models, focusing on GPT-3.5 and GPT-4, proprietary LLMs developed by OpenAI (<span citationid="CR38" class="CitationRef">2023</span>). These models, which are transformer-based models pre-trained to predict the next token in a document, have marked a significant evolution in computational linguistics and NLP, demonstrating their prowess across a variety of benchmarks, including dialogue analysis, text summarization, and machine translation. Notably, GPT-4 has demonstrated exceptional capabilities, placing in the top 10% of participants in simulated tests like the bar exam. This ability to understand and generate natural language in complex scenarios underscores the advanced capabilities of these models. Differing from the other models we have discussed, these proprietary LLMs leverage vast linguistic datasets for training and can often forego traditional fine-tuning on specific datasets. Instead, they employ in-context learning methods through prompting, including zero-shot learning (e.g., using only label definitions) and few-shot learning (e.g., combining a minimal number of labeled examples with label definitions), to match or surpass the predictive accuracy of other established models.</p> </div></div></span></div></div></div><div class="tw-border-b-2 tw-border-gray-100 tw-px-4"><div class="tw-flex tw-items-start tw-justify-between tw-min-w-full"><button type="button" class="focus:tw-outline-none tw-group tw-min-w-full"><div class="tw-flex tw-items-start tw-justify-between"><div class="tw-py-2 tw-w-full"><h2 class="tw-antialiased tw-font-bold tw-text-left tw-text-blue-900 tw-text-xl">Results</h2></div><div class="tw-py-2 sm:tw-border-r-2 sm:tw-border-white"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" aria-hidden="true" class="tw-ml-4 tw-h-7 tw-w-7 tw-flex-shrink-0 tw-rounded-full tw-text-gray-500 group-hover:tw-bg-gray-200 group-hover:tw-text-gray-900 tw-transform tw-duration-300 -tw-rotate-180"><path fill-rule="evenodd" d="M5.293 7.293a1 1 0 011.414 0L10 10.586l3.293-3.293a1 1 0 111.414 1.414l-4 4a1 1 0 01-1.414 0l-4-4a1 1 0 010-1.414z" clip-rule="evenodd"></path></svg></div></div></button></div><div class="tw-pb-1"><div class="tw-text-base tw-antialiased tw-leading-relaxed tw-text-left tw-text-black md:tw-leading-loose _fulltext-content"><span style="display:block"><div><p>We conducted an evaluation of our team communication analysis framework with four different modeling techniques by dividing the dataset into two distinct segments: data from five squads was allocated for cross-validation, while data from an additional squad was reserved for held-out testing. This process enabled us to select the most effective hyperparameters for each model, which were those that demonstrated the highest average accuracy rate during cross-validation. The highest performing model from each modeling technique is evaluated with the held-out test set, which was not exposed to the cross-validation phase, ensuring an unbiased assessment of the models’ ability to generalize. For a fair comparison, we used the same data split between the training set and test set across all the models.</p> <p>The results from cross-validation for accuracy across all configurations indicate that machine learning models investigated for our team communication framework significantly surpasses the majority-class baseline performance (25.7% in dialogue act recognition) under all hyperparameter settings. Table 3 shows the cross-validation performance of the CRFs, BLSTMs, T5s. The results reveal that the best-performing model for dialogue act recognition featured (1) an optimizer regularization parameter of 0.1, an optimizer convergence tolerance of 0.01, and PCA dimensions of 32 for CRFs, (2) multi-task learning, early fusion, and PCA dimensions of 64 for BLSTMs, and (3) a learning rate of 3e-4, a batch size of eight, and warm-up steps constituting 10% of the total training data instances for T5s. The best-performing model hyperparameters, identified through cross-validation for each machine learning algorithm, were applied to the entire training set to develop the final classifiers, which were then assessed using the held-out test set. These fine-tuned, best-performing CRF, BLSTM, and T5 models were then compared against the performance of GPT-3.5 and GPT-4.</p> <p>​​<b>Table 3.</b> Averaged cross-validation accuracy rates (%) for CRF, BLSTM, and T5. The highest predictive accuracy rates for dialogue act per modeling technique are marked in bold.</p> <p> </p><div class="gridtable"><table float="No" id="Taba" border="1"> <colgroup cols="7"> <div align="left" class="colspec" colname="c1" colnum="1"></div> <div align="left" class="colspec" colname="c2" colnum="2"></div> <div align="left" class="colspec" colname="c3" colnum="3"></div> <div align="left" class="colspec" colname="c4" colnum="4"></div> <div align="left" class="colspec" colname="c5" colnum="5"></div> <div align="left" class="colspec" colname="c6" colnum="6"></div> <div align="left" class="colspec" colname="c7" colnum="7"></div> <thead> <tr> <th align="left" colname="c1"> <p>CRF</p> </th> <th align="left" colname="c2"> <p>Accuracy Rates</p> <p>(%)</p> </th> <th align="left" colname="c3"> <p>BLSTM</p> </th> <th align="left" colname="c4"> <p>Accuracy Rates</p> <p>(%)</p> </th> <th align="left" colname="c5"> <p>T5</p> </th> <th align="left" colname="c6"> <p>Accuracy Rates</p> <p>(%)</p> </th> <th align="left" colname="c7"> </th> </tr> </thead> <tbody> <tr> <td align="left" colname="c1"> <p>{0.1, 0.001, 32}</p> </td> <td align="left" colname="c2"> <p>68.80</p> </td> <td align="left" colname="c3"> <p>{Multi-task, Early, 32}</p> </td> <td align="left" colname="c4"> <p>61.44</p> </td> <td align="left" colname="c5"> <p>{1e-4, 4, 5%}</p> </td> <td align="left" colname="c6"> <p>72.69</p> </td> <td align="left" colname="c7"> </td> </tr> <tr> <td align="left" colname="c1"> <p>{0.1, 0.001, 64}</p> </td> <td align="left" colname="c2"> <p>67.88</p> </td> <td align="left" colname="c3"> <p>{Multi-task, Early, 64}</p> </td> <td align="left" colname="c4"> <p><b>62.07</b></p> </td> <td align="left" colname="c5"> <p>{1e-4, 4, 10%}</p> </td> <td align="left" colname="c6"> <p>72.77</p> </td> <td align="left" colname="c7"> </td> </tr> <tr> <td align="left" colname="c1"> <p>{0.1, 0.001, 128}</p> </td> <td align="left" colname="c2"> <p>64.85</p> </td> <td align="left" colname="c3"> <p>{Multi-task, Early, 128}</p> </td> <td align="left" colname="c4"> <p>61.97</p> </td> <td align="left" colname="c5"> <p>{1e-4, 8, 5%}</p> </td> <td align="left" colname="c6"> <p>72.91</p> </td> <td align="left" colname="c7"> </td> </tr> <tr> <td align="left" colname="c1"> <p>{0.1, 0.01, 32}</p> </td> <td align="left" colname="c2"> <p><b>68.88</b></p> </td> <td align="left" colname="c3"> <p>{Multi-task, Late, 32}</p> </td> <td align="left" colname="c4"> <p>60.22</p> </td> <td align="left" colname="c5"> <p>{1e-4, 8, 10%}</p> </td> <td align="left" colname="c6"> <p>72.28</p> </td> <td align="left" colname="c7"> </td> </tr> <tr> <td align="left" colname="c1"> <p>{0.1, 0.01, 64}</p> </td> <td align="left" colname="c2"> <p>67.87</p> </td> <td align="left" colname="c3"> <p>{Multi-task, Late, 64}</p> </td> <td align="left" colname="c4"> <p>60.19</p> </td> <td align="left" colname="c5"> <p>{3e-4, 4, 5%}</p> </td> <td align="left" colname="c6"> <p>71.90</p> </td> <td align="left" colname="c7"> </td> </tr> <tr> <td align="left" colname="c1"> <p>{0.1, 0.01, 128}</p> </td> <td align="left" colname="c2"> <p>64.87</p> </td> <td align="left" colname="c3"> <p>{Multi-task, Late, 128}</p> </td> <td align="left" colname="c4"> <p>59.77</p> </td> <td align="left" colname="c5"> <p>{3e-4, 4, 10%}</p> </td> <td align="left" colname="c6"> <p>71.98</p> </td> <td align="left" colname="c7"> </td> </tr> <tr> <td align="left" colname="c1"> <p>{1.0, 0.001, 32}</p> </td> <td align="left" colname="c2"> <p>68.76</p> </td> <td align="left" colname="c3"> <p>{Single-task, Early, 32}</p> </td> <td align="left" colname="c4"> <p>62.04</p> </td> <td align="left" colname="c5"> <p>{3e-4, 8, 5%}</p> </td> <td align="left" colname="c6"> <p>73.33</p> </td> <td align="left" colname="c7"> </td> </tr> <tr> <td align="left" colname="c1"> <p>{1.0, 0.001, 64}</p> </td> <td align="left" colname="c2"> <p>67.45</p> </td> <td align="left" colname="c3"> <p>{Single-task, Early, 64}</p> </td> <td align="left" colname="c4"> <p>61.16</p> </td> <td align="left" colname="c5"> <p>{3e-4, 8, 10%}</p> </td> <td align="left" colname="c6"> <p><b>73.55</b></p> </td> <td align="left" colname="c7"> </td> </tr> <tr> <td align="left" colname="c1"> <p>{1.0, 0.001, 128}</p> </td> <td align="left" colname="c2"> <p>65.72</p> </td> <td align="left" colname="c3"> <p>{Single-task, Early, 128}</p> </td> <td align="left" colname="c4"> <p>61.84</p> </td> <td align="left" colname="c5"> </td> <td align="left" colSpan="2" nameend="c7" namest="c6"> </td> </tr> <tr> <td align="left" colname="c1"> <p>{1.0, 0.01, 32}</p> </td> <td align="left" colname="c2"> <p>68.83</p> </td> <td align="left" colname="c3"> <p>{Single-task, Late, 32}</p> </td> <td align="left" colname="c4"> <p>61.48</p> </td> <td align="left" colname="c5"> </td> <td align="left" colSpan="2" nameend="c7" namest="c6"> </td> </tr> <tr> <td align="left" colname="c1"> <p>{1.0, 0.01, 64}</p> </td> <td align="left" colname="c2"> <p>67.45</p> </td> <td align="left" colname="c3"> <p>{Single-task, Late, 64}</p> </td> <td align="left" colname="c4"> <p>60.53</p> </td> <td align="left" colname="c5"> </td> <td align="left" colSpan="2" nameend="c7" namest="c6"> </td> </tr> <tr> <td align="left" colname="c1"> <p>{1.0, 0.01, 128}</p> </td> <td align="left" colname="c2"> <p>65.62</p> </td> <td align="left" colname="c3"> <p>{Single-task, Late, 128}</p> </td> <td align="left" colname="c4"> <p>61.96</p> </td> <td align="left" colname="c5"> </td> <td align="left" colSpan="2" nameend="c7" namest="c6"> </td> </tr> </tbody> </colgroup> </table></div> <p></p> <p> <sup>1</sup>CRF parameters: <em>optimizer regularization parameter</em>, <em>optimizer convergence tolerance</em>, <em>reduced PCA dimensions.</em></p> <p> <sup>2</sup>BLSTM parameters: {<em>task modeling type</em>, <em>fusion mode</em>, <em>reduced PCA dimensions</em>}</p> <p> <sup>3</sup>T5 parameters: {<em>learning rate, batch size, warm-up steps</em>}</p> <p> <b>RQ1.</b> Can adding contextual information through iterative stages of prompt engineering enhance the dialogue act recognition performance of the GPT models?</p> <p>To address our first research question, we devised a multi-stage engineering process for dialogue act recognition with GPT 3.5, wherein we iteratively refined the classification prompts across four stages (Table <span refid="Tab3" class="InternalRef">4</span>), starting with a foundational prompt in the first stage that introduced definitions for all the communication categories (i.e., all classes in the classification task). This was followed by the second prompt engineering stage, which added illustrative examples for each category to clarify distinctions. The third prompt engineering stage focused on resolving ambiguities by providing additional scenario context and participant roles. The final prompt engineering stage addressed specific confusions between misclassified labels by incorporating detailed clarifications generated by GPT, alongside enriched contextual details. This progressive refinement aimed to enhance the model’s understanding and accurate classification of complex military dialogues, showcasing a nuanced approach to improving NLP tasks through targeted prompt adjustments. The OpenAI models utilized were “gpt-4” and “gpt-3.5-turbo,” with a temperature setting of 0.2. We opted for a lower temperature to minimize randomness and steer the model’s output to be more deterministic and focused.</p> <p> </p><div class="gridtable"><table float="Yes" id="Tab3" border="1"> <caption language="En"> <div class="CaptionNumber">Table 4</div> <div class="CaptionContent"> <p>Stages of improvement illustrated by prompt text snippets.</p> </div> </caption> <colgroup cols="2"> <div align="left" class="colspec" colname="c1" colnum="1"></div> <div align="left" class="colspec" colname="c2" colnum="2"></div> <thead> <tr> <th align="left" colname="c1"> <p>Prompt Improvement Stages</p> </th> <th align="left" colname="c2"> <p>Prompt Text Snippets</p> </th> </tr> </thead> <tbody> <tr> <td align="left" colname="c1"> <p>Stage 1: Label definition</p> </td> <td align="left" colname="c2"> <p>&quot;Command&quot;: {{</p> <p>&quot;command&quot;: &quot;One user issues a command to another&quot;,</p> <p>&quot;grant-permission&quot;: &quot;Grants permission to perform requested action&quot;,</p> <p>&quot;deny-permission&quot;: &quot;Denies permission to perform requested action&quot;</p> <p>}}</p> </td> </tr> <tr> <td align="left" colname="c1"> <p>Stage 2: Category examples</p> </td> <td align="left" colname="c2"> <p>Examples:</p> <p>&#x27;speaker&#x27;:&quot;Bravo Team Leader&quot;, &#x27;text&#x27;: &quot;alright yeah over&quot;, &#x27;label&#x27;: &quot;Ack&quot;</p> <p>&#x27;speaker&#x27;:&quot;Bravo Team Leader&quot;, &#x27;text&#x27;: &quot;a little sketchy yeah?&quot;, &#x27;label&#x27;: &quot;Ack&quot;</p> <p>&#x27;speaker&#x27;:&quot;Squad Leader&quot;, &#x27;text&#x27;: &quot;request QRF time now&quot;, &#x27;label&#x27;: &quot;Action-Request&quot;</p> <p>&#x27;speaker&#x27;:&quot;Platoon Leader&quot;, &#x27;text&#x27;: &quot;say again 3–3&quot;, &#x27;label&#x27;: &quot;Action-Request&quot;</p> </td> </tr> <tr> <td align="left" colname="c1"> <p>Stage 3: Added context and Command label clarification</p> </td> <td align="left" colname="c2"> <p>Context:</p> <p>During the virtual army training scenario, Soldiers found themselves in a simulated environment with a tree line providing cover as they observed a village featuring a market square, multi-story buildings, and a shattered church. Within the market square, Soldiers took on the roles of villagers. Notably, one Soldier assumed the role of a parish priest who possessed crucial information about insurgents.</p> </td> </tr> <tr> <td align="left" colname="c1"> <p>Stage 4: Inter-label clarification</p> </td> <td align="left" colname="c2"> <p>&#x27;Command&#x27; vs &#x27;Provide-Info&#x27;: &#x27;Command&#x27; (&quot;Secure the perimeter now&quot;) is an order, while &#x27;Provide-Info&#x27; (&quot;The perimeter appears unguarded&quot;) is merely sharing information.</p> <p>&#x27;Command&#x27; vs &#x27;Request-Info&#x27;: &#x27;Command&#x27; (&quot;Report your position&quot;) is an order, whereas &#x27;Request-Info&#x27; (&quot;Are you in position?&quot;) seeks information.</p> </td> </tr> </tbody> </colgroup> </table></div> <p></p> <p>The results of our incremental prompting approach are detailed in Table <span refid="Tab4" class="InternalRef">5</span>. We utilized an incremental approach to refine the prompt used on a training dataset, subsequently testing its efficacy on a separate held-out test dataset. As described above, initially, the prompt was structured around a coding sheet employed for manual labeling, comprising a “Definitions” section derived from the textual content of the coding sheet (Stage 1 in Table <span refid="Tab3" class="InternalRef">4</span>) and an “Instructions” section outlining the task and the utterances to be labeled. This initial setup yielded an accuracy of 0.369. To improve on these results, we integrated examples from the training set into the prompt—specifically, four examples per class with their corresponding labels (i.e., few-shot learning), as shown in Stage 2 in Table <span refid="Tab3" class="InternalRef">4</span>. This modification led to an improved system accuracy of approximately 0.425. However, we chose not to pursue the few-shot learning strategy further, as it only marginally improved accuracy while significantly increasing the prompt size, opting instead to focus on zero-shot learning.</p> <p>To enhance the classifier&#x27;s accuracy for participant utterances, we supplemented the Stage 1 prompt with a “Context” section (Stage 3 in Table <span refid="Tab3" class="InternalRef">4</span>). This addition includes details of the training exercise, the scenario encountered by participants, the character ensemble, and the geographical setting of the village. By offering a high-level overview of the exercise’s events, clarifying the roles of squad members, and detailing the team&#x27;s leadership structure, this broad context improves the model’s ability to interpret the intent and relevance of the often brief utterances within the exercise. In addition, our analysis pinpointed that the label “Command,” despite being the second most frequent in the dataset, exhibited the lowest F1 score at 0.12.</p> <p> </p><div class="gridtable"><table float="Yes" id="Tab4" border="1"> <caption language="En"> <div class="CaptionNumber">Table 5</div> <div class="CaptionContent"> <p>Iterative prompt refinements for GPT dialogue act recognition models based on evaluations using the training set.</p> </div> </caption> <colgroup cols="4"> <div align="left" class="colspec" colname="c1" colnum="1"></div> <div align="left" class="colspec" colname="c2" colnum="2"></div> <div align="left" class="colspec" colname="c3" colnum="3"></div> <div align="left" class="colspec" colname="c4" colnum="4"></div> <thead> <tr> <th align="left" colname="c1"> <p>Prompt Content and Details</p> </th> <th align="left" colname="c2"> <p>Accuracy</p> </th> <th align="left" colname="c3"> <p>F1 Score (macro)</p> </th> <th align="left" colname="c4"> <p>Matthews Correlation Coefficient (MCC)</p> </th> </tr> </thead> <tbody> <tr> <td align="left" colname="c1"> <p>Simple coding sheet (GPT 3.5)</p> </td> <td align="left" colname="c2"> <p>0.369</p> </td> <td align="left" colname="c3"> <p>0.39</p> </td> <td align="left" colname="c4"> <p>0.344</p> </td> </tr> <tr> <td align="left" colname="c1"> <p>Coding sheet with examples (GPT 3.5)</p> </td> <td align="left" colname="c2"> <p>0.425</p> </td> <td align="left" colname="c3"> <p>0.30</p> </td> <td align="left" colname="c4"> <p>0.403</p> </td> </tr> <tr> <td align="left" colname="c1"> <p>&quot;Command&quot; clarification + Training Context (GPT 3.5)</p> </td> <td align="left" colname="c2"> <p>0.600</p> </td> <td align="left" colname="c3"> <p>0.47</p> </td> <td align="left" colname="c4"> <p>0.558</p> </td> </tr> <tr> <td align="left" colname="c1"> <p>With inter-class clarification (GPT 3.5)</p> </td> <td align="left" colname="c2"> <p>0.631</p> </td> <td align="left" colname="c3"> <p>0.44</p> </td> <td align="left" colname="c4"> <p>0.541</p> </td> </tr> <tr> <td align="left" colname="c1"> <p>With inter-class clarification (GPT 4)</p> </td> <td align="left" colname="c2"> <p><b>0.781</b></p> </td> <td align="left" colname="c3"> <p><b>0.64</b></p> </td> <td align="left" colname="c4"> <p><b>0.730</b></p> </td> </tr> </tbody> </colgroup> </table></div> <p></p> <p>To address the mislabeling issue, we utilized GPT-4 to generate clarification for the label definitions, leveraging its outstanding performance in language comprehension and generation. It is trained on a larger dataset that incorporates more diverse training data, enhancing its ability to understand language constructs and generate context-aware text (OpenAI, <span citationid="CR38" class="CitationRef">2023</span>). To clarify the prompt for the “Command” label, we created a separate prompt improvement request incorporating mislabeled examples and the existing coding definition as context. The clarification generated by the GPT-4 model regarding the “Command” coding definition was used to improve the original prompt. This clarification, along with the new “Context” section detailing the training scenario and team roles, and including the speaker&#x27;s role in the utterances, notably enhanced the accuracy to 0.6 and raised the “Command” label’s F1 score to 0.75 for GPT-3.5.</p> <p>Further refinement (Stage 4 in Table <span refid="Tab3" class="InternalRef">4</span>) was achieved by examining the confusion matrix of the classifier and instructing GPT-4 to create a clarification prompt to distinguish between the two most conflated classes: “Provide-Info” and “Command.” Adding this clarification to the prompt boosted the accuracy to 0.631 when utilizing the GPT-3.5 model for classification. Transitioning the base inference model to GPT-4, we observed a significant accuracy increase to 0.781 for the training dataset (Table <span refid="Tab4" class="InternalRef">5</span>).</p> <p> </p><div class="gridtable"><table float="Yes" id="Tab5" border="1"> <caption language="En"> <div class="CaptionNumber">Table 6</div> <div class="CaptionContent"> <p>The dialogue act recognition performance of the highest-performing GPT-3.5 and GPT-4 models on the test set.</p> </div> </caption> <colgroup cols="4"> <div align="left" class="colspec" colname="c1" colnum="1"></div> <div align="left" class="colspec" colname="c2" colnum="2"></div> <div align="left" class="colspec" colname="c3" colnum="3"></div> <div align="left" class="colspec" colname="c4" colnum="4"></div> <thead> <tr> <th align="left" colname="c1"> <p>Prompt Content and Details</p> </th> <th align="left" colname="c2"> <p>Accuracy</p> </th> <th align="left" colname="c3"> <p>F1 Score (macro)</p> </th> <th align="left" colname="c4"> <p>Mathews Correlation Coefficient (MCC)</p> </th> </tr> </thead> <tbody> <tr> <td align="left" colname="c1"> <p>With inter-class clarification (GPT 3.5)</p> </td> <td align="left" colname="c2"> <p>0.669</p> </td> <td align="left" colname="c3"> <p>0.47</p> </td> <td align="left" colname="c4"> <p>0.600</p> </td> </tr> <tr> <td align="left" colname="c1"> <p>With inter-class clarification (GPT 4)</p> </td> <td align="left" colname="c2"> <p><b>0.773</b></p> </td> <td align="left" colname="c3"> <p><b>0.62</b></p> </td> <td align="left" colname="c4"> <p><b>0.727</b></p> </td> </tr> </tbody> </colgroup> </table></div> <p></p> <p> <b>RQ2.</b> What is the accuracy of the top-performing GPT models for dialogue act recognition compared to the previous state-of-the-art approach based on T5, as well as other competitive baselines?</p> <p>As Stage 4 of the prompt engineering process achieved the highest dialogue act recognition results, we further evaluated the generalization performance of both GPT-3.5 and GPT-4 models using this prompt with the held-out test set. As demonstrated in Table <span refid="Tab5" class="InternalRef">6</span>, the Stage 4 prompt-based GPT models achieved classification accuracy of 0.669 and 0.773 for GPT-3.5 and GPT-4, respectively. Table <span refid="Tab6" class="InternalRef">7</span> shows the accuracy rates of the dialogue act recognition models, including CRF, BLSTM, T5, and GPT-4, on the same held-out test set. GPT-4, augmented with inter-class clarification based on zero-shot learning, achieved the highest dialogue act recognition accuracy at 77.30%, surpassing three competitive baselines. Notably, the GPT-4 model outperformed the previous state-of-the-art models using T5 (Pande et al., <span citationid="CR41" class="CitationRef">2023</span>) with a normalized accuracy improvement of 5.73%. It surpassed the performance of the CRF and BLSTM baselines (Min et al., 2021) as well, and did not require fine-tuning with training data or extensive hyperparameter optimization.</p> <p> </p><div class="gridtable"><table float="Yes" id="Tab6" border="1"> <caption language="En"> <div class="CaptionNumber">Table 7</div> <div class="CaptionContent"> <p>Test accuracy rates (%) for best performing CRF, BLSTM, T5, and GPT models.</p> </div> </caption> <colgroup cols="2"> <div align="left" class="colspec" colname="c1" colnum="1"></div> <div align="left" class="colspec" colname="c2" colnum="2"></div> <thead> <tr> <th align="left" colname="c1"> <p>Model</p> </th> <th align="left" colname="c2"> <p>Dialogue Act Recognition</p> </th> </tr> </thead> <tbody> <tr> <td align="left" colname="c1"> <p><b>CRF</b></p> </td> <td align="left" colname="c2"> <p>69.42</p> </td> </tr> <tr> <td align="left" colname="c1"> <p><b>BLSTM</b></p> </td> <td align="left" colname="c2"> <p>64.61</p> </td> </tr> <tr> <td align="left" colname="c1"> <p><b>T5</b></p> </td> <td align="left" colname="c2"> <p>75.92</p> </td> </tr> <tr> <td align="left" colname="c1"> <p><b>GPT-4</b></p> </td> <td align="left" colname="c2"> <p>77.30</p> </td> </tr> </tbody> </colgroup> </table></div> <p></p> <p> <b>RQ3.</b> What types of classification errors are most prominent in the LLMs’ inferences, and how can the performance of these NLP models be enhanced based on error analysis findings?</p> <p>Based on the dialogue act recognition accuracy results, we delved deeper into two highest-performing models (i.e., GPT-4, T5) through an error analysis to scrutinize the labels recognized by our framework in finer detail. To achieve this, we constructed confusion matrices based on the models’ classification results, aiming to delineate the discrepancies between the inferred labels and the true labels within our test dataset. In Fig. <span refid="Fig2" class="InternalRef">2</span>, the heatmaps of the confusion matrices for the GPT-4 and T5 dialogue act recognition models are shown left and right, respectively, across the nine dialogue acts. The element at position (<em>i</em>, <em>j</em>) within each matrix denotes the count of instances where the true label corresponds to row <em>i</em>, while the classifier&#x27;s inferred label corresponds to column <em>j</em>. Correct inferences by the classifier are represented by the counts along the matrix’s principal diagonal, where the inferred label matches the true label. Conversely, all other matrix entries represent instances of misclassification.</p> <p>In Fig. <span refid="Fig2" class="InternalRef">2</span>, the support (i.e., the number of actual instances of each category) for each true label in the dataset is used to scale the values (i.e., normalization per row). Notably, the mislabeling patterns are similar for both classifiers. “Provide-Info,” “Request-Info,” and “Other” are the labels most commonly confused by both classifiers. We suspect that the short sentences used in the communication during training challenge the classifiers’ ability to distinguish nuances among these labels. Further investigation of the “Other” label reveals that both classifiers had difficulty recognizing it correctly; however, the fine-tuned T5 classifier, with a recall rate of 0.54, performed slightly worse than GPT-4, which had a recall rate of 0.67. A potential explanation for this underperformance is that both models encounter challenges learning the characteristics of utterances with the “Other” label, as this label acts as a catch-all default lacking a precise definition. It is also interesting to note that while fine-tuned T5 often confuses “Action-Request” with various labels including “Command,” GPT-4 predominantly misclassifies “Action-Request” as “Request-Info” only. In addition, “Action-Statement” is another label with which fine-tuned T5 has more difficulty than GPT-4 (recall rates: 0.46 vs. 0.73). The lower recall for fine-tuned T5 may indicate a difference in approach between fine-tuning and zero-shot learning. The labels associated with “Action,” which are not very common in the dataset, benefit from zero-shot learning. Unlike fine-tuned models, GPT-4 with zero-shot learning remains effective despite imbalanced training data because its performance on the label does not depend on the label’s frequency in the data; rather performance is driven by the ability to semantically map the prompt instruction to the utterance, utilizing various contextual clues. However, fine-tuned models improve as they are trained on more examples of each label, and it is challenging for these models to adapt to accurately recognize imbalanced labels that rarely appear in the training data.</p> <p>  </p> <p>Overall, these results show that using zero-shot learning and iterative prompt refinement with GPT-4 achieved the highest dialogue act recognition accuracy. However, T5 remains a viable option because it is open-source, whereas GPT-4 is proprietary. This distinction becomes more critical as the implementation of our team communication analysis framework scales up, particularly in training settings with multiple sessions where the costs of model deployment are significant. Conversely, in a training environment where T5 models have not been trained with data from that specific training, GPT-4 with zero-shot or few-shot learning might significantly outperform T5, making it potentially more suitable for team communication analytics despite a disparity in cost. Weighing practical benefits, scalability, and cost considerations will be crucial when deploying a runtime version of our team communication analysis framework.</p></div></span></div></div></div><div class="tw-border-b-2 tw-border-gray-100 tw-px-4"><div class="tw-flex tw-items-start tw-justify-between tw-min-w-full"><button type="button" class="focus:tw-outline-none tw-group tw-min-w-full"><div class="tw-flex tw-items-start tw-justify-between"><div class="tw-py-2 tw-w-full"><h2 class="tw-antialiased tw-font-bold tw-text-left tw-text-blue-900 tw-text-xl">Discussion</h2></div><div class="tw-py-2 sm:tw-border-r-2 sm:tw-border-white"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" aria-hidden="true" class="tw-ml-4 tw-h-7 tw-w-7 tw-flex-shrink-0 tw-rounded-full tw-text-gray-500 group-hover:tw-bg-gray-200 group-hover:tw-text-gray-900 tw-transform tw-duration-300 -tw-rotate-180"><path fill-rule="evenodd" d="M5.293 7.293a1 1 0 011.414 0L10 10.586l3.293-3.293a1 1 0 111.414 1.414l-4 4a1 1 0 01-1.414 0l-4-4a1 1 0 010-1.414z" clip-rule="evenodd"></path></svg></div></div></button></div><div class="tw-pb-1"><div class="tw-text-base tw-antialiased tw-leading-relaxed tw-text-left tw-text-black md:tw-leading-loose _fulltext-content"><span style="display:block"><div><p>Team dialogue offers a rich source of data for investigating the processes and behaviors that impact team performance.  Recent advances in deep learning-driven NLP show significant promise for automatically analyzing team communication data while simultaneously providing insights to guide adaptive team training. In this work, we introduce a dialogue act recognition framework that leverages the advanced capabilities of LLMs to support team communication analysis with limited training data, ultimately aiming to provide adaptive team training experiences. Our evaluation of GPT-based models within our dialogue act recognition framework highlighted their superior performance. GPT-4 achieved a recognition accuracy of 77.30% on the held-out test set, surpassing T5, the previous state-of-the-art model, as well as other competitive baseline models including CRF and BLSTM. Particularly, the use of zero-shot learning for GPT models along with iterative prompt enhancements—inspired by human coding practices—proved effective in addressing complex dialogue scenarios with minimal data. Error analysis revealed areas for further refinement, particularly in differentiating subtle dialogue acts, and GPT-4 demonstrated stronger recall compared to other models in these cases. This highlights the potential of LLMs to significantly enhance team communication analytics for team training by accurately interpreting and adaptively responding to team communication behaviors, utilizing real-world data from live training exercises.</p> <p>Regarding our first research question, we adopted an interactive prompt engineering process to optimize the prompt for GPT models.  Our methodology mirrors human coders’ iterative labeling processes, where they review and discuss the utterances causing the most disagreement post-labeling, refining their coding guidelines. This strategy is intuitive and effective, as demonstrated by our results. With a limited training dataset, our approach made the classifier human-readable and reliable, significantly outperforming the baseline. This iterative process of refining the prompt based on specific areas of confusion, much like iterative discussions among human coders, proves to be a potent strategy for enhancing classifier performance with constrained datasets. For the second research question, we compared the accuracy of the highest-performing LLMs against our framework’s baseline models. Specifically, our GPT-4 approach, utilizing the last iteration of prompt engineering, attained the highest accuracy rates overall, as well as higher recall rates on confusing dialogue act labels, relative to T5, the previous state-of-the-art approach. Lastly, our error analysis showed that GPT-4 outperformed T5 in recognizing complex categories like “Other” and less frequent dialogue act labels like “Action-Statement”, utilizing zero-shot learning with refined prompts containing high-quality contextual clues, thereby effectively handling nuances and imbalances in training data.</p> <p>Results from this study encourage the continued exploration of Transformer-based models such as GPT-4 and T5 for developing advanced computational approaches to facilitate team communication analytics. Future work should focus on strategies that: (1) further refine the prompt used for GPT-4 to more effectively represent team communication tasks and clarify properties associated with each label, (2) allow past utterances to provide additional context, enhancing both GPT-4 and T5 model performance in classifying subsequent utterances, and (3) identify characteristics of frequently misclassified utterances based on error analysis results, and use these characteristics to improve NLP models by refining label definitions to enhance label distinction and clarity. Additionally, it will be important to explore other LLMs such as Llama 3 and FLAN-T5 from the transformer language model family to augment the capacity of the team communication analysis framework. Lastly, examining other fundamental dialogue analysis tasks, such as information flow classification, alongside dialogue act recognition, could expand the framework’s ability to provide adaptive training experiences that effectively enhance individual trainees’ learning outcomes. These investigations will deepen our understanding of how team communication influences team performance and guide the development of increasingly effective adaptive team training environments.</p></div></span></div></div></div><div class="tw-border-b-2 tw-border-gray-100 tw-px-4"><div class="tw-flex tw-items-start tw-justify-between tw-min-w-full"><button type="button" class="focus:tw-outline-none tw-group tw-min-w-full"><div class="tw-flex tw-items-start tw-justify-between"><div class="tw-py-2 tw-w-full"><h2 class="tw-antialiased tw-font-bold tw-text-left tw-text-blue-900 tw-text-xl">Conclusion</h2></div><div class="tw-py-2 sm:tw-border-r-2 sm:tw-border-white"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" aria-hidden="true" class="tw-ml-4 tw-h-7 tw-w-7 tw-flex-shrink-0 tw-rounded-full tw-text-gray-500 group-hover:tw-bg-gray-200 group-hover:tw-text-gray-900 tw-transform tw-duration-300 -tw-rotate-180"><path fill-rule="evenodd" d="M5.293 7.293a1 1 0 011.414 0L10 10.586l3.293-3.293a1 1 0 111.414 1.414l-4 4a1 1 0 01-1.414 0l-4-4a1 1 0 010-1.414z" clip-rule="evenodd"></path></svg></div></div></button></div><div class="tw-pb-1"><div class="tw-text-base tw-antialiased tw-leading-relaxed tw-text-left tw-text-black md:tw-leading-loose _fulltext-content"><span style="display:block"><div><p>Adaptive training environments capable of analyzing team communication hold significant promise for improving team training outcomes. Developing computational models that can perform robust team communication analytics based on small datasets is a critical challenge. LLMs offer significant potential to address these challenges and enhance dialogue act classification performance. Results from this study show that zero-shot LLMs, refined through prompt engineering, offer a promising means for supporting dialogue act recognition and that prompt refinements, particularly those addressing confusion between dialogue acts, can lead to superior recall rates for challenging labels. Moving forward, it will be important to incorporate these capabilities into future training systems to provide learners and instructors with tailored feedback to support and improve team communication behaviors. These capabilities will allow instructors and researchers to gain insights into the team actions and processes that impact team coordination.</p></div></span></div></div></div><div class="tw-border-b-2 tw-border-gray-100 tw-px-4"><div class="tw-flex tw-items-start tw-justify-between tw-min-w-full"><button type="button" class="focus:tw-outline-none tw-group tw-min-w-full"><div class="tw-flex tw-items-start tw-justify-between"><div class="tw-py-2 tw-w-full"><h2 class="tw-antialiased tw-font-bold tw-text-left tw-text-blue-900 tw-text-xl">Declarations</h2></div><div class="tw-py-2 sm:tw-border-r-2 sm:tw-border-white"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" aria-hidden="true" class="tw-ml-4 tw-h-7 tw-w-7 tw-flex-shrink-0 tw-rounded-full tw-text-gray-500 group-hover:tw-bg-gray-200 group-hover:tw-text-gray-900 tw-transform tw-duration-300 -tw-rotate-180"><path fill-rule="evenodd" d="M5.293 7.293a1 1 0 011.414 0L10 10.586l3.293-3.293a1 1 0 111.414 1.414l-4 4a1 1 0 01-1.414 0l-4-4a1 1 0 010-1.414z" clip-rule="evenodd"></path></svg></div></div></button></div><div class="tw-pb-1"><div class="tw-text-base tw-antialiased tw-leading-relaxed tw-text-left tw-text-black md:tw-leading-loose _fulltext-content"><span style="display:block"><div><p><strong>Funding. </strong>This research was supported by funding from the U.S. Army Combat Capabilities Development Command, Soldier Center under cooperative agreement W912CG-19-2-0001. </p>
<p><strong>Authors’ contributions. </strong>All authors contributed to the study conception and design. Data analysis was performed by Vikram Kumaran, Jay Pande, and Wookhee Min. The first draft of the manuscript was written by Randall Spain and Wookhee Min, and all authors commented on previous versions of the manuscript. All authors read and approved the final manuscript.<strong> </strong></p>
<p><strong>Acknowledgements</strong></p>
<p>The research described herein has been sponsored by the U.S. Army Combat Capabilities Development Command under cooperative agreement W912CG-19-2-0001. The statements and opinions expressed in this article do not necessarily reflect the position or the policy of the United States Government, and no official endorsement should be inferred.</p>
<p><strong>Conflicts of interest/Competing interests. </strong>No potential conflicts of interest.</p>
<p><strong>Availability of data and material. </strong>Data and materials created for this research are available upon request. Please direct all inquiries to the corresponding author.</p>
<p><strong>Code availability. </strong>Code created for this research is available upon request. Please direct all inquiries to the corresponding author.</p>
<p><strong>Statements on Open Data, Ethics and Conflict of Interest. </strong>This study was conducted with the IRB approval of North Carolina State University. There is no potential conflict of interest in this work. </p></div></span></div></div></div><div class="tw-border-b-2 tw-border-gray-100 tw-px-4"><div class="tw-flex tw-items-start tw-justify-between tw-min-w-full"><button type="button" class="focus:tw-outline-none tw-group tw-min-w-full"><div class="tw-flex tw-items-start tw-justify-between"><div class="tw-py-2 tw-w-full"><h2 class="tw-antialiased tw-font-bold tw-text-left tw-text-blue-900 tw-text-xl">References</h2></div><div class="tw-py-2 sm:tw-border-r-2 sm:tw-border-white"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" aria-hidden="true" class="tw-ml-4 tw-h-7 tw-w-7 tw-flex-shrink-0 tw-rounded-full tw-text-gray-500 group-hover:tw-bg-gray-200 group-hover:tw-text-gray-900 tw-transform tw-duration-300 -tw-rotate-180"><path fill-rule="evenodd" d="M5.293 7.293a1 1 0 011.414 0L10 10.586l3.293-3.293a1 1 0 111.414 1.414l-4 4a1 1 0 01-1.414 0l-4-4a1 1 0 010-1.414z" clip-rule="evenodd"></path></svg></div></div></button></div><div class="tw-pb-1"><div class="tw-text-base tw-antialiased tw-leading-relaxed tw-text-left tw-text-black md:tw-leading-loose _fulltext-content"><span style="display:block"><div><ol>
<li>Alnasyan, B., Basheri, M., &amp; Alassafi, M. (2024). The Power of Deep Learning Techniques for Predicting Student Performance in Virtual Learning Environments: A Systematic Literature Review. <em>Computers and Education: Artificial Intelligence</em>, 100231.</li>
<li>Baber, C., Leggett, A., Attfield, S., &amp; Elliott, E. (2022). Using speech act theory to apply automated communications analysis to distributed sensemaking. In <em>Proceedings of the Human Factors and Ergonomics Society Annual Meeting</em> (Vol. 66, No. 1, pp. 55–59). SAGE Publications.</li>
<li>Badrinath, S., &amp; Balakrishnan, H. (2022). Automatic speech recognition for air traffic control communications. <em>Transportation research record, 2676</em>(1), 798–810.</li>
<li>Bothe, C., Weber, C., Magg, S., &amp; Wermter, S. (2018). A context-based approach for dialogue act recognition using simple recurrent neural networks. <em>arXiv preprint arXiv:1805.06280</em>.</li>
<li>Bowers, C. A., Jentsch, F., Salas, E., &amp; Braun, C. C. (1998). Analyzing communication sequences for team training needs assessment. <em>Human Factors, 40</em>(4), 672–679.</li>
<li>Butchibabu, A., Sparano-Huiban, C., Sonenberg, L., &amp; Shah, J. (2016). Implicit coordination strategies for effective team communication.<em> Human Factors, 58</em>(4), 595–610.</li>
<li>Cannon-Bowers, J. A., Salas, E., &amp; Converse, S. (1993). Shared mental models in expert team decision making. <em>Current issues in individual and group decision making. </em>Lawrence Erlbaum, 221–246.</li>
<li>Carpenter, D., Emerson, A., Mott, B. W., Saleh, A., Glazewski, K. D., Hmelo-Silver, C. E., &amp; Lester, J. C. (2020). Detecting off-task behavior from student dialogue in game-based collaborative learning. In <em>Artificial Intelligence in Education: 21st International Conference, AIED 2020, Ifrane, Morocco, July 6–10, 2020, Proceedings, Part I 21</em> (pp. 55–66). Springer International Publishing.</li>
<li>Carpenter, D., Min, W., Lee, S., Ozogul, G., Zheng, X., &amp; Lester, J. (in press). Assessing Student Explanations with Large Language Models Using Fine-Tuning and Few-Shot Learning. To appear in <em>Proceedings of the Nineteenth Workshop on Innovative Use of NLP for Building Educational Applications</em>.</li>
<li>Chen, Z., Yang, R., Zhao, Z., Cai, D., &amp; He, X. (2018, June). Dialogue act recognition via crf-attentive structured network.. In <em>The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval (SIGIR &#x27;18)</em>. Association for Computing Machinery, 225–234. https://doi.org/10.1145/3209978.3209997</li>
<li>Dillenbourg, P., Järvelä, S., &amp; Fischer, F. (2009). <em>The evolution of research on computer-supported collaborative learning: From design to orchestration</em> (pp. 3–19). Springer Netherlands.</li>
<li>Egaña, A., Aldabe, I., &amp; de Lacalle, O. L. (2023, June). Exploration of Annotation Strategies for Automatic Short Answer Grading. In <em>International Conference on Artificial Intelligence in Education</em> (pp. 377-388). Cham: Springer Nature Switzerland.</li>
<li>Emerson, A., Min, W., Azevedo, R., &amp; Lester, J. (2022). Early prediction of student knowledge in game‐based learning with distributed representations of assessment questions. <em>British Journal of Educational Technology 54</em>(1), 40–57.</li>
<li>Entin, E. E., &amp; Serfaty, D. (1999). Adaptive team coordination. <em>Human Factors, 41</em>(2), 312–325.</li>
<li>Firdaus, M., Golchha, H., Ekbal, A., &amp; Bhattacharyya, P. (2021). A deep multi-task model for dialogue act classification, intent detection and slot filling. <em>Cognitive Computation,</em> <em>13</em>, 626–645. https://doi.org/10.1007/s12559-020-09718-4</li>
<li>Ganesh, A., Palmer, M., &amp; Kann, K. (2023, July). A survey of challenges and methods in the computational modeling of multi-party dialog. In <em>Proceedings of the 5th Workshop on NLP for Conversational AI (NLP4ConvAI 2023)</em>, pages 140–154. Association for Computational Linguistics.</li>
<li>Garosi, E., Kalantari, R., Zanjirani Farahani, A., Zuaktafi, M., Hosseinzadeh Roknabadi, E., &amp; Bakhshi, E. (2020). Concerns about verbal communication in the operating room: A field study. <em>Human Factors, 62</em>(6), 940–953.</li>
<li>Gorman, J. C., Cooke, N. J., &amp; Winner, J. L. (2017). Measuring team situation awareness in decentralized command and control environments. In Sals, E. (Ed.) <em>Situational awareness</em> (pp. 183–196). Routledge.</li>
<li>Gorman, J. C., Grimm, D. A., Stevens, R. H., Galloway, T., Willemsen-Dunlap, A. M., &amp; Halpin, D. J. (2020). Measuring real-time team cognition during team training. <em>Human Factors, 62</em>(5), 825–860.</li>
<li>Grau, S., Sanchis, E., Castro, M. J., &amp; Vilar, D. (2004). Dialogue act classification using a Bayesian approach. In <em>9th Conference Speech and Computer</em>. Retrieved from: https://www.isca-archive.org/specom_2004/grau04_specom.pdf</li>
<li>Graves, A., &amp; Schmidhuber, J. (2005). Framewise phoneme classification with bidirectional LSTM and other neural network architectures. <em>Neural networks</em>, <em>18</em>(5-6), 602–610.</li>
<li>Grimm, D. A., Gorman, J. C., Robinson, E., &amp; Winner, J. (2022, September). Measuring adaptive team coordination in an enroute care training scenario. In <em>Proceedings of the Human Factors and Ergonomics Society Annual Meeting</em> (Vol. 66, No. 1, pp. 50–54). SAGE Publications.</li>
<li>Gupta, A., Carpenter, D., Min, W., Rowe, J., Azevedo, R., &amp; Lester, J. (2022). Enhancing multimodal goal recognition in open-world games with natural language player reflections. In S. G. Ware, &amp; M. Eger (Eds.), <em>Proceedings of the Eighteenth Annual AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment </em>(pp. 37–44). AAAI Press.</li>
<li>Ishizaki, M., &amp; Kato, T. (1998, August). Exploring the characteristics of multi-party dialogues. In <em>36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1</em> (pp. 583–589).</li>
<li>Jensen, E., L. Pugh, S., &amp; K. D&#x27;Mello, S. (2021, April). A deep transfer learning approach to modeling teacher discourse in the classroom. In LAK21: <em>11th international learning analytics and knowledge conference</em> (pp. 302-312).</li>
<li>Jeong, H., Hmelo-Silver, C. E., &amp; Jo, K. (2019). Ten years of computer-supported collaborative learning: A meta-analysis of CSCL in STEM education during 2005–2014. <em>Educational Research Review</em>, <em>28</em>, 100284.</li>
<li>John, P., Brooks, B., &amp; Schriever, U. (2019). Speech acts in professional maritime discourse: A pragmatic risk analysis of bridge team communication directives and commissives in full-mission simulation. <em>Journal of Pragmatics, 140</em>, 12–21.</li>
<li>Johnston, J. H., Phillips, H. L., Milham, L. M., Riddle, D. L., Townsend, L. N., DeCostanza, A. H., Patton, D. J., Cox, K. R., &amp; Fitzhugh, S. M. (2019). A team training field research study: extending a theory of team development. <em>Frontiers in Psychology</em>, 10, 1480.</li>
<li>Johnston, J., Sottilare, R., Sinatra, A. M., &amp; Burke, C. S. (Eds.). (2018). <em>Building intelligent tutoring systems for teams: What matters.</em> Emerald Group Publishing.</li>
<li>Kingma, D.P., Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.</li>
<li>Kumaran, V., Rowe, J., Mott, B., &amp; Lester, J. (2023). SCENECRAFT: Automating interactive narrative scene generation in digital games with large language models. In<em> Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment</em> (Vol. 19, No. 1, pp. 86–96).</li>
<li>Marlow, S. L., Lacerenza, C. N., Paoletti, J., Burke, C. S., &amp; Salas, E. (2018). Does team communication represent a one-size-fits-all approach?: A meta-analysis of team communication and performance. <em>Organizational Behavior and Human Decision Processes, 144</em>, 145–170.</li>
<li>Mayfield, E., Adamson, D., &amp; Rose, C. (2012, July). Hierarchical conversation structure prediction in multi-party chat. In <em>Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue</em> (pp. 60–69).</li>
<li>Min, W., Spain, R., Saville, J. D., Mott, B., Brawner, K., Johnston, J., &amp; Lester, J. (2021, June). Multidimensional team communication modeling for adaptive team training: A hybrid deep learning and graphical modeling framework. In I. Roll, D. McNamara, S. Sosnovsky, R. Luckin, &amp; V. Dimitrova (Eds.), <em>Lecture notes in computer science: Vol. 12748. Artificial intelligence in education</em> (pp. 293–305). Springer, Cham.</li>
<li>Moore, S., Tong, R., Singh, A., Liu, Z., Hu, X., Lu, Y., Liang, J., Cao, C., Khosravi, H., Denny, P., Brooks, C., &amp; Stamper, J. (2023, June). Empowering education with LLMs-the next-gen interface and content generation. In<em> International Conference on Artificial Intelligence in Education</em> (pp. 32–37). Cham: Springer Nature Switzerland.</li>
<li>Morris, W., Crossley, S., Holmes, L., Ou, C., Dascalu, M., &amp; McNamara, D. (2024). Formative feedback on student-authored summaries in intelligent textbooks using large language models. <em>International Journal of Artificial Intelligence in Education</em>. https://doi.org/10.1007/s40593-024-00395-0</li>
<li>Müller, A. C., &amp; Behnke, S. (2014). PyStruct: learning structured prediction in python. <em>Journal of Machine Learning Research</em>, <em>15</em>(1), 2055–2060.</li>
<li>OpenAI. (2023). GPT-4 Technical Report. Retrieved from: https://cdn.openai.com/papers/gpt-4.pdf</li>
<li>Ouahrani, L., &amp; Bennouar, D. (2024). Paraphrase Generation and Supervised Learning for Improved Automatic Short Answer Grading. <em>International Journal of Artificial Intelligence in Education</em>, 1-44.</li>
<li>Owan, V. J., Abang, K. B., Idika, D. O., Etta, E. O., &amp; Bassey, B. A. (2023). Exploring the potential of artificial intelligence tools in educational measurement and assessment.<em> EURASIA Journal of Mathematics, Science and Technology Education, 19</em>(8), em2307.</li>
<li>Pande, J., Min, W., Spain, R. D., Saville, J. D., &amp; Lester, J. (2023). Robust team communication analytics with transformer-based dialogue modeling. In <em>International Conference on Artificial Intelligence in Education</em> (pp. 639–650). Cham: Springer Nature Switzerland.</li>
<li>Park, K., Sohn, H., Mott, B., Min, W., Saleh, A., Glazewski, K., Hmelo-Silver, C. E., &amp; Lester, J. (2021, April). Detecting disruptive talk in student chat-based discussion within collaborative game-based learning environments. In <em>LAK21: 11th International Learning Analytics and Knowledge Conference</em> (pp. 405–415). Society for Learning Analytics Research.</li>
<li>Park, K., Sohn, H., Min, W., Mott, B., Glazewski, K., Hmelo-Silver, C., &amp; Lester, J. (2022). Disruptive talk detection in multi-party dialogue within collaborative learning environments with a regularized user-aware network. In O. Lemon, D. Hakkani-Tur, J. J. Li, A. Ashrafzadeh, D. Hernández Garcia, M. Alikhani, D. Vandyke, &amp; O. Dušek, (Eds.), <em>Proceedings of the 23rd Annual Meeting of the Special Interest Group on Discourse and Dialogue</em> (pp. 490–499). Association for Computational Linguistics.</li>
<li>Pennington, J., Socher, R., &amp; Manning, C. D. (2014). Glove: Global vectors for word representation. In <em>Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</em> (pp. 1532–1543).</li>
<li>Peters, M.E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., Zettlemoyer, L. (2018).Deep contextualized word representations. arXiv preprint arXiv:1802.05365.</li>
<li>Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., &amp; Liu, P. J. (2020). Exploring the limits of transfer learning with a unified text-to-text transformer. <em>The Journal of Machine Learning Research 21</em>(1), 5485–5551.</li>
<li>Rahimi, Z., &amp; Litman, D. (2018). Weighting model based on group dynamics to measure convergence in multi-party dialogue. In <em>Proceedings of the 19th annual SIGdial meeting on discourse and dialogue</em> (pp. 385–390).</li>
<li>Ramesh, D., &amp; Sanampudi, S. K. (2022). An automated essay scoring systems: A systematic literature review. <em>Artificial Intelligence Review 55</em>(3), 2495–2527.</li>
<li>Robinson, F. E., Huffman, L. C. S., Bevington, L. C. D., French, D., Rothwell, C., Stucky, L. C., Tharp, M., &amp; Hughies, A. (2023). Team coordination style is an adaptive, emergent property of interactions between critical care air transport team personnel. <em>Air Medical Journal, 42</em>(3), 174–183.</li>
<li>Ruseti, S., Paraschiv, I., Dascalu, M., &amp; McNamara, D. S. (2024). Automated Pipeline for Multi-lingual Automated Essay Scoring with ReaderBench. <em>International Journal of Artificial Intelligence in Education</em>, 1-22.</li>
<li>Saville, J. D., Spain, R. D., Johnston, J. H., &amp; Lester, J. C. (2021, June). Exploration of team communication behaviors from a live training event. In <em>International Conference on Applied Human Factors and Ergonomics</em> (pp. 101–108). Cham: Springer International Publishing.</li>
<li>Saville, J., Spain, R., Johnston, J., &amp; Lester, J. (2022). An analysis of squad communication behaviors during a field-training exercise to support tactical decision making. In J. Wright and D. Barber (Eds.)<em> Human Factors and Simulation. Vol. 30 </em>(pp. 109–116)<em>. </em>AHFE Open Access.</li>
<li>Sawatzki, J., Schlippe, T., &amp; Benner-Wickner, M. (2022). Deep learning techniques for automatic short answer grading: Predicting scores for English and German answers. In E. C. K. Cheng, R. B. Koul, T. Wang, &amp; Y. Xinguo (Eds.), <em>Artificial Intelligence in Education: Emerging Technologies, Models and Applications, LNDECT, vol. 104</em> (pp. 65–75). Springer.</li>
<li>Shahriar, T., Matsuda, N., &amp; Ramos, K. (2023). Assertion enhanced few-shot learning: Instructive technique for large language models to generate educational explanations. <em>arXiv preprint arXiv</em>:2312.03122.</li>
<li>Shamekhi, A., Liao, Q. V., Wang, D., Bellamy, R. K., &amp; Erickson, T. (2018, April). Face Value? Exploring the effects of embodiment for a group facilitation agent. In <em>Proceedings of the 2018 CHI conference on human factors in computing systems</em> (pp. 1–13).</li>
<li>Sottilare, R. A., Burke, C., Salas, E., Sinatra, A. M., Johnston, J. H., &amp; Gilbert, S. B. (2018). Designing adaptive instruction for teams: A meta-analysis. <em>International Journal of Artificial Intelligence in Education</em>, <em>28</em>, 225–264. </li>
<li>Stolcke, A., Ries, K., Coccaro, N., Shriberg, E., Bates, R., Jurafsky, D., ... &amp; Meteer, M. (2000). Dialogue act modeling for automatic tagging and recognition of conversational speech.<em> Computational linguistics, 26</em>(3), 339–373.</li>
<li>Stucky, C. H., De Jong, M. J., &amp; Kabo, F. W. (2020). Military surgical team communication: Implications for safety. <em>Military Medicine, 185</em>(3-4), e448–e456.</li>
<li>Su, L., Kaplan, S., Burd, R., Winslow, C., Hargrove, A., &amp; Waller, M. (2017). Trauma resuscitation: Can team behaviours in the prearrival period predict resuscitation performance?. <em>BMJ Simulation &amp; Technology Enhanced Learning</em>, <em>3</em>(3), 106.</li>
<li>Surendran, D., &amp; Levow, G. A. (2006). Dialog act tagging with support vector machines and hidden Markov models. In <em>Interspeech, ICSLP </em>(pp. 1950–1953). Retrieved from: https://faculty.washington.edu/levow/papers/IS06_da.pdf</li>
<li>Tan, M., Wang, D., Gao, Y., Wang, H., Potdar, S., Guo, X., Chang, S., &amp; Yu, M. (2019, November). Context-aware conversation thread detection in multi-party chat. In <em>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em> (pp. 6456–6461).</li>
<li>Wahlster, W. (2023). Understanding computational dialogue understanding. <em>Philosophical Transactions of the Royal Society A, 381</em>(2251), 20220049.</li>
<li>Wang, D., Shan, D., Zheng, Y., Guo, K., Chen, G., &amp; Lu, Y. (2023, July). Can chatgpt detect student talk moves in classroom discourse? a preliminary comparison with bert. In <em>Proceedings of the 16th International Conference on Educational Data Mining</em> (pp. 515-519). International Educational Data Mining Society.</li>
<li>Willms, C., Houy, C., Rehse, J. R., Fettke, P., &amp; Kruijff-Korbayová, I. (2019). Team communication processing and process analytics for supporting robot-assisted emergency response. In <em>2019 IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR)</em> (pp. 216–221). IEEE.</li>
<li>van den Oever, F., &amp; Schraagen, J. M. (2021). Team communication patterns in critical situations. <em>Journal of Cognitive Engineering and Decision Making, 15</em>(1), 28–51.</li>
<li>Yamaura, M., Fukuda, I., &amp; Uto, M. (2023, June). Neural Automated Essay Scoring Considering Logical Structure. In <em>International Conference on Artificial Intelligence in Education</em> (pp. 267-278). Cham: Springer Nature Switzerland.</li>
</ol></div></span></div></div></div><div class="tw-border-b-2 tw-border-gray-100 tw-px-4"><div class="tw-flex tw-items-start tw-justify-between tw-min-w-full"><button type="button" class="focus:tw-outline-none tw-group tw-min-w-full"><div class="tw-flex tw-items-start tw-justify-between"><div class="tw-py-2 tw-w-full"><h2 class="tw-antialiased tw-font-bold tw-text-left tw-text-blue-900 tw-text-xl">Additional Declarations</h2></div><div class="tw-py-2 sm:tw-border-r-2 sm:tw-border-white"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" aria-hidden="true" class="tw-ml-4 tw-h-7 tw-w-7 tw-flex-shrink-0 tw-rounded-full tw-text-gray-500 group-hover:tw-bg-gray-200 group-hover:tw-text-gray-900 tw-transform tw-duration-300 -tw-rotate-180"><path fill-rule="evenodd" d="M5.293 7.293a1 1 0 011.414 0L10 10.586l3.293-3.293a1 1 0 111.414 1.414l-4 4a1 1 0 01-1.414 0l-4-4a1 1 0 010-1.414z" clip-rule="evenodd"></path></svg></div></div></button></div><div class="tw-pb-1"><p class="tw-text-base tw-antialiased tw-leading-relaxed tw-text-left tw-text-black md:tw-leading-loose _fulltext-content"><span style="display:block"><p>No competing interests reported.</p></span></p></div></div></div></div></div><div class="tw-col-span-4 lg:tw-min-h-screen xl:tw-px-0"><div class="tw-bg-gray-50 md:tw-px-1.5 tw-max-w-reading tw-mx-auto lg:tw-bg-white lg:tw-sticky lg:tw-top-0 lg:tw-flex-1 lg:tw-h-screen lg:tw-overflow-y-scroll lg:tw-overflow-visible lg:tw-mx-0 lg:tw-px-0 lg:tw-max-w-none _rs-scrollbar"><div class="tw-flex tw-w-full lg:tw-justify-end"><div class="tw-pt-6 tw-pb-24 tw-w-full sm:tw-max-w-xl lg:tw-px-4 xl:tw-px-8"><div class="tw-hidden lg:tw-flex tw-pl-3 tw-justify-end"><div class="tw-pr-3"><div class="tw-relative tw-inline-block tw-text-left" data-headlessui-state=""><div><button class="tw-group tw-inline-flex tw-items-center tw-px-3 tw-py-2 tw-transition-all tw-border-2 tw-rounded-md tw-group tw-bg-white tw-border-gray-100 hover:tw-bg-gray-100 focus:tw-outline-none" id="headlessui-menu-button-:Rkqlm:" type="button" aria-haspopup="menu" aria-expanded="false" data-headlessui-state=""><svg xmlns="http://www.w3.org/2000/svg" class="tw-w-5 tw-h-5 tw-text-gray-600" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path d="M16.6187 6.07577C17.6476 6.07404 18.4714 6.44992 19.09 7.20342C19.6858 7.97982 19.9846 8.89392 19.9864 9.94571C19.9879 10.8146 19.8292 11.6266 19.5103 12.3816C19.1915 13.1367 18.7925 13.8234 18.3134 14.4415C17.8114 15.0597 17.2864 15.5979 16.7385 16.0562C16.1676 16.5373 15.6424 16.9383 15.1628 17.2592C14.6832 17.603 14.2949 17.8552 13.998 18.0158C13.6781 18.1763 13.5296 18.2566 13.5525 18.2566L12.7619 17.1947C12.9675 17.1029 13.2416 16.9195 13.5841 16.6446C13.9266 16.3925 14.2691 16.0832 14.6114 15.7168C14.9309 15.3504 15.2161 14.9498 15.4668 14.5149C15.7177 14.1029 15.8427 13.7026 15.8421 13.3139C15.8417 13.0852 15.7041 12.8568 15.4294 12.6286C15.1546 12.4233 14.857 12.1723 14.5363 11.8756C14.2158 11.6017 13.9179 11.2593 13.6429 10.8482C13.3678 10.4599 13.2298 9.97999 13.2289 9.40837C13.228 8.90534 13.3301 8.44786 13.5352 8.03595C13.7175 7.62407 13.9684 7.26924 14.288 6.97145C14.5848 6.69657 14.9388 6.47875 15.3501 6.318C15.7614 6.15725 16.1843 6.07651 16.6187 6.07577ZM7.29256 6.02144C7.81845 6.02055 8.29881 6.13406 8.73363 6.36198C9.16845 6.58989 9.54624 6.89793 9.86701 7.28609C10.1878 7.67426 10.44 8.1197 10.6238 8.62242C10.7848 9.14805 10.8657 9.7081 10.8667 10.3026C10.8687 11.4916 10.6191 12.5895 10.1178 13.5964C9.5936 14.6262 9.02346 15.5075 8.40734 16.2402C7.67713 17.1103 6.84389 17.9006 5.90763 18.611L4.91144 17.6866C5.11699 17.5491 5.36814 17.3314 5.66488 17.0337C5.93876 16.736 6.20111 16.3811 6.45192 15.9691C6.67992 15.5801 6.88501 15.1681 7.06719 14.7334C7.22651 14.2987 7.30583 13.8756 7.30513 13.464C7.30467 13.1896 7.13279 12.9612 6.78951 12.7789C6.44623 12.5966 6.06857 12.3686 5.65653 12.0949C5.2445 11.8212 4.86663 11.4674 4.52292 11.0336C4.17925 10.6226 4.00678 10.0398 4.00551 9.28526C4.00473 8.82796 4.09548 8.4048 4.27774 8.01579C4.46 7.62677 4.71094 7.28337 5.03055 6.98559C5.32729 6.68784 5.68129 6.44716 6.09255 6.26355C6.48099 6.10283 6.88099 6.02213 7.29256 6.02144Z" stroke="currentColor" fill="currentColor"></path></svg><p class="tw-antialiased tw-hidden tw-ml-2 tw-text-sm tw-font-bold tw-text-gray-700 group-hover:tw-underline md:tw-block lg:tw-hidden xl:tw-block">Cite</p></button></div></div></div><div class="last:tw-pr-0"><div class="tw-relative tw-inline-block tw-text-left" data-headlessui-state=""><div><button class="tw-group tw-inline-flex tw-items-center tw-px-3 tw-py-2 tw-transition-all tw-border-2 tw-rounded-md tw-group tw-bg-white tw-border-gray-100 hover:tw-bg-gray-100 focus:tw-outline-none" id="headlessui-menu-button-:Rmqlm:" type="button" aria-haspopup="menu" aria-expanded="false" data-headlessui-state=""><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" aria-hidden="true" class="tw-w-5 tw-h-5 tw-text-gray-600"><path stroke-linecap="round" stroke-linejoin="round" d="M8.684 13.342C8.886 12.938 9 12.482 9 12c0-.482-.114-.938-.316-1.342m0 2.684a3 3 0 110-2.684m0 2.684l6.632 3.316m-6.632-6l6.632-3.316m0 0a3 3 0 105.367-2.684 3 3 0 00-5.367 2.684zm0 9.316a3 3 0 105.368 2.684 3 3 0 00-5.368-2.684z"></path></svg><p class="tw-antialiased tw-hidden tw-ml-2 tw-text-sm tw-font-bold tw-text-gray-700 group-hover:tw-underline md:tw-block lg:tw-hidden xl:tw-block">Share</p></button></div></div></div><a href="https://www.researchsquare.com/article/rs-4565500/v1.pdf?c=1747685311000" target="_blank" rel="noopener noreferrer" class="tw-text-blue-600 hover:tw-text-blue-800 hover:tw-underline tw-inline-flex tw-items-center tw-px-3.5 tw-py-1.5 tw-transition-all tw-border-2 tw-rounded-md tw-group tw-border-blue-50 tw-bg-blue-50 hover:tw-bg-blue-100 hover:tw-border-blue-100 focus:tw-outline-none tw-text-blue-600 hover:tw-text-blue-800 hover:tw-underline tw-ml-3" data-track="content_download" data-track-context="article pdf" data-track-category="Published" data-track-content-type="Research Article" data-track-label="rs-4565500"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" aria-hidden="true" class="tw-w-5 tw-h-5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 10v6m0 0l-3-3m3 3l3-3m2 8H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"></path></svg><span class="tw-antialiased tw-ml-2 tw-text-sm tw-font-bold"><span class="tw-hidden xl:tw-contents">Download</span> PDF</span></a></div><div class="tw-mx-2.5 lg:tw-mx-0 tw-border-2 tw-border-gray-100 tw-bg-white tw-rounded-lg tw-hidden lg:tw-block lg:tw-mt-4"><div class="tw-px-4"><button type="button" class="tw-min-w-full focus:tw-outline-none tw-group"><div class="tw-flex tw-items-center tw-justify-between"><div class="tw-w-full tw-py-2"><div class="tw-flex tw-items-center"><h3 class="tw-antialiased tw-font-bold tw-text-left tw-text-blue-900 tw-text-base">Status:</h3><div class="tw-rounded-xl tw-font-bold tw-px-3 tw-py-1 tw-bg-green-600 tw-text-white tw-ml-2"><p class="tw-antialiased tw-text-left tw-text-sm tw-text-white">Published</p></div></div></div><div class="tw-rounded-xl tw-font-bold tw-px-3 tw-py-1 tw-bg-gray-100 tw-text-gray-700 tw-my-2 tw-flex tw-items-center tw-duration-300 tw-transform tw-bg-gray-100 group-hover:tw-bg-gray-200"><p class="tw-antialiased tw-text-left tw-text-sm tw-text-black tw-text-gray-800 group-hover:tw-text-gray-900 tw-flex"></p><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" aria-hidden="true" class="tw-text-gray-800 group-hover:tw-text-gray-900 tw-h-6 tw-w-6 tw-flex-shrink-0 tw-rounded-full tw-transform tw-duration-300"><path fill-rule="evenodd" d="M5.293 7.293a1 1 0 011.414 0L10 10.586l3.293-3.293a1 1 0 111.414 1.414l-4 4a1 1 0 01-1.414 0l-4-4a1 1 0 010-1.414z" clip-rule="evenodd"></path></svg></div></div><div class="tw-mb-3 tw-rounded-lg sm:tw-mx-0 tw-bg-white tw-border-2 tw-border-gray-100"><div class="tw-px-2.5 sm:tw-px-4"><img src="https://assets-eu.researchsquare.com/logos/journals/logo-international-journal-of-artificial-intelligence-in-education-m.svg" alt="International Journal of Artificial Intelligence in Education" class="tw-py-3 tw-w-80"/></div></div></button></div><div class="tw-px-4 tw-py-2 tw-border-t-2 tw-border-gray-100"><div class="tw-text-left"><h3 class="tw-antialiased tw-font-bold tw-text-left tw-text-blue-900 tw-text-base">Journal Publication</h3><p class="tw-antialiased tw-text-left tw-text-sm tw-text-black">published <!-- -->14 May, 2025</p></div><p class="tw-antialiased tw-text-left tw-text-sm tw-text-black tw-mt-3 tw-mb-1"><a href="https://doi.org/10.1007/s40593-025-00479-5" target="_blank" rel="noopener noreferrer" class="tw-text-blue-600 hover:tw-text-blue-800 hover:tw-underline" data-track="click_reading_the_published_version" data-track-context="International Journal of Artificial Intelligence in Education" data-track-category="Published" data-track-link-text="Read the published version in International Journal of Artificial Intelligence in Education" data-track-content-type="Research Article" data-track-label="Springer Link"><span class="tw-rounded-xl tw-font-bold tw-px-3 tw-py-1 tw-bg-blue-50 hover:tw-bg-blue-100 tw-transition tw-block tw-w-full tw-text-center">Read the published version in<!-- --> <span class="tw-italic">International Journal of Artificial Intelligence in Education</span>  →</span></a></p></div><div class="tw-px-4 tw-border-t-2 tw-border-gray-100"><button type="button" class="tw-min-w-full focus:tw-outline-none tw-group"><div class="tw-flex tw-items-center tw-justify-between"><div class="tw-w-full tw-py-2"><div class="tw-text-left"><h3 class="tw-antialiased tw-font-bold tw-text-left tw-text-blue-900 tw-text-base">Version <!-- -->1</h3><p class="tw-antialiased tw-text-left tw-text-sm tw-text-black tw-text-gray-700">posted<!-- --> </p></div></div><div class="tw-rounded-xl tw-font-bold tw-px-3 tw-py-1 tw-bg-gray-100 tw-text-gray-700 tw-my-2 tw-flex tw-items-center tw-duration-300 tw-transform tw-bg-gray-100 group-hover:tw-bg-gray-200"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" aria-hidden="true" class="tw-text-gray-800 group-hover:tw-text-gray-900 tw-h-6 tw-w-6 tw-flex-shrink-0 tw-rounded-full tw-transform tw-duration-300 -tw-rotate-180"><path fill-rule="evenodd" d="M5.293 7.293a1 1 0 011.414 0L10 10.586l3.293-3.293a1 1 0 111.414 1.414l-4 4a1 1 0 01-1.414 0l-4-4a1 1 0 010-1.414z" clip-rule="evenodd"></path></svg></div></div></button><div class="tw-pb-1"><ul class="tw-py-1 tw-list-none"><li class="tw-pb-2"><div class="tw-relative"><span class="tw-absolute -tw-top-1.5 tw-left-3.5 -tw-ml-px tw-h-full tw-w-0.5 tw-bg-gray-200" aria-hidden="true"></span><div class="tw-relative tw-flex tw-space-x-3"><div><span class="tw-h-7 tw-w-7 tw-rounded-full tw-flex tw-items-center tw-justify-center tw-bg-gray-200"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" aria-hidden="true" class="tw-h-4 tw-w-4 tw-text-gray-700"><path d="M13.586 3.586a2 2 0 112.828 2.828l-.793.793-2.828-2.828.793-.793zM11.379 5.793L3 14.172V17h2.828l8.38-8.379-2.83-2.828z"></path></svg></span></div><div class="tw-block tw-min-w-0 md:tw-pt-1 md:tw-flex-1 md:tw-flex md:tw-justify-between"><p class="tw-antialiased tw-text-left tw-text-sm tw-text-black tw-pr-2">Editorial decision: <b>Revision requested</b></p><p class="tw-antialiased tw-text-left tw-text-sm tw-text-black tw-flex-shrink-0 lg:tw-text-right">13 Oct, 2024</p></div></div></div></li><li class="tw-pb-2"><div class="tw-relative"><span class="tw-absolute -tw-top-1.5 tw-left-3.5 -tw-ml-px tw-h-full tw-w-0.5 tw-bg-gray-200" aria-hidden="true"></span><div class="tw-relative tw-flex tw-space-x-3"><div><span class="tw-h-7 tw-w-7 tw-rounded-full tw-flex tw-items-center tw-justify-center tw-bg-gray-200"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" aria-hidden="true" class="tw-h-4 tw-w-4 tw-text-gray-700"><path d="M9 2a1 1 0 000 2h2a1 1 0 100-2H9z"></path><path fill-rule="evenodd" d="M4 5a2 2 0 012-2 3 3 0 003 3h2a3 3 0 003-3 2 2 0 012 2v11a2 2 0 01-2 2H6a2 2 0 01-2-2V5zm9.707 5.707a1 1 0 00-1.414-1.414L9 12.586l-1.293-1.293a1 1 0 00-1.414 1.414l2 2a1 1 0 001.414 0l4-4z" clip-rule="evenodd"></path></svg></span></div><div class="tw-block tw-min-w-0 md:tw-pt-1 md:tw-flex-1 md:tw-flex md:tw-justify-between"><p class="tw-antialiased tw-text-left tw-text-sm tw-text-black tw-pr-2">Reviews received at journal</p><p class="tw-antialiased tw-text-left tw-text-sm tw-text-black tw-flex-shrink-0 lg:tw-text-right">17 Sep, 2024</p></div></div></div></li><li class="tw-pb-2"><div class="tw-relative"><span class="tw-absolute -tw-top-1.5 tw-left-3.5 -tw-ml-px tw-h-full tw-w-0.5 tw-bg-gray-200" aria-hidden="true"></span><div class="tw-relative tw-flex tw-space-x-3"><div><span class="tw-h-7 tw-w-7 tw-rounded-full tw-flex tw-items-center tw-justify-center tw-bg-gray-200"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" aria-hidden="true" class="tw-h-4 tw-w-4 tw-text-gray-700"><path d="M9 2a1 1 0 000 2h2a1 1 0 100-2H9z"></path><path fill-rule="evenodd" d="M4 5a2 2 0 012-2 3 3 0 003 3h2a3 3 0 003-3 2 2 0 012 2v11a2 2 0 01-2 2H6a2 2 0 01-2-2V5zm9.707 5.707a1 1 0 00-1.414-1.414L9 12.586l-1.293-1.293a1 1 0 00-1.414 1.414l2 2a1 1 0 001.414 0l4-4z" clip-rule="evenodd"></path></svg></span></div><div class="tw-block tw-min-w-0 md:tw-pt-1 md:tw-flex-1 md:tw-flex md:tw-justify-between"><p class="tw-antialiased tw-text-left tw-text-sm tw-text-black tw-pr-2">Reviews received at journal</p><p class="tw-antialiased tw-text-left tw-text-sm tw-text-black tw-flex-shrink-0 lg:tw-text-right">25 Aug, 2024</p></div></div></div></li><li class="tw-pb-2"><div class="tw-relative"><span class="tw-absolute -tw-top-1.5 tw-left-3.5 -tw-ml-px tw-h-full tw-w-0.5 tw-bg-gray-200" aria-hidden="true"></span><div class="tw-relative tw-flex tw-space-x-3"><div><span class="tw-h-7 tw-w-7 tw-rounded-full tw-flex tw-items-center tw-justify-center tw-bg-gray-200"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" aria-hidden="true" class="tw-h-4 tw-w-4 tw-text-gray-700"><path d="M2 10.5a1.5 1.5 0 113 0v6a1.5 1.5 0 01-3 0v-6zM6 10.333v5.43a2 2 0 001.106 1.79l.05.025A4 4 0 008.943 18h5.416a2 2 0 001.962-1.608l1.2-6A2 2 0 0015.56 8H12V4a2 2 0 00-2-2 1 1 0 00-1 1v.667a4 4 0 01-.8 2.4L6.8 7.933a4 4 0 00-.8 2.4z"></path></svg></span></div><div class="tw-block tw-min-w-0 md:tw-pt-1 md:tw-flex-1 md:tw-flex md:tw-justify-between"><p class="tw-antialiased tw-text-left tw-text-sm tw-text-black tw-pr-2">Reviewers agreed at journal</p><p class="tw-antialiased tw-text-left tw-text-sm tw-text-black tw-flex-shrink-0 lg:tw-text-right">31 Jul, 2024</p></div></div></div></li><li class="tw-pb-2"><div class="tw-relative"><span class="tw-absolute -tw-top-1.5 tw-left-3.5 -tw-ml-px tw-h-full tw-w-0.5 tw-bg-gray-200" aria-hidden="true"></span><div class="tw-relative tw-flex tw-space-x-3"><div><span class="tw-h-7 tw-w-7 tw-rounded-full tw-flex tw-items-center tw-justify-center tw-bg-gray-200"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" aria-hidden="true" class="tw-h-4 tw-w-4 tw-text-gray-700"><path d="M9 2a1 1 0 000 2h2a1 1 0 100-2H9z"></path><path fill-rule="evenodd" d="M4 5a2 2 0 012-2 3 3 0 003 3h2a3 3 0 003-3 2 2 0 012 2v11a2 2 0 01-2 2H6a2 2 0 01-2-2V5zm9.707 5.707a1 1 0 00-1.414-1.414L9 12.586l-1.293-1.293a1 1 0 00-1.414 1.414l2 2a1 1 0 001.414 0l4-4z" clip-rule="evenodd"></path></svg></span></div><div class="tw-block tw-min-w-0 md:tw-pt-1 md:tw-flex-1 md:tw-flex md:tw-justify-between"><p class="tw-antialiased tw-text-left tw-text-sm tw-text-black tw-pr-2">Reviews received at journal</p><p class="tw-antialiased tw-text-left tw-text-sm tw-text-black tw-flex-shrink-0 lg:tw-text-right">15 Jul, 2024</p></div></div></div></li><li class="tw-pb-2"><div class="tw-relative"><span class="tw-absolute -tw-top-1.5 tw-left-3.5 -tw-ml-px tw-h-full tw-w-0.5 tw-bg-gray-200" aria-hidden="true"></span><div class="tw-relative tw-flex tw-space-x-3"><div><span class="tw-h-7 tw-w-7 tw-rounded-full tw-flex tw-items-center tw-justify-center tw-bg-gray-200"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" aria-hidden="true" class="tw-h-4 tw-w-4 tw-text-gray-700"><path d="M2 10.5a1.5 1.5 0 113 0v6a1.5 1.5 0 01-3 0v-6zM6 10.333v5.43a2 2 0 001.106 1.79l.05.025A4 4 0 008.943 18h5.416a2 2 0 001.962-1.608l1.2-6A2 2 0 0015.56 8H12V4a2 2 0 00-2-2 1 1 0 00-1 1v.667a4 4 0 01-.8 2.4L6.8 7.933a4 4 0 00-.8 2.4z"></path></svg></span></div><div class="tw-block tw-min-w-0 md:tw-pt-1 md:tw-flex-1 md:tw-flex md:tw-justify-between"><p class="tw-antialiased tw-text-left tw-text-sm tw-text-black tw-pr-2">Reviewers agreed at journal</p><p class="tw-antialiased tw-text-left tw-text-sm tw-text-black tw-flex-shrink-0 lg:tw-text-right">24 Jun, 2024</p></div></div></div></li><li class="tw-pb-2"><div class="tw-relative"><span class="tw-absolute -tw-top-1.5 tw-left-3.5 -tw-ml-px tw-h-full tw-w-0.5 tw-bg-gray-200" aria-hidden="true"></span><div class="tw-relative tw-flex tw-space-x-3"><div><span class="tw-h-7 tw-w-7 tw-rounded-full tw-flex tw-items-center tw-justify-center tw-bg-gray-200"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" aria-hidden="true" class="tw-h-4 tw-w-4 tw-text-gray-700"><path d="M2 10.5a1.5 1.5 0 113 0v6a1.5 1.5 0 01-3 0v-6zM6 10.333v5.43a2 2 0 001.106 1.79l.05.025A4 4 0 008.943 18h5.416a2 2 0 001.962-1.608l1.2-6A2 2 0 0015.56 8H12V4a2 2 0 00-2-2 1 1 0 00-1 1v.667a4 4 0 01-.8 2.4L6.8 7.933a4 4 0 00-.8 2.4z"></path></svg></span></div><div class="tw-block tw-min-w-0 md:tw-pt-1 md:tw-flex-1 md:tw-flex md:tw-justify-between"><p class="tw-antialiased tw-text-left tw-text-sm tw-text-black tw-pr-2">Reviewers agreed at journal</p><p class="tw-antialiased tw-text-left tw-text-sm tw-text-black tw-flex-shrink-0 lg:tw-text-right">23 Jun, 2024</p></div></div></div></li><li class="tw-pb-2"><div class="tw-relative"><span class="tw-absolute -tw-top-1.5 tw-left-3.5 -tw-ml-px tw-h-full tw-w-0.5 tw-bg-gray-200" aria-hidden="true"></span><div class="tw-relative tw-flex tw-space-x-3"><div><span class="tw-h-7 tw-w-7 tw-rounded-full tw-flex tw-items-center tw-justify-center tw-bg-gray-200"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" aria-hidden="true" class="tw-h-4 tw-w-4 tw-text-gray-700"><path d="M2.003 5.884L10 9.882l7.997-3.998A2 2 0 0016 4H4a2 2 0 00-1.997 1.884z"></path><path d="M18 8.118l-8 4-8-4V14a2 2 0 002 2h12a2 2 0 002-2V8.118z"></path></svg></span></div><div class="tw-block tw-min-w-0 md:tw-pt-1 md:tw-flex-1 md:tw-flex md:tw-justify-between"><p class="tw-antialiased tw-text-left tw-text-sm tw-text-black tw-pr-2">Reviewers invited by journal</p><p class="tw-antialiased tw-text-left tw-text-sm tw-text-black tw-flex-shrink-0 lg:tw-text-right">23 Jun, 2024</p></div></div></div></li><li class="tw-pb-2"><div class="tw-relative"><span class="tw-absolute -tw-top-1.5 tw-left-3.5 -tw-ml-px tw-h-full tw-w-0.5 tw-bg-gray-200" aria-hidden="true"></span><div class="tw-relative tw-flex tw-space-x-3"><div><span class="tw-h-7 tw-w-7 tw-rounded-full tw-flex tw-items-center tw-justify-center tw-bg-gray-200"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" aria-hidden="true" class="tw-h-4 tw-w-4 tw-text-gray-700"><path fill-rule="evenodd" d="M10 9a3 3 0 100-6 3 3 0 000 6zm-7 9a7 7 0 1114 0H3z" clip-rule="evenodd"></path></svg></span></div><div class="tw-block tw-min-w-0 md:tw-pt-1 md:tw-flex-1 md:tw-flex md:tw-justify-between"><p class="tw-antialiased tw-text-left tw-text-sm tw-text-black tw-pr-2">Editor assigned by journal</p><p class="tw-antialiased tw-text-left tw-text-sm tw-text-black tw-flex-shrink-0 lg:tw-text-right">14 Jun, 2024</p></div></div></div></li><li class="tw-pb-2"><div class="tw-relative"><span class="tw-absolute -tw-top-1.5 tw-left-3.5 -tw-ml-px tw-h-full tw-w-0.5 tw-bg-gray-200" aria-hidden="true"></span><div class="tw-relative tw-flex tw-space-x-3"><div><span class="tw-h-7 tw-w-7 tw-rounded-full tw-flex tw-items-center tw-justify-center tw-bg-gray-200"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" aria-hidden="true" class="tw-h-4 tw-w-4 tw-text-gray-700"><path fill-rule="evenodd" d="M10 18a8 8 0 100-16 8 8 0 000 16zm3.707-9.293a1 1 0 00-1.414-1.414L9 10.586 7.707 9.293a1 1 0 00-1.414 1.414l2 2a1 1 0 001.414 0l4-4z" clip-rule="evenodd"></path></svg></span></div><div class="tw-block tw-min-w-0 md:tw-pt-1 md:tw-flex-1 md:tw-flex md:tw-justify-between"><p class="tw-antialiased tw-text-left tw-text-sm tw-text-black tw-pr-2">Submission checks completed at journal</p><p class="tw-antialiased tw-text-left tw-text-sm tw-text-black tw-flex-shrink-0 lg:tw-text-right">14 Jun, 2024</p></div></div></div></li><li class="tw-pb-2"><div class="tw-relative"><span class="tw-absolute -tw-top-1.5 tw-left-3.5 -tw-ml-px tw-h-full tw-w-0.5 tw-bg-gray-200" aria-hidden="true"></span><div class="tw-relative tw-flex tw-space-x-3"><div><span class="tw-h-7 tw-w-7 tw-rounded-full tw-flex tw-items-center tw-justify-center tw-bg-gray-200"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" aria-hidden="true" class="tw-h-4 tw-w-4 tw-text-gray-700"><path fill-rule="evenodd" d="M6 2a2 2 0 00-2 2v12a2 2 0 002 2h8a2 2 0 002-2V7.414A2 2 0 0015.414 6L12 2.586A2 2 0 0010.586 2H6zm5 6a1 1 0 10-2 0v2H7a1 1 0 100 2h2v2a1 1 0 102 0v-2h2a1 1 0 100-2h-2V8z" clip-rule="evenodd"></path></svg></span></div><div class="tw-block tw-min-w-0 md:tw-pt-1 md:tw-flex-1 md:tw-flex md:tw-justify-between"><p class="tw-antialiased tw-text-left tw-text-sm tw-text-black tw-pr-2">First submitted to journal</p><p class="tw-antialiased tw-text-left tw-text-sm tw-text-black tw-flex-shrink-0 lg:tw-text-right">11 Jun, 2024</p></div></div></div></li></ul></div><div class="tw-pb-1"><p class="tw-antialiased tw-text-left tw-text-sm tw-text-black"><span class="tw-rounded-xl tw-font-bold tw-px-3 tw-py-1 tw-bg-gray-100 tw-text-gray-700 tw-flex tw-justify-center tw-w-full tw-mb-2">You are reading this <!-- -->latest<!-- --> <!-- -->preprint<!-- --> version</span></p></div></div></div></div></div></div></div></div></div></div></div></div><div class="original-css"><footer class="section b-pb-60-d b-pb-20-m"><div class="tw-justify-between tw-block tw-p-4 tw-space-y-6 lg:tw-space-x-6 lg:tw-space-y-0 rs-container lg:tw-flex"><div class="tw-max-w-lg tw-b-mb-20"><a href="/"><img class="logo-rs" src="/static/img/logos/logo-rs.svg" alt="Research Square"/></a><p class="footer-desc b-pb-15">Research Square lets you share your work early, gain feedback from the community, and start making changes to your manuscript prior to peer review in a journal.</p><p class="footer-desc">As a division of Research Square Company, we’re committed to making research communication faster, fairer, and more useful. We do this by developing innovative software and high quality services for the global research community. Our growing team is made up of researchers and industry professionals working together to solve the most critical problems facing scientific publishing.</p><div class="icons is-flex b-pt-25 b-pb-20"><a href="https://twitter.com/researchsquare" rel="nofollow"><img src="/static/img/icons/icon-twitter.svg" alt="Twitter logo"/></a><a href="https://www.facebook.com/researchsquare" rel="nofollow"><img src="/static/img/icons/icon-facebook.svg" alt="Facebook logo"/></a><a href="https://www.youtube.com/c/researchsquare" rel="nofollow"><img src="/static/img/icons/icon-youtube.svg" alt="YouTube logo"/></a><a href="https://www.instagram.com/researchsquarecompany/" rel="nofollow"><img src="/static/img/icons/icon-instagram.svg" alt="Instagram logo"/></a><a href="https://www.linkedin.com/company/research-square-co" rel="nofollow"><img src="/static/img/icons/icon-linkedin.svg" alt="LinkedIn logo"/></a></div><div class="researcher-app"><a href="https://www.researcher-app.com/?utm_source=researchsquare&amp;utm_medium=referral&amp;utm_campaign=&amp;utm_content=logo" class="align-m" target="_blank" rel="noopener noreferrer"><h6 class="color-text-90 b-pr-10">Also discoverable on</h6><img src="/static/img/logos/logo-researcher-app.svg" alt="ResearcherApp logo"/></a></div></div><div class="tw-justify-between tw-block tw-space-y-6 lg:tw-space-x-6 lg:tw-space-y-0 lg:tw-flex"><div id="platform"><h5 class="caps color-text-b4 b-mt-3 b-pb-15 b-mr-40-d">Platform</h5><p class="footer-link"><a href="/researchers/preprints">About</a></p><p class="footer-link"><a href="/about">Our Team</a></p><p class="footer-link"><a href="/researchers/in-review">In Review</a></p><p class="footer-link"><a href="/legal/editorial">Editorial Policies</a></p><p class="footer-link"><a href="/advisory-board">Advisory Board</a></p><p class="footer-link"><a target="_blank" rel="noopener noreferrer" data-track="click_contact_us_button" data-track-context="Contact us button click footer help center" href="https://support.researchsquare.com">Help Center</a></p></div><div id="resources"><h5 class="caps color-text-b4 b-mt-3 b-pb-15 b-mr-20-d">Resources</h5><p class="footer-link"><a href="/researchers">Author Services</a></p><p class="footer-link"><a href="/accessibility">Accessibility</a></p><p class="footer-link"><a href="/request-api">API Access</a></p><p class="footer-link"><a href="/rss.xml" rel="alternate" type="application/rss+xml">RSS feed</a></p><p class="footer-link"><button data-cc-action="preferences">Manage Cookie Preferences</button></p></div></div></div></footer><footer class="section columns tail"><div class="column is-narrow is-paddingless"><div class="navbar"><p class="navbar-item">© Research Square <!-- -->2025<!-- --> | ISSN 2693-5015 (online)<!-- --> </p><a class="navbar-item" href="/legal/privacy">Privacy Policy</a><a class="navbar-item" href="/legal/terms-of-service">Terms of Service</a><a class="navbar-item" href="/legal/do-not-sell-my-personal-information">Do Not Sell My Personal Information</a></div></div></footer></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"initialData":{"identity":"rs-4565500","acceptedTermsAndConditions":true,"allowDirectSubmit":false,"archivedVersions":[],"articleType":"Research Article","associatedPublications":[],"authors":[{"id":320320116,"identity":"92402132-847c-42f1-ad03-92bcfb0f3312","order_by":0,"name":"Randall Spain","email":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAZAAAAAyAQMAAABI0h/eAAAABlBMVEX///8AAABVwtN+AAAACXBIWXMAAA7EAAAOxAGVKw4bAAABEUlEQVRIie3RsUrDQBzH8X85uC5Hs8mFxD7DlcBFURAfJQi6BKlbx3RplkDfxsnhyoEuurfEIV1uusWtQgYvXjMU0tBR8L4Q+IXkwwUC4HL90ShMLwBEM5m5EAwq+2AwF0cJowcEsT3JTiA2TNvVReL8Y7XeGTISQ1Xtpp/g5YTPvl+ek+xMdp4Svj/eXRaG+ILEk4IpoJLwTaHKJAuSTkIh5ZQYwgTBZkhgCN+vieghnuZ+/UuGygxLnuo+QlMe2FOgGQ1Br6j3FKqjq5BR4ksSRyGThEokg1CU0eLoh6WTjZ5dj0dvudrqWo695Wr+pUV5vgwetlUHaSPmD2I79t0uel5vwwd3NycIl8vl+h/9ACuCYMBZ3JcqAAAAAElFTkSuQmCC","orcid":"","institution":"U.S. Army Combat Capabilities Development Command","correspondingAuthor":true,"prefix":"","firstName":"Randall","middleName":"","lastName":"Spain","suffix":""},{"id":320320118,"identity":"8b0932a8-e50f-4d76-9d74-188ebd7d0c9e","order_by":1,"name":"Wookhee Min","email":"","orcid":"","institution":"North Carolina State University","correspondingAuthor":false,"prefix":"","firstName":"Wookhee","middleName":"","lastName":"Min","suffix":""},{"id":320320120,"identity":"eb61dfb7-0cc1-4943-a7bc-79ebb5ecf3b6","order_by":2,"name":"Vikram Kumaran","email":"","orcid":"","institution":"North Carolina State University","correspondingAuthor":false,"prefix":"","firstName":"Vikram","middleName":"","lastName":"Kumaran","suffix":""},{"id":320320121,"identity":"f204bbda-1252-42d2-a672-dce83058e179","order_by":3,"name":"Jay Pande","email":"","orcid":"","institution":"North Carolina State University","correspondingAuthor":false,"prefix":"","firstName":"Jay","middleName":"","lastName":"Pande","suffix":""},{"id":320320122,"identity":"f954ec46-3406-43f0-aaa7-6011c951ec25","order_by":4,"name":"Jason Saville","email":"","orcid":"","institution":"Industrial/Organizational Solutions","correspondingAuthor":false,"prefix":"","firstName":"Jason","middleName":"","lastName":"Saville","suffix":""},{"id":320320123,"identity":"3d1e4614-9c17-4053-997f-d78cc267b781","order_by":5,"name":"James Lester","email":"","orcid":"","institution":"North Carolina State University","correspondingAuthor":false,"prefix":"","firstName":"James","middleName":"","lastName":"Lester","suffix":""}],"badges":[],"createdAt":"2024-06-11 16:25:38","currentVersionCode":1,"declarations":"","doi":"10.21203/rs.3.rs-4565500/v1","doiUrl":"https://doi.org/10.21203/rs.3.rs-4565500/v1","draftVersion":[],"editorialEvents":[{"content":"https://doi.org/10.1007/s40593-025-00479-5","type":"published","date":"2025-05-14T15:57:57+00:00"}],"editorialNote":"","failedWorkflow":false,"files":[{"id":59429849,"identity":"28891c4b-1741-4773-86fe-0e92a4d4d475","added_by":"auto","created_at":"2024-07-01 17:33:54","extension":"jpg","order_by":1,"title":"Figure 1","display":"","copyAsset":false,"role":"figure","size":174508,"visible":true,"origin":"","legend":"\u003cp\u003eTeam communication analysis framework supporting adaptive team training environments.\u003c/p\u003e","description":"","filename":"1.jpg","url":"https://assets-eu.researchsquare.com/files/rs-4565500/v1/d928177ce6493f8dad72a910.jpg"},{"id":59429848,"identity":"ed79fd84-c198-45df-af5b-c94ddb4aea3f","added_by":"auto","created_at":"2024-07-01 17:33:54","extension":"jpg","order_by":2,"title":"Figure 2","display":"","copyAsset":false,"role":"figure","size":72222,"visible":true,"origin":"","legend":"\u003cp\u003e(a) Heatmap of the GPT-4 classifier confusion matrix. (b) Heatmap of the fine-tuned T5 classifier confusion matrix. The rows indicate true labels, while the columns are for the classifier's inferred labels.\u003c/p\u003e","description":"","filename":"2.jpg","url":"https://assets-eu.researchsquare.com/files/rs-4565500/v1/28e6b7341485c5137e1cd266.jpg"},{"id":83067948,"identity":"6ff2227c-c867-4b91-815a-a0c8cae6d61f","added_by":"auto","created_at":"2025-05-19 16:08:31","extension":"pdf","order_by":0,"title":"","display":"","copyAsset":false,"role":"manuscript-pdf","size":1301151,"visible":true,"origin":"","legend":"","description":"","filename":"manuscript.pdf","url":"https://assets-eu.researchsquare.com/files/rs-4565500/v1/c3c79ede-ff1f-4ccf-9f3a-1510db25ad38.pdf"}],"financialInterests":"No competing interests reported.","formattedTitle":"Applying Large Language Models to Enhance Dialogue and Communication Analysis for Adaptive Team Training","fulltext":[{"header":"Introduction","content":"\u003cp\u003eEffective communication among team members is essential in many domains, especially those characterized by complex, high-stakes, time-limited tasks such operating rooms, aircraft cockpits, command and control stations, and flight control centers. In these environments, team members must effectively communicate with one another to minimize errors, improve coordination, and facilitate team action processes required for effective performance. Conversely, ineffective communication can lead to disastrous consequences. For decades, team science researchers have investigated methods to support team communication analysis to better understand the behaviors that exemplify high-performing teams and to support team training (Marlow et al., 2018). As a result, speech-based communication data is playing an increasingly important role in the advancement of adaptive training systems for teams (Johnston et al., 2018).\u0026nbsp;\u003c/p\u003e\n\u003cp\u003eAdaptive team training systems aim to improve both team performance outcomes and teamwork behaviors that facilitate team performance by providing tailored feedback based on patterns of their verbal and nonverbal interactions (Sottilaire et al., 2018).\u0026nbsp;Because many teamwork and team decision-making behaviors can be assessed by monitoring a team’s verbal communication, a significant task for the AI-driven training research community is to develop speech recognition and natural language processing (NLP) capabilities that facilitate team performance measurement (Badrinath \u0026amp; Balakrishnan, 2022; Willms et al., 2019). In particular, being able to leverage NLP capabilities to automatically classify and label spoken communication between team members would represent a significant step towards the development of training environments that can partially or fully automate the assessment of teamwork skills and team performance. Indeed, research has shown that speech patterns can provide meaningful insights into team performance (Baber et al., 2022; John et al., 2019; van den Oever \u0026amp; Schraagen, 2021) and can be used to identify team communication behaviors associated with high functioning teams (Saville et al., 2022). Dialogue act recognition is the classification of utterances based on their communicative function or intent in a conversation (Stolcke et al., 2000). When dialogue act recognition is integrated into adaptive training environments, instructors and individuals can obtain timely insights on team actions through automatic analysis of team communication behaviors. A key challenge in this process is devising reliable and accurate dialogue act recognition models.\u003c/p\u003e\n\u003cp\u003eResearchers have investigated a broad range of supervised learning techniques for dialogue act recognition, including probabilistic graphical modeling approaches such as Bayesian networks (Grau et al., 2004) and conditional random fields (CRF; Chen et al., 2018), support vector machines (Surendran \u0026amp; Levow, 2006), and neural networks (Bothe et al., 2018; Firdaus et al., 2021). While these techniques have often been evaluated with real-world dialogue corpora, a significant need for the team science research community is to develop dialogue act classifiers that can assist with team communication analysis when training data are limited.\u0026nbsp;\u003c/p\u003e\n\u003cp\u003eIn our previous research, we explored an approach that combines deep neural networks with probabilistic graphical models to recognize dialogue acts in team communication, using a hybrid NLP framework (Min et al., 2021). This framework combined ELMo word embeddings with CRF to facilitate dialogue act recognition and information flow classification (i.e., the directional flow of communication within the team) with team training transcripts. Subsequently, we devised a team communication analysis framework (Pande et al., 2023) that leveraged the adaptability and transfer learning capabilities of the Text-to-Text Transfer Transformer (T5) model, an open-source language model based on the Transformer architecture (Raffel et al., 2020). This enabled us to attain high accuracy in both dialogue act recognition and information flow classification, aiding in the analysis of team communication.\u0026nbsp;\u003c/p\u003e\n\u003cp\u003eIn this article, we extend this line of research to investigate how large language models (LLMs) can support accurate team dialogue act classification in situations where labeled training data is limited. LLMs have shown outstanding predictive performance in a wide range of downstream NLP tasks (e.g., text summarization, language understanding, translation), even with limited training data (i.e., few-shot learning) or even no training data (i.e., zero-shot learning). GPT-3.5 and GPT-4 have additional advantages over other LLMs, such as the ability for users to provide verbose prompts with detailed information about tasks and the use of human feedback during training to reduce undesirable outputs (OpenAI, 2023). Because of the flexibility and the rich representational capacity afforded by the transformer architecture, LLMs offer significant versatility while also being more generalizable than previous approaches to support dialogue act classification.\u0026nbsp;\u003c/p\u003e\n\u003cp\u003eThe motivation for investigating the dialogue act recognition performance of LLMs stems from the substantial challenges encountered in crafting robust team communication analysis models under the constraint of limited labeled training data. The process of transcribing and annotating team communications from training events is notably resource- and labor-intensive. Although various tools exist to aid researchers in this endeavor, the availability of publicly accessible labeled datasets for developing and testing approaches for these tasks remains limited. Moreover, the practice among team science researchers of devising unique dialogue act labeling schemes tailored to the specific demands of each team training domain can limit the generalizability of approaches. Consequently, there is an increasing need to develop flexible NLP-driven solutions capable of supporting team communication analytics—particularly dialogue act recognition—on a scalable level. To address these challenges, we investigate the capabilities of GPT-3.5 and GPT-4 in supporting dialogue act recognition using small-sized transcripts captured from a team training exercise. We explore how prompt engineering can be used to further enhance the accuracy of these LLMs and compare model classification performance to our previous work using CRF, bidirectional long short-term memory networks, and T5-based models evaluated with the same team communication dataset. We also investigate common classification errors produced by the LLMs.\u0026nbsp;\u003c/p\u003e\n\u003cp\u003eIn particular, we explore the following research questions:\u0026nbsp;\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eRQ1.\u003c/strong\u003e Can adding contextual information through iterative stages of prompt engineering enhance the dialogue act recognition performance of the GPT models?\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eRQ2.\u003c/strong\u003e What is the accuracy of the top-performing GPT models for dialogue act recognition compared to the previous state-of-the-art approach based on T5, as well as other competitive baselines?\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eRQ3.\u003c/strong\u003e What types of classification errors are most prominent in the LLMs’ inferences, and how can the performance of these NLP models be enhanced based on error analysis findings?\u0026nbsp;\u003c/p\u003e\n\u003cp\u003eIn the following sections, we introduce a generalizable team communication analysis framework designed to analyze spoken team dialogue. This framework leverages NLP models examine dialogue acts among team members, with the ultimate objective of designing adaptive training environments that employ such analytics to enhance teamwork. To address RQ1, we investigate a prompt engineering process to iteratively optimize the performance of GPT-3.5 and GPT-4 models. This method is akin to the iterative training process of human coders, who revise and refine coding guidelines after reviewing and discussing utterances that generated the most disagreement post-labeling. For RQ2, we compare the accuracy of the highest-performing LLMs against our framework’s baseline models. Specifically, we evaluate LLMs’ performance compared to models investigated in prior work, such as CRF-based probabilistic graphical models (Min et al., 2021) and T5-based models (Pande et al., 2023) for the dialogue act recognition task. Finally, for RQ3, we present an error analysis of our team communication models. This analysis informs our discussion on the implications of our work for future research, particularly in further improving NLP models’ performance and choosing suitable NLP models for reliable, real-time analysis of team communication.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eBackground\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eNatural Language Processing for Education\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe increasing availability of large-scale textual and speech datasets has fueled advances in NLP, propelling forward a wide array of applications ranging from text summarization to translation (see e.g., Raffel et al., 2020). NLP is emerging as a cornerstone technology within the rapidly evolving landscape of educational technology, revolutionizing AI-enabled learning and learning analytics. Recent work has explored NLP’s capacity to automate and refine assessment, such as essay scoring (Ramesh \u0026amp; Sanampudi, 2022; Ruseti et al., 2024; Yamaura et al., 2023) and short answer grading (Egaña et al., 2023; Ouahrani \u0026amp; Bennouar, 2024; Sawatski, Schlippe, \u0026amp; Benner-Wickner, 2022). Beyond assessment, these technologies extend to predicting learning outcomes (Alnasyan et al., 2024; Emerson et al., 2022) and goals (Gupta et al., 2022), thereby paving the way for more tailored educational experiences. Furthermore, NLP has been instrumental in analyzing classroom discourse, providing insights into the dynamics of student-teacher interactions and the cognitive underpinnings of learning processes (Jensen et al., 2021; Park et al., 2022; Wang et al., 2023). More recently, LLMs have shown significant promise for supporting student assessment (Carpenter et al., in press; Owan et al., 2023), content and scenario generation (Moore et al., 2023; Kumaran et al., 2023), and feedback and explanation generation (Morris et al., 2024; Shahriar et al., 2024). While proprietary LLMs have often achieved the state-of-the-art performance for these downstream NLP tasks, open-source LLMs offer a practical alternative, particularly for implementing scalable team training environments across diverse sessions. These offer advantages like cost-effectiveness and addressing critical ethical concerns, including data privacy. Considering these factors of accuracy, practicality, scalability, and ethics, our research explores both proprietary and open-source LLMs to assess the performance of our team communication analysis framework.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eTeam Communication\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eTeam tasks require team members to actively share and provide information with one another to achieve a shared goal (Garosi et al., 2020; Marlow et al., 2018). Research has consistently demonstrated that effective team communication is a key factor in enhancing team performance. It contributes to improved team coordination (Grimm et al., 2022), heightened team situation awareness (Gorman et al., 2017), and facilitates the development of high levels of team cognition (Gorman et al., 2020). Effective communication also strengthens team cohesion (Stucky et al., 2020). Research shows teams whose members readily share information among themselves can recognize and anticipate the needs of one another (Cannon-Bowers et al., 1993; Entin \u0026amp; Serfaty, 1999), adapt to stressful situations (Butchibabu et al., 2016; Su et al., 2017), and function more cohesively as a unit (Gorman et al., 2020; Stucky et al., 2020). Moreover, meta-analytic findings indicate that it is not solely the quantity of communication among team members that impacts performance, but quality of a team’s communication behaviors (Marlow et al., 2018). Therefore, examining how teams communicate and function in dynamic, real-world environments is critical for gaining insights into the processes teams use to collaborate and coordinate and for advancing theories of team development.\u0026nbsp;\u003c/p\u003e\n\u003cp\u003eTo investigate communication behaviors within teams, researchers often develop and apply content labeling schemes to transcripts of team communication. These schemes enable researchers to classify individual utterances according to their intent. Common labeling schemes include classifying utterances into statements that correspond with requesting information, providing information, demonstrating closed loop communication, providing command statements, and requesting help (Bower et al., 1998; Robinson et al., 2023). By using these label outputs researchers may perform frequency or sequence analyses to analyze team communication behaviors and gather insights into team performance during simulated activities and training events. For instance, Saville et al., (2022) used labeled transcripts to examine communication behaviors across low and high functioning military teams who completed a stressful live training exercise. They found that high-performing teams shared more information than low-performing teams and gave fewer commands during periods of high demand. As another example, Garosi et al. (2022) examined verbal communication among surgical teams to identify communication patterns that contributed to poor surgical team performance.\u0026nbsp;\u003c/p\u003e\n\u003cp\u003eIn recent years, there has been an expansion of research investigating NLP-driven approaches for dialogue act classification labeling (Min et al., 2021; Pande et al., 2023). When integrated into team training systems, automatically recognized dialogue acts can provide insights that support additional downstream tasks essential for adaptive team training.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eAI-Driven Multi-Party Dialogue Analysis\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eMulti-party dialogues, compared to dyadic conversations, are complex conversational interactions involving three or more participants in a dynamic exchange of information (Rahimi \u0026amp; Litman, 2018; Ishizaki \u0026amp; Kato, 1998). The complexity of analyzing team communication in multi-party paradigms arises not only from the number of participants but also from concurrent conversation threads and multi-threaded discourse (Shamekhi et al., 2018; Tan et al., 2019). Analysis of multi-party dialogue is further complicated by the variability in group dynamics, where each participant may contribute differently to the conversation, necessitating sophisticated computational models that can account for these variances to reliably parse and understand the discourse (Ganesh et al., 2023; Mayfield et al., 2012; Rahimi \u0026amp; Litman, 2018). Recent approaches to modeling multi-party dialogues have emphasized the need for contextual awareness, drawing on advanced language models and machine learning techniques that can navigate the nuanced interplay of dialogue acts and thematic thread detection (Pande et al., 2023; Wahlster, 2023).\u0026nbsp;\u003c/p\u003e\n\u003cp\u003eMulti-party dialogues in team training environments play a pivotal role in the social construction of knowledge, where the interplay among learners or trainees can either facilitate or hinder collective learning objectives (Dillenbourg et al., 2009; Jeong et al., 2019). For example, detecting disruptive talk within these environments, which can manifest as off-task behavior or negative socio-emotional engagement, presents a significant barrier to the efficacy of these educational interactions. To address this, research has investigated various computational approaches, ranging from classic \u003cem\u003en\u003c/em\u003e-grams and random forests to more sophisticated deep learning architectures such as long short-term memory networks, enhanced with recent advances in contextualized language representations such as BERT, and attention-based user-aware neural architecture (Carpenter et al., 2020; Park et al., 2021; 2022). Such work aims not only to detect disruptive elements but also to preserve the collaborative integrity of the learning experience, by ensuring that communication remains on-task and conducive to the shared learning objectives of the group.\u003c/p\u003e\n\u003cp\u003eIn the context of team training, deep learning-based NLP models, such as a hybrid team communication analytics model combining ELMo embeddings with CRF (Min et al., 2021), have shown promise in identifying dialogue acts and information flow during team activities to support multi-party dialogue analysis. More recently, team communication analysis models based on the T5 architecture have demonstrated high accuracy in dialogue act recognition and information flow classification tasks. These models encode both the utterances of team members and their hierarchical roles within the team using natural language processing techniques. These models outperform simple majority class baselines and hybrid models trained on the same dataset (Pande et al., 2023), which comprised utterances from live team training exercises. These transformer-based models also exceeded benchmarks from prior work for generalizing to data from a different training event.\u003c/p\u003e\n\u003cp\u003e\u0026nbsp;In this article, we extend upon our prior research (Min et al., 2021; Pande et al., 2023) to examine the dialogue act recognition performance of GPT models and how prompt engineering strategies can be used to iteratively enhance their accuracy. We compare the accuracy of the top-performing GPT models for dialogue act recognition to previous state-of-the-art approaches based on T5, as well as other competitive baselines. Finally, we examine the types of classification errors that are most prominent in the LLMs’ predictions and discuss how the performance of these NLP models can be enhanced based on error analysis findings.\u003c/p\u003e"},{"header":"Methods","content":"\u003cdiv id=\"Sec6\" class=\"Section2\"\u003e \u003ch2\u003eFramework\u003c/h2\u003e \u003cp\u003eInitial research into automating team communication analysis has shown promising results. This research aims to present a team communication analysis framework, illustrated in Fig.\u0026nbsp;\u003cspan refid=\"Fig1\" class=\"InternalRef\"\u003e1\u003c/span\u003e, that explores the advanced capabilities of transformer-based LLMs, with a particular focus on GPTs. It specifically addresses dialogue act recognition, especially in scenarios with limited training data. As shown in Fig.\u0026nbsp;\u003cspan refid=\"Fig1\" class=\"InternalRef\"\u003e1\u003c/span\u003e, speech uttered by team members is collected by the learning environment and converted into transcripts using automatic speech recognition. Then, the framework analyzes the transcripts to produce insights about each utterance, recognizing dialogue acts in the team communication. Next, these insights are passed to another component of the learning environment, the adaptation engine, which provides adaptive scaffolding to each team member (e.g., feedback about ways in which their communication has been effective or ineffective thus far in the activity, or hints about how to communicate more effectively going forward) to help team members improve their performance. The analysis from the framework can also help researchers make decisions about how to refine the learning environment to provide even more effective adaptive scaffolding. Finally, upon the conclusion of a learning activity, the framework can help team members better understand how they performed during the activity, using the insights associated with each utterance as supporting evidence for a summary of overall performance. This article focuses on evaluating the lower part of Fig.\u0026nbsp;\u003cspan refid=\"Fig1\" class=\"InternalRef\"\u003e1\u003c/span\u003e (i.e., the Team Communication Analysis Framework box), which incorporates the development, evaluation, and refinement of robust team communication analysis models, as well as detailed error analysis based on the model inference results.\u003c/p\u003e \u003cp\u003e  \u003c/p\u003e \u003c/div\u003e \u003cdiv id=\"Sec7\" class=\"Section2\"\u003e \u003ch2\u003eTraining Environment and Dataset\u003c/h2\u003e \u003cp\u003eTo develop and evaluate our framework, we used labeled transcripts compiled from a U.S. Army training event (Johnston et al., \u003cspan citationid=\"CR28\" class=\"CitationRef\"\u003e2019\u003c/span\u003e). The transcripts contain utterances captured from squad members as they completed a 45-minute live training exercise. The training exercise was designed to assess decision-making and team development under stress. Each squad was composed of nine members, including a squad leader, two team leaders, and specialists, who each fulfilled specific roles within the squad. These roles influenced their interactions with one another and the actors in the exercise. The squad leader oversaw coordinating and overseeing the operations of the squad\u0026rsquo;s subordinate teams and reporting information up the chain of command to the platoon leader. The Alpha and Bravo team leaders reported to the squad leader and were responsible for directing activities for their respective fireteams. The alpha and bravo teams each included four to five team members.  In the exercise, squads were tasked with observing and maintaining overwatch of a village, interacting with key local leaders to gain intelligence on local gang movements and activity, and maintaining security. As the mission narrative unfolded, squad members were faced with critical events such as aiding villagers, responding to tactical threats, aiding team members, and administering casualty care to squad members. These events required trainees to coordinate actions, exchange information, and provide supportive behaviors, all while fulfilling distinct roles within a structured chain of command.\u003c/p\u003e \u003cp\u003eThe transcribed audio logs underwent a detailed annotation process by experts, utilizing a coding scheme that categorized utterances into 27 types of dialogue acts, alongside identifying the speaker\u0026rsquo;s role within the team. This granular classification aimed to elucidate the nature of information exchange within the squads, capturing what their communicative intent or function is (i.e., dialogue acts). The dialogue act labels capture what a speaker intends to communicate in an utterance, such as information about what they see, a suggestion about what to do next, or a question for another squad member. Each utterance was labeled with its corresponding dialogue act label. Inter-rater reliability for labeling dialogue acts demonstrated a Krippendorff\u0026rsquo;s alpha of 0.679, with a raw agreement was 0.715 and an expected agreement was 0.113 (Pande et al., \u003cspan citationid=\"CR41\" class=\"CitationRef\"\u003e2023\u003c/span\u003e).\u003c/p\u003e \u003cp\u003eAfter annotation, the dialogue act labels were consolidated into a more manageable and analytically useful set to enhance analysis and intervention and to improve recognition accuracy. This resulted in nine dialogue act labels: \u003cspan type=\"SmallCaps\" class=\"SmallCaps\" name=\"Emphasis\"\u003eAcknowledgement\u003c/span\u003e, \u003cspan type=\"SmallCaps\" class=\"SmallCaps\" name=\"Emphasis\"\u003eAction request\u003c/span\u003e, \u003cspan type=\"SmallCaps\" class=\"SmallCaps\" name=\"Emphasis\"\u003eAction Statement\u003c/span\u003e, \u003cspan type=\"SmallCaps\" class=\"SmallCaps\" name=\"Emphasis\"\u003eCommand\u003c/span\u003e, \u003cspan type=\"SmallCaps\" class=\"SmallCaps\" name=\"Emphasis\"\u003eAttention\u003c/span\u003e, \u003cspan type=\"SmallCaps\" class=\"SmallCaps\" name=\"Emphasis\"\u003eGreeting\u003c/span\u003e, \u003cspan type=\"SmallCaps\" class=\"SmallCaps\" name=\"Emphasis\"\u003eProvide information\u003c/span\u003e, \u003cspan type=\"SmallCaps\" class=\"SmallCaps\" name=\"Emphasis\"\u003eRequest information\u003c/span\u003e, and \u003cspan type=\"SmallCaps\" class=\"SmallCaps\" name=\"Emphasis\"\u003eOther\u003c/span\u003e (Table\u0026nbsp;\u003cspan refid=\"Tab1\" class=\"InternalRef\"\u003e1\u003c/span\u003e). The \"\u003cspan type=\"SmallCaps\" class=\"SmallCaps\" name=\"Emphasis\"\u003eOther\u003c/span\u003e\" category encompasses utterances that do not fit any defined dialogue acts or are non-meaningful, such as incomplete statements and expletives. Analysis of the 4,315 coded utterances revealed that providing information and issuing commands were the most prevalent forms of communication, highlighting the importance of clear, directive exchanges in team operations. Table\u0026nbsp;\u003cspan refid=\"Tab2\" class=\"InternalRef\"\u003e2\u003c/span\u003e presents some example utterances along with their labels.\u003c/p\u003e \u003cp\u003e \u003cdiv class=\"gridtable\"\u003e\u003ctable float=\"Yes\" id=\"Tab1\" border=\"1\"\u003e \u003ccaption language=\"En\"\u003e \u003cdiv class=\"CaptionNumber\"\u003eTable 1\u003c/div\u003e \u003cdiv class=\"CaptionContent\"\u003e \u003cp\u003eFrequency of dialogue act labels.\u003c/p\u003e \u003c/div\u003e \u003c/caption\u003e \u003ccolgroup cols=\"3\"\u003e \u003cdiv align=\"left\" class=\"colspec\" colname=\"c1\" colnum=\"1\"\u003e\u003c/div\u003e \u003cdiv align=\"left\" class=\"colspec\" colname=\"c2\" colnum=\"2\"\u003e\u003c/div\u003e \u003cdiv align=\"char\" char=\".\" class=\"colspec\" colname=\"c3\" colnum=\"3\"\u003e\u003c/div\u003e \u003cthead\u003e \u003ctr\u003e \u003cth align=\"left\" colname=\"c1\"\u003e \u003cp\u003eDialogue Act Label\u003c/p\u003e \u003c/th\u003e \u003cth align=\"left\" colname=\"c2\"\u003e \u003cp\u003eDefinition\u003c/p\u003e \u003c/th\u003e \u003cth align=\"left\" colname=\"c3\"\u003e \u003cp\u003eNumber of Occurrences\u003c/p\u003e \u003c/th\u003e \u003c/tr\u003e \u003c/thead\u003e \u003ctbody\u003e \u003ctr\u003e \u003ctd align=\"left\" colname=\"c1\"\u003e \u003cp\u003e\u003cspan type=\"SmallCaps\" class=\"SmallCaps\" name=\"Emphasis\"\u003eProvide information\u003c/span\u003e\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c2\"\u003e \u003cp\u003eProviding (solicited or unsolicited) information, including repeating back information\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"char\" char=\".\" colname=\"c3\"\u003e \u003cp\u003e1109\u003c/p\u003e \u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd align=\"left\" colname=\"c1\"\u003e \u003cp\u003e\u003cspan type=\"SmallCaps\" class=\"SmallCaps\" name=\"Emphasis\"\u003eCommand\u003c/span\u003e\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c2\"\u003e \u003cp\u003eIssuing a command, including denying or granting permission\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"char\" char=\".\" colname=\"c3\"\u003e \u003cp\u003e805\u003c/p\u003e \u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd align=\"left\" colname=\"c1\"\u003e \u003cp\u003e\u003cspan type=\"SmallCaps\" class=\"SmallCaps\" name=\"Emphasis\"\u003eAcknowledgement\u003c/span\u003e\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c2\"\u003e \u003cp\u003eSignaling understanding of the previous utterance, including simple agreement or disagreement\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"char\" char=\".\" colname=\"c3\"\u003e \u003cp\u003e762\u003c/p\u003e \u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd align=\"left\" colname=\"c1\"\u003e \u003cp\u003e\u003cspan type=\"SmallCaps\" class=\"SmallCaps\" name=\"Emphasis\"\u003eRequest information\u003c/span\u003e\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c2\"\u003e \u003cp\u003eRequesting information or anything not covered by \u003cspan type=\"SmallCaps\" class=\"SmallCaps\" name=\"Emphasis\"\u003eAction request\u003c/span\u003e\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"char\" char=\".\" colname=\"c3\"\u003e \u003cp\u003e611\u003c/p\u003e \u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd align=\"left\" colname=\"c1\"\u003e \u003cp\u003e\u003cspan type=\"SmallCaps\" class=\"SmallCaps\" name=\"Emphasis\"\u003eOther\u003c/span\u003e\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c2\"\u003e \u003cp\u003eAny utterance not covered by another dialogue act label (e.g., miscellaneous)\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"char\" char=\".\" colname=\"c3\"\u003e \u003cp\u003e601\u003c/p\u003e \u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd align=\"left\" colname=\"c1\"\u003e \u003cp\u003e\u003cspan type=\"SmallCaps\" class=\"SmallCaps\" name=\"Emphasis\"\u003eAttention\u003c/span\u003e\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c2\"\u003e \u003cp\u003eGetting the attention of another team member\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"char\" char=\".\" colname=\"c3\"\u003e \u003cp\u003e203\u003c/p\u003e \u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd align=\"left\" colname=\"c1\"\u003e \u003cp\u003e\u003cspan type=\"SmallCaps\" class=\"SmallCaps\" name=\"Emphasis\"\u003eAction Statement\u003c/span\u003e\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c2\"\u003e \u003cp\u003eCommitting to, suggesting, or confirming completion of an action\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"char\" char=\".\" colname=\"c3\"\u003e \u003cp\u003e95\u003c/p\u003e \u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd align=\"left\" colname=\"c1\"\u003e \u003cp\u003e\u003cspan type=\"SmallCaps\" class=\"SmallCaps\" name=\"Emphasis\"\u003eGreeting\u003c/span\u003e\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c2\"\u003e \u003cp\u003eUttering greetings or pleasantries\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"char\" char=\".\" colname=\"c3\"\u003e \u003cp\u003e83\u003c/p\u003e \u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd align=\"left\" colname=\"c1\"\u003e \u003cp\u003e\u003cspan type=\"SmallCaps\" class=\"SmallCaps\" name=\"Emphasis\"\u003eAction request\u003c/span\u003e\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c2\"\u003e \u003cp\u003eRequesting permission, backup, or repeat of a previous utterance\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"char\" char=\".\" colname=\"c3\"\u003e \u003cp\u003e46\u003c/p\u003e \u003c/td\u003e \u003c/tr\u003e \u003c/tbody\u003e \u003c/colgroup\u003e \u003c/table\u003e\u003c/div\u003e \u003c/p\u003e \u003cp\u003e \u003cdiv class=\"gridtable\"\u003e\u003ctable float=\"Yes\" id=\"Tab2\" border=\"1\"\u003e \u003ccaption language=\"En\"\u003e \u003cdiv class=\"CaptionNumber\"\u003eTable 2\u003c/div\u003e \u003cdiv class=\"CaptionContent\"\u003e \u003cp\u003eExample utterances and their dialogue act labels.\u003c/p\u003e \u003c/div\u003e \u003c/caption\u003e \u003ccolgroup cols=\"3\"\u003e \u003cdiv align=\"left\" class=\"colspec\" colname=\"c1\" colnum=\"1\"\u003e\u003c/div\u003e \u003cdiv align=\"left\" class=\"colspec\" colname=\"c2\" colnum=\"2\"\u003e\u003c/div\u003e \u003cdiv align=\"left\" class=\"colspec\" colname=\"c3\" colnum=\"3\"\u003e\u003c/div\u003e \u003cthead\u003e \u003ctr\u003e \u003cth align=\"left\" colname=\"c1\"\u003e \u003cp\u003eSpeaker\u003c/p\u003e \u003c/th\u003e \u003cth align=\"left\" colname=\"c2\"\u003e \u003cp\u003eExample Utterances\u003c/p\u003e \u003c/th\u003e \u003cth align=\"left\" colname=\"c3\"\u003e \u003cp\u003eDialogue Act\u003c/p\u003e \u003c/th\u003e \u003c/tr\u003e \u003c/thead\u003e \u003ctbody\u003e \u003ctr\u003e \u003ctd align=\"left\" colname=\"c1\"\u003e \u003cp\u003eSquad Leader\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c2\"\u003e \u003cp\u003eWe\u0026rsquo;re moving into town now\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c3\"\u003e \u003cp\u003e\u003cspan type=\"SmallCaps\" class=\"SmallCaps\" name=\"Emphasis\"\u003eAction statement\u003c/span\u003e\u003c/p\u003e \u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd align=\"left\" colname=\"c1\"\u003e \u003cp\u003eAlpha Team Leader\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c2\"\u003e \u003cp\u003eI don\u0026rsquo;t see any humans but I do see the black truck\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c3\"\u003e \u003cp\u003e\u003cspan type=\"SmallCaps\" class=\"SmallCaps\" name=\"Emphasis\"\u003eProvide information\u003c/span\u003e\u003c/p\u003e \u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd align=\"left\" colname=\"c1\"\u003e \u003cp\u003eSquad Leader\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c2\"\u003e \u003cp\u003eRoger, where is it at?\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c3\"\u003e \u003cp\u003e\u003cspan type=\"SmallCaps\" class=\"SmallCaps\" name=\"Emphasis\"\u003eRequest information\u003c/span\u003e\u003c/p\u003e \u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd align=\"left\" colname=\"c1\"\u003e \u003cp\u003eBravo Team Leader\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c2\"\u003e \u003cp\u003eEyes up for wires on that bridge\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c3\"\u003e \u003cp\u003e\u003cspan type=\"SmallCaps\" class=\"SmallCaps\" name=\"Emphasis\"\u003eCommand\u003c/span\u003e\u003c/p\u003e \u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd align=\"left\" colname=\"c1\"\u003e \u003cp\u003eSquad Leader\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c2\"\u003e \u003cp\u003eGo ahead and shift security to bravo four\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c3\"\u003e \u003cp\u003e\u003cspan type=\"SmallCaps\" class=\"SmallCaps\" name=\"Emphasis\"\u003eCommand\u003c/span\u003e\u003c/p\u003e \u003c/td\u003e \u003c/tr\u003e \u003c/tbody\u003e \u003c/colgroup\u003e \u003c/table\u003e\u003c/div\u003e \u003c/p\u003e \u003cp\u003ePrior studies that used this labeled data found that high-performing squads shared information more frequently and issued more commands to teammates compared to their lower-performing counterparts (Saville et al., 2021). Additionally, high-performing teams maintained consistent rates of information exchange regardless of scenario difficulty, while low-performing teams showed significant increases in commands and information exchange during stressful situations (Saville et al., \u003cspan citationid=\"CR52\" class=\"CitationRef\"\u003e2022\u003c/span\u003e). These investigations not only confirm the critical role of communication in team performance but also demonstrate the value of using dialogue act analysis to investigate communication behaviors.  Thus, accurate classification of utterances according to dialogue act labels can support more rigorous assessment of team performance and richer adaptive team training systems.\u003c/p\u003e \u003c/div\u003e \u003cdiv id=\"Sec8\" class=\"Section2\"\u003e \u003ch2\u003eBaseline Models\u003c/h2\u003e \u003cp\u003eTo identify the highest-performing models for dialogue act recognition for the dataset, we evaluate four different machine learning methods with our team communication analysis framework. First, we describe three models grounded in our prior work, including CRF (Min et al., 2021), bidirectional long short-term memory networks (Min et al., 2021), and Text-To-Text Transfer Transformers (Pande et al., \u003cspan citationid=\"CR41\" class=\"CitationRef\"\u003e2023\u003c/span\u003e). Then, we describe generative pre-trained transformers (GPTs), which are refined through an iterative process of prompt engineering and compared to the previous approaches.\u003c/p\u003e \u003c/div\u003e \u003cdiv id=\"Sec9\" class=\"Section2\"\u003e \u003ch2\u003eBaseline 1: Conditional Random Fields Models\u003c/h2\u003e \u003cp\u003eIn our prior work we investigated linear-chain CRFs models with deep learning-based inputs. These models combine probabilistic graphical models\u0026rsquo; inference capacity and deep neural networks\u0026rsquo; representation capacity for team analytics, aiming to categorize team communication into dialogue acts (Min et al., 2021). CRFs demonstrated notable utility in structured inference and sequence modeling, employing undirected graphical models for classifying multivariate data. They are particularly effective at capturing dependencies between predictive features and their associated class labels. While conventional representation techniques for natural language, such as bag-of-words, term frequency-inverse document frequency (TF-IDF), and GloVe (Pennington et al., \u003cspan citationid=\"CR44\" class=\"CitationRef\"\u003e2014\u003c/span\u003e), use static feature vectors or word embeddings, these approaches often fall short in capturing the semantics and syntactics inherent in dialogue. To overcome this limitation, our research explored the use of ELMo embeddings (Peters et al., \u003cspan citationid=\"CR45\" class=\"CitationRef\"\u003e2018\u003c/span\u003e), contextual representations generated by a bidirectional long short-term memory network, which were integrated into the CRFs as utterance-level input.\u003c/p\u003e \u003cp\u003eTo develop CRF models, we used a pre-trained ELMo model to generate an embedding of each utterance. This ELMo model was built with stacked bidirectional LSTMs trained on the 1\u0026nbsp;Billion Word Benchmark, approximately 800M tokens of news crawl data from WMT 2011.  The ELMo model provided a 1,024-dimensional embedding of each word in an utterance, and then we averaged each dimension over all words to obtain an utterance-level embedding. Due to the large dimensionality of these ELMo embeddings, we applied principal component analysis (PCA) to reduce the 1,024 dimensions down to one of 32, 64, or 128 dimensions, identifying the optimal reduced dimension through cross-validation. Finally, the PCA-transformed utterance features were concatenated with the speaker role-based features, which are represented using one-hot encoding, to collectively serve as the input features for the CRF models.\u003c/p\u003e \u003cp\u003eWe capped the input sequence length at 100 and limited training to 100 iterations for training CRF models. To determine the best-performing CRF models through cross-validation, we carried out hyperparameter optimization, focusing on three hyperparameters: the regularization parameter from the set {0.1, 0.5, 1}, the optimizer convergence tolerance from the set {0.01, 0.001}, and the reduced PCA dimensions from the set {32, 64, 128}. We employed PyStruct (M\u0026uuml;ller \u0026amp; Behnke, \u003cspan citationid=\"CR37\" class=\"CitationRef\"\u003e2014\u003c/span\u003e), a Python-based, off-the-shelf CRF modeling library, for training the models. The optimal hyperparameter values were identified through a cross-validation process per classification task. Utilizing the identified optimal hyperparameter values, the final models for dialogue act recognition were trained using the entire training set and subsequently evaluated using a held-out test set.\u003c/p\u003e \u003cdiv id=\"Sec10\" class=\"Section3\"\u003e \u003ch2\u003eBaseline 2: Bi-directional Long Short-Term Memory Networks\u003c/h2\u003e \u003cp\u003eSecondly, our investigation extended to two-layer bidirectional long short-term memory networks (BLSTMs) (Graves \u0026amp; Schmidhuber, \u003cspan citationid=\"CR21\" class=\"CitationRef\"\u003e2005\u003c/span\u003e) that utilized as input the same ELMo embeddings as the CRFs (Min et al., 2021). The choice to employ stacked BLSTMs over simpler, single-layer LSTMs was influenced by preliminary evidence indicating that stacked configurations, capable of capturing the bidirectional context of speaker utterances through their multi-layer architecture, were more effective in dialogue act classification. Specifically, we explored the capabilities of multi-task BLSTMs aiming to harness the benefits of models designed for effectively handling multiple, interrelated tasks, in addition to single-task BLSTMs (which handle dialogue act recognition task only). To achieve this goal, the multi-task models utilized labels for information flow in the communication data used for our dialogue act recognition. Information flow indicates the direction of information being passed (e.g., up or down the chain of command) during communication. Prior research suggests that the nuances in communication\u0026mdash;specifically, the frequency and type of dialogue acts and information flows\u0026mdash;correlate strongly with team performance levels (Saville et al., 2021; \u003cspan citationid=\"CR52\" class=\"CitationRef\"\u003e2022\u003c/span\u003e). These findings underscore the critical role of information flow in understanding team dynamics and outcomes, which exhibits significant potential to model together with dialogue acts through multi-task modeling. For more details about the information flow labels, refer to our previous work (Min et al., 2021; Pande et al., \u003cspan citationid=\"CR41\" class=\"CitationRef\"\u003e2023\u003c/span\u003e).\u003c/p\u003e \u003cp\u003eFurthermore, we delved into two fusion techniques\u0026mdash;early fusion and late fusion\u0026mdash;to refine our modeling strategy based on different sets of input features (utterance-based and speaker role-based). In early fusion, PCA-reduced ELMo representations of the speaker\u0026rsquo;s utterance and one-hot-encoded speaker role representations were merged into a single vector, then fed into the BLSTMs. Late fusion, on the other hand, involves running two separate BLSTMs for the two distinct sets of input features, with their outputs later merged for classification in a softmax layer. These approaches were evaluated using the same PCA-reduced dimensionality options (32, 64, or 128) as those used in the CRF models.\u003c/p\u003e \u003cp\u003eFor the BLSTM layers, we opted for 32 hidden units and incorporated a dropout rate of 0.25 to regularize the models, employed the softmax function for output layer activations, and utilized the Adam optimizer (Kingma \u0026amp; Ba, \u003cspan citationid=\"CR30\" class=\"CitationRef\"\u003e2014\u003c/span\u003e). Echoing the configuration for CRF models, we capped the input sequence length at 100 and limited training to a maximum of 100 epochs. The models employed an early stopping mechanism triggered after 10 epochs (i.e., patience duration) without improvement in validation loss, calculated using 10% of the training data, to optimize the training process.\u003c/p\u003e \u003c/div\u003e \u003c/div\u003e \u003cdiv id=\"Sec11\" class=\"Section2\"\u003e \u003ch2\u003eBaseline 3: Text-to-Text Transfer Transformer Models\u003c/h2\u003e \u003cp\u003eThird, we explored the capabilities of T5 for dialogue act recognition (Pande et al., \u003cspan citationid=\"CR41\" class=\"CitationRef\"\u003e2023\u003c/span\u003e). T5 is a text-to-text transfer Transformer, renowned for its superior performance across a range of NLP tasks such as machine translation, abstractive summarization, and comprehensive language understanding (Raffel et al., \u003cspan citationid=\"CR46\" class=\"CitationRef\"\u003e2020\u003c/span\u003e). T5\u0026rsquo;s architecture employs multi-head self-attention mechanisms grounded in the Transformer architecture to intricately understand the relationships between tokens in a sequence, allowing it to consider the context of each token irrespective of its position. This is achieved through parallel computations across multiple heads, each specializing in different aspects of token relationships by projecting the sequence into a lower-dimensional space. Specifically, the model structure includes an encoder and a decoder, each with six layers featuring multi-head attention and a feedforward neural network. The encoder computes intermediate representations of inputs, and the decoder generates output tokens based on the encoder\u0026rsquo;s computations and previous outputs. The model was pre-trained in two stages: unsupervised learning on the Colossal Clean Crawled Corpus (C4) and supervised fine-tuning on NLP benchmarks, using a denoising objective for the initial unsupervised phase.\u003c/p\u003e \u003cp\u003eWe selected the \u0026ldquo;small\u0026rdquo; variant of T5, which comprises 60\u0026nbsp;million parameters, due to its efficient training capabilities, making it ideal for application in scenarios with limited resources. This model is capable of processing inputs up to 512 tokens in length, utilizing a vocabulary of 32,128 distinct tokens. In addition, our T5 model utilized eight heads in its multi-head attention sublayers, projecting sequences down to 64 dimensions in each head. We formatted inputs to include both the speaker role and utterance, using task-specific prefixes to guide the T5 model\u0026rsquo;s classification processes. For instance, an utterance by the bravo team leader was formatted as \u003cem\u003eb_tmldr: Keep an eye on everybody\u003c/em\u003e. As each speaker has a unique task-specific prefix, T5 can be readily trained to tailor how it classifies utterances based on who is speaking. The training examples formatted with this method are used to fine-tune the pre-trained T5-small models to fit our purposes for dialogue act recognition.\u003c/p\u003e \u003cp\u003eThrough cross-validation, we determined the optimal hyperparameter values for each model, involving learning rate, warm-up steps, and batch size adjustments. Following the process utilized for training the CRF and BLSTM models, this method reserved one squad\u0026rsquo;s data as the same held-out test set, while utilizing the rest for training and validation, applying early stopping with a patience of 10 epochs to refine our training process. Our evaluation focused on achieving the highest macro-average classification accuracy across all data folds, aiming to enhance the precision of our dialogue analyses.\u003c/p\u003e \u003c/div\u003e \u003cdiv id=\"Sec12\" class=\"Section2\"\u003e \u003ch2\u003eGenerative Pre-Trained Transformer Models\u003c/h2\u003e \u003cp\u003eWe investigate the capabilities of generative pre-trained Transformer models, focusing on GPT-3.5 and GPT-4, proprietary LLMs developed by OpenAI (\u003cspan citationid=\"CR38\" class=\"CitationRef\"\u003e2023\u003c/span\u003e). These models, which are transformer-based models pre-trained to predict the next token in a document, have marked a significant evolution in computational linguistics and NLP, demonstrating their prowess across a variety of benchmarks, including dialogue analysis, text summarization, and machine translation. Notably, GPT-4 has demonstrated exceptional capabilities, placing in the top 10% of participants in simulated tests like the bar exam. This ability to understand and generate natural language in complex scenarios underscores the advanced capabilities of these models. Differing from the other models we have discussed, these proprietary LLMs leverage vast linguistic datasets for training and can often forego traditional fine-tuning on specific datasets. Instead, they employ in-context learning methods through prompting, including zero-shot learning (e.g., using only label definitions) and few-shot learning (e.g., combining a minimal number of labeled examples with label definitions), to match or surpass the predictive accuracy of other established models.\u003c/p\u003e \u003c/div\u003e"},{"header":"Results","content":"\u003cp\u003eWe conducted an evaluation of our team communication analysis framework with four different modeling techniques by dividing the dataset into two distinct segments: data from five squads was allocated for cross-validation, while data from an additional squad was reserved for held-out testing. This process enabled us to select the most effective hyperparameters for each model, which were those that demonstrated the highest average accuracy rate during cross-validation. The highest performing model from each modeling technique is evaluated with the held-out test set, which was not exposed to the cross-validation phase, ensuring an unbiased assessment of the models\u0026rsquo; ability to generalize. For a fair comparison, we used the same data split between the training set and test set across all the models.\u003c/p\u003e \u003cp\u003eThe results from cross-validation for accuracy across all configurations indicate that machine learning models investigated for our team communication framework significantly surpasses the majority-class baseline performance (25.7% in dialogue act recognition) under all hyperparameter settings. Table\u0026nbsp;3 shows the cross-validation performance of the CRFs, BLSTMs, T5s. The results reveal that the best-performing model for dialogue act recognition featured (1) an optimizer regularization parameter of 0.1, an optimizer convergence tolerance of 0.01, and PCA dimensions of 32 for CRFs, (2) multi-task learning, early fusion, and PCA dimensions of 64 for BLSTMs, and (3) a learning rate of 3e-4, a batch size of eight, and warm-up steps constituting 10% of the total training data instances for T5s. The best-performing model hyperparameters, identified through cross-validation for each machine learning algorithm, were applied to the entire training set to develop the final classifiers, which were then assessed using the held-out test set. These fine-tuned, best-performing CRF, BLSTM, and T5 models were then compared against the performance of GPT-3.5 and GPT-4.\u003c/p\u003e \u003cp\u003e​​\u003cb\u003eTable\u0026nbsp;3.\u003c/b\u003e Averaged cross-validation accuracy rates (%) for CRF, BLSTM, and T5. The highest predictive accuracy rates for dialogue act per modeling technique are marked in bold.\u003c/p\u003e \u003cp\u003e \u003cdiv class=\"gridtable\"\u003e\u003ctable float=\"No\" id=\"Taba\" border=\"1\"\u003e \u003ccolgroup cols=\"7\"\u003e \u003cdiv align=\"left\" class=\"colspec\" colname=\"c1\" colnum=\"1\"\u003e\u003c/div\u003e \u003cdiv align=\"left\" class=\"colspec\" colname=\"c2\" colnum=\"2\"\u003e\u003c/div\u003e \u003cdiv align=\"left\" class=\"colspec\" colname=\"c3\" colnum=\"3\"\u003e\u003c/div\u003e \u003cdiv align=\"left\" class=\"colspec\" colname=\"c4\" colnum=\"4\"\u003e\u003c/div\u003e \u003cdiv align=\"left\" class=\"colspec\" colname=\"c5\" colnum=\"5\"\u003e\u003c/div\u003e \u003cdiv align=\"left\" class=\"colspec\" colname=\"c6\" colnum=\"6\"\u003e\u003c/div\u003e \u003cdiv align=\"left\" class=\"colspec\" colname=\"c7\" colnum=\"7\"\u003e\u003c/div\u003e \u003cthead\u003e \u003ctr\u003e \u003cth align=\"left\" colname=\"c1\"\u003e \u003cp\u003eCRF\u003c/p\u003e \u003c/th\u003e \u003cth align=\"left\" colname=\"c2\"\u003e \u003cp\u003eAccuracy Rates\u003c/p\u003e \u003cp\u003e(%)\u003c/p\u003e \u003c/th\u003e \u003cth align=\"left\" colname=\"c3\"\u003e \u003cp\u003eBLSTM\u003c/p\u003e \u003c/th\u003e \u003cth align=\"left\" colname=\"c4\"\u003e \u003cp\u003eAccuracy Rates\u003c/p\u003e \u003cp\u003e(%)\u003c/p\u003e \u003c/th\u003e \u003cth align=\"left\" colname=\"c5\"\u003e \u003cp\u003eT5\u003c/p\u003e \u003c/th\u003e \u003cth align=\"left\" colname=\"c6\"\u003e \u003cp\u003eAccuracy Rates\u003c/p\u003e \u003cp\u003e(%)\u003c/p\u003e \u003c/th\u003e \u003cth align=\"left\" colname=\"c7\"\u003e\u0026nbsp;\u003c/th\u003e \u003c/tr\u003e \u003c/thead\u003e \u003ctbody\u003e \u003ctr\u003e \u003ctd align=\"left\" colname=\"c1\"\u003e \u003cp\u003e{0.1, 0.001, 32}\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c2\"\u003e \u003cp\u003e68.80\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c3\"\u003e \u003cp\u003e{Multi-task, Early, 32}\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c4\"\u003e \u003cp\u003e61.44\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c5\"\u003e \u003cp\u003e{1e-4, 4, 5%}\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c6\"\u003e \u003cp\u003e72.69\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c7\"\u003e\u0026nbsp;\u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd align=\"left\" colname=\"c1\"\u003e \u003cp\u003e{0.1, 0.001, 64}\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c2\"\u003e \u003cp\u003e67.88\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c3\"\u003e \u003cp\u003e{Multi-task, Early, 64}\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c4\"\u003e \u003cp\u003e\u003cb\u003e62.07\u003c/b\u003e\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c5\"\u003e \u003cp\u003e{1e-4, 4, 10%}\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c6\"\u003e \u003cp\u003e72.77\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c7\"\u003e\u0026nbsp;\u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd align=\"left\" colname=\"c1\"\u003e \u003cp\u003e{0.1, 0.001, 128}\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c2\"\u003e \u003cp\u003e64.85\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c3\"\u003e \u003cp\u003e{Multi-task, Early, 128}\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c4\"\u003e \u003cp\u003e61.97\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c5\"\u003e \u003cp\u003e{1e-4, 8, 5%}\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c6\"\u003e \u003cp\u003e72.91\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c7\"\u003e\u0026nbsp;\u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd align=\"left\" colname=\"c1\"\u003e \u003cp\u003e{0.1, 0.01, 32}\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c2\"\u003e \u003cp\u003e\u003cb\u003e68.88\u003c/b\u003e\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c3\"\u003e \u003cp\u003e{Multi-task, Late, 32}\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c4\"\u003e \u003cp\u003e60.22\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c5\"\u003e \u003cp\u003e{1e-4, 8, 10%}\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c6\"\u003e \u003cp\u003e72.28\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c7\"\u003e\u0026nbsp;\u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd align=\"left\" colname=\"c1\"\u003e \u003cp\u003e{0.1, 0.01, 64}\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c2\"\u003e \u003cp\u003e67.87\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c3\"\u003e \u003cp\u003e{Multi-task, Late, 64}\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c4\"\u003e \u003cp\u003e60.19\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c5\"\u003e \u003cp\u003e{3e-4, 4, 5%}\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c6\"\u003e \u003cp\u003e71.90\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c7\"\u003e\u0026nbsp;\u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd align=\"left\" colname=\"c1\"\u003e \u003cp\u003e{0.1, 0.01, 128}\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c2\"\u003e \u003cp\u003e64.87\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c3\"\u003e \u003cp\u003e{Multi-task, Late, 128}\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c4\"\u003e \u003cp\u003e59.77\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c5\"\u003e \u003cp\u003e{3e-4, 4, 10%}\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c6\"\u003e \u003cp\u003e71.98\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c7\"\u003e\u0026nbsp;\u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd align=\"left\" colname=\"c1\"\u003e \u003cp\u003e{1.0, 0.001, 32}\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c2\"\u003e \u003cp\u003e68.76\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c3\"\u003e \u003cp\u003e{Single-task, Early, 32}\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c4\"\u003e \u003cp\u003e62.04\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c5\"\u003e \u003cp\u003e{3e-4, 8, 5%}\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c6\"\u003e \u003cp\u003e73.33\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c7\"\u003e\u0026nbsp;\u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd align=\"left\" colname=\"c1\"\u003e \u003cp\u003e{1.0, 0.001, 64}\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c2\"\u003e \u003cp\u003e67.45\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c3\"\u003e \u003cp\u003e{Single-task, Early, 64}\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c4\"\u003e \u003cp\u003e61.16\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c5\"\u003e \u003cp\u003e{3e-4, 8, 10%}\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c6\"\u003e \u003cp\u003e\u003cb\u003e73.55\u003c/b\u003e\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c7\"\u003e\u0026nbsp;\u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd align=\"left\" colname=\"c1\"\u003e \u003cp\u003e{1.0, 0.001, 128}\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c2\"\u003e \u003cp\u003e65.72\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c3\"\u003e \u003cp\u003e{Single-task, Early, 128}\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c4\"\u003e \u003cp\u003e61.84\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c5\"\u003e\u0026nbsp;\u003c/td\u003e \u003ctd align=\"left\" colspan=\"2\" nameend=\"c7\" namest=\"c6\"\u003e\u0026nbsp;\u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd align=\"left\" colname=\"c1\"\u003e \u003cp\u003e{1.0, 0.01, 32}\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c2\"\u003e \u003cp\u003e68.83\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c3\"\u003e \u003cp\u003e{Single-task, Late, 32}\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c4\"\u003e \u003cp\u003e61.48\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c5\"\u003e\u0026nbsp;\u003c/td\u003e \u003ctd align=\"left\" colspan=\"2\" nameend=\"c7\" namest=\"c6\"\u003e\u0026nbsp;\u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd align=\"left\" colname=\"c1\"\u003e \u003cp\u003e{1.0, 0.01, 64}\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c2\"\u003e \u003cp\u003e67.45\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c3\"\u003e \u003cp\u003e{Single-task, Late, 64}\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c4\"\u003e \u003cp\u003e60.53\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c5\"\u003e\u0026nbsp;\u003c/td\u003e \u003ctd align=\"left\" colspan=\"2\" nameend=\"c7\" namest=\"c6\"\u003e\u0026nbsp;\u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd align=\"left\" colname=\"c1\"\u003e \u003cp\u003e{1.0, 0.01, 128}\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c2\"\u003e \u003cp\u003e65.62\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c3\"\u003e \u003cp\u003e{Single-task, Late, 128}\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c4\"\u003e \u003cp\u003e61.96\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c5\"\u003e\u0026nbsp;\u003c/td\u003e \u003ctd align=\"left\" colspan=\"2\" nameend=\"c7\" namest=\"c6\"\u003e\u0026nbsp;\u003c/td\u003e \u003c/tr\u003e \u003c/tbody\u003e \u003c/colgroup\u003e \u003c/table\u003e\u003c/div\u003e \u003c/p\u003e \u003cp\u003e \u003csup\u003e1\u003c/sup\u003eCRF parameters: \u003cem\u003eoptimizer regularization parameter\u003c/em\u003e, \u003cem\u003eoptimizer convergence tolerance\u003c/em\u003e, \u003cem\u003ereduced PCA dimensions.\u003c/em\u003e\u003c/p\u003e \u003cp\u003e \u003csup\u003e2\u003c/sup\u003eBLSTM parameters: {\u003cem\u003etask modeling type\u003c/em\u003e, \u003cem\u003efusion mode\u003c/em\u003e, \u003cem\u003ereduced PCA dimensions\u003c/em\u003e}\u003c/p\u003e \u003cp\u003e \u003csup\u003e3\u003c/sup\u003eT5 parameters: {\u003cem\u003elearning rate, batch size, warm-up steps\u003c/em\u003e}\u003c/p\u003e \u003cp\u003e \u003cb\u003eRQ1.\u003c/b\u003e Can adding contextual information through iterative stages of prompt engineering enhance the dialogue act recognition performance of the GPT models?\u003c/p\u003e \u003cp\u003eTo address our first research question, we devised a multi-stage engineering process for dialogue act recognition with GPT 3.5, wherein we iteratively refined the classification prompts across four stages (Table\u0026nbsp;\u003cspan refid=\"Tab3\" class=\"InternalRef\"\u003e4\u003c/span\u003e), starting with a foundational prompt in the first stage that introduced definitions for all the communication categories (i.e., all classes in the classification task). This was followed by the second prompt engineering stage, which added illustrative examples for each category to clarify distinctions. The third prompt engineering stage focused on resolving ambiguities by providing additional scenario context and participant roles. The final prompt engineering stage addressed specific confusions between misclassified labels by incorporating detailed clarifications generated by GPT, alongside enriched contextual details. This progressive refinement aimed to enhance the model\u0026rsquo;s understanding and accurate classification of complex military dialogues, showcasing a nuanced approach to improving NLP tasks through targeted prompt adjustments. The OpenAI models utilized were \u0026ldquo;gpt-4\u0026rdquo; and \u0026ldquo;gpt-3.5-turbo,\u0026rdquo; with a temperature setting of 0.2. We opted for a lower temperature to minimize randomness and steer the model\u0026rsquo;s output to be more deterministic and focused.\u003c/p\u003e \u003cp\u003e \u003cdiv class=\"gridtable\"\u003e\u003ctable float=\"Yes\" id=\"Tab3\" border=\"1\"\u003e \u003ccaption language=\"En\"\u003e \u003cdiv class=\"CaptionNumber\"\u003eTable 4\u003c/div\u003e \u003cdiv class=\"CaptionContent\"\u003e \u003cp\u003eStages of improvement illustrated by prompt text snippets.\u003c/p\u003e \u003c/div\u003e \u003c/caption\u003e \u003ccolgroup cols=\"2\"\u003e \u003cdiv align=\"left\" class=\"colspec\" colname=\"c1\" colnum=\"1\"\u003e\u003c/div\u003e \u003cdiv align=\"left\" class=\"colspec\" colname=\"c2\" colnum=\"2\"\u003e\u003c/div\u003e \u003cthead\u003e \u003ctr\u003e \u003cth align=\"left\" colname=\"c1\"\u003e \u003cp\u003ePrompt Improvement Stages\u003c/p\u003e \u003c/th\u003e \u003cth align=\"left\" colname=\"c2\"\u003e \u003cp\u003ePrompt Text Snippets\u003c/p\u003e \u003c/th\u003e \u003c/tr\u003e \u003c/thead\u003e \u003ctbody\u003e \u003ctr\u003e \u003ctd align=\"left\" colname=\"c1\"\u003e \u003cp\u003eStage 1: Label definition\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c2\"\u003e \u003cp\u003e\"Command\": {{\u003c/p\u003e \u003cp\u003e\"command\": \"One user issues a command to another\",\u003c/p\u003e \u003cp\u003e\"grant-permission\": \"Grants permission to perform requested action\",\u003c/p\u003e \u003cp\u003e\"deny-permission\": \"Denies permission to perform requested action\"\u003c/p\u003e \u003cp\u003e}}\u003c/p\u003e \u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd align=\"left\" colname=\"c1\"\u003e \u003cp\u003eStage 2: Category examples\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c2\"\u003e \u003cp\u003eExamples:\u003c/p\u003e \u003cp\u003e'speaker':\"Bravo Team Leader\", 'text': \"alright yeah over\", 'label': \"Ack\"\u003c/p\u003e \u003cp\u003e'speaker':\"Bravo Team Leader\", 'text': \"a little sketchy yeah?\", 'label': \"Ack\"\u003c/p\u003e \u003cp\u003e'speaker':\"Squad Leader\", 'text': \"request QRF time now\", 'label': \"Action-Request\"\u003c/p\u003e \u003cp\u003e'speaker':\"Platoon Leader\", 'text': \"say again 3\u0026ndash;3\", 'label': \"Action-Request\"\u003c/p\u003e \u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd align=\"left\" colname=\"c1\"\u003e \u003cp\u003eStage 3: Added context and Command label clarification\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c2\"\u003e \u003cp\u003eContext:\u003c/p\u003e \u003cp\u003eDuring the virtual army training scenario, Soldiers found themselves in a simulated environment with a tree line providing cover as they observed a village featuring a market square, multi-story buildings, and a shattered church. Within the market square, Soldiers took on the roles of villagers. Notably, one Soldier assumed the role of a parish priest who possessed crucial information about insurgents.\u003c/p\u003e \u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd align=\"left\" colname=\"c1\"\u003e \u003cp\u003eStage 4: Inter-label clarification\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c2\"\u003e \u003cp\u003e'Command' vs 'Provide-Info': 'Command' (\"Secure the perimeter now\") is an order, while 'Provide-Info' (\"The perimeter appears unguarded\") is merely sharing information.\u003c/p\u003e \u003cp\u003e'Command' vs 'Request-Info': 'Command' (\"Report your position\") is an order, whereas 'Request-Info' (\"Are you in position?\") seeks information.\u003c/p\u003e \u003c/td\u003e \u003c/tr\u003e \u003c/tbody\u003e \u003c/colgroup\u003e \u003c/table\u003e\u003c/div\u003e \u003c/p\u003e \u003cp\u003eThe results of our incremental prompting approach are detailed in Table\u0026nbsp;\u003cspan refid=\"Tab4\" class=\"InternalRef\"\u003e5\u003c/span\u003e. We utilized an incremental approach to refine the prompt used on a training dataset, subsequently testing its efficacy on a separate held-out test dataset. As described above, initially, the prompt was structured around a coding sheet employed for manual labeling, comprising a \u0026ldquo;Definitions\u0026rdquo; section derived from the textual content of the coding sheet (Stage 1 in Table\u0026nbsp;\u003cspan refid=\"Tab3\" class=\"InternalRef\"\u003e4\u003c/span\u003e) and an \u0026ldquo;Instructions\u0026rdquo; section outlining the task and the utterances to be labeled. This initial setup yielded an accuracy of 0.369. To improve on these results, we integrated examples from the training set into the prompt\u0026mdash;specifically, four examples per class with their corresponding labels (i.e., few-shot learning), as shown in Stage 2 in Table\u0026nbsp;\u003cspan refid=\"Tab3\" class=\"InternalRef\"\u003e4\u003c/span\u003e. This modification led to an improved system accuracy of approximately 0.425. However, we chose not to pursue the few-shot learning strategy further, as it only marginally improved accuracy while significantly increasing the prompt size, opting instead to focus on zero-shot learning.\u003c/p\u003e \u003cp\u003eTo enhance the classifier's accuracy for participant utterances, we supplemented the Stage 1 prompt with a \u0026ldquo;Context\u0026rdquo; section (Stage 3 in Table\u0026nbsp;\u003cspan refid=\"Tab3\" class=\"InternalRef\"\u003e4\u003c/span\u003e). This addition includes details of the training exercise, the scenario encountered by participants, the character ensemble, and the geographical setting of the village. By offering a high-level overview of the exercise\u0026rsquo;s events, clarifying the roles of squad members, and detailing the team's leadership structure, this broad context improves the model\u0026rsquo;s ability to interpret the intent and relevance of the often brief utterances within the exercise. In addition, our analysis pinpointed that the label \u0026ldquo;Command,\u0026rdquo; despite being the second most frequent in the dataset, exhibited the lowest F1 score at 0.12.\u003c/p\u003e \u003cp\u003e \u003cdiv class=\"gridtable\"\u003e\u003ctable float=\"Yes\" id=\"Tab4\" border=\"1\"\u003e \u003ccaption language=\"En\"\u003e \u003cdiv class=\"CaptionNumber\"\u003eTable 5\u003c/div\u003e \u003cdiv class=\"CaptionContent\"\u003e \u003cp\u003eIterative prompt refinements for GPT dialogue act recognition models based on evaluations using the training set.\u003c/p\u003e \u003c/div\u003e \u003c/caption\u003e \u003ccolgroup cols=\"4\"\u003e \u003cdiv align=\"left\" class=\"colspec\" colname=\"c1\" colnum=\"1\"\u003e\u003c/div\u003e \u003cdiv align=\"left\" class=\"colspec\" colname=\"c2\" colnum=\"2\"\u003e\u003c/div\u003e \u003cdiv align=\"left\" class=\"colspec\" colname=\"c3\" colnum=\"3\"\u003e\u003c/div\u003e \u003cdiv align=\"left\" class=\"colspec\" colname=\"c4\" colnum=\"4\"\u003e\u003c/div\u003e \u003cthead\u003e \u003ctr\u003e \u003cth align=\"left\" colname=\"c1\"\u003e \u003cp\u003ePrompt Content and Details\u003c/p\u003e \u003c/th\u003e \u003cth align=\"left\" colname=\"c2\"\u003e \u003cp\u003eAccuracy\u003c/p\u003e \u003c/th\u003e \u003cth align=\"left\" colname=\"c3\"\u003e \u003cp\u003eF1 Score (macro)\u003c/p\u003e \u003c/th\u003e \u003cth align=\"left\" colname=\"c4\"\u003e \u003cp\u003eMatthews Correlation Coefficient (MCC)\u003c/p\u003e \u003c/th\u003e \u003c/tr\u003e \u003c/thead\u003e \u003ctbody\u003e \u003ctr\u003e \u003ctd align=\"left\" colname=\"c1\"\u003e \u003cp\u003eSimple coding sheet (GPT 3.5)\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c2\"\u003e \u003cp\u003e0.369\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c3\"\u003e \u003cp\u003e0.39\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c4\"\u003e \u003cp\u003e0.344\u003c/p\u003e \u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd align=\"left\" colname=\"c1\"\u003e \u003cp\u003eCoding sheet with examples (GPT 3.5)\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c2\"\u003e \u003cp\u003e0.425\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c3\"\u003e \u003cp\u003e0.30\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c4\"\u003e \u003cp\u003e0.403\u003c/p\u003e \u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd align=\"left\" colname=\"c1\"\u003e \u003cp\u003e\"Command\" clarification\u0026thinsp;+\u0026thinsp;Training Context (GPT 3.5)\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c2\"\u003e \u003cp\u003e0.600\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c3\"\u003e \u003cp\u003e0.47\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c4\"\u003e \u003cp\u003e0.558\u003c/p\u003e \u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd align=\"left\" colname=\"c1\"\u003e \u003cp\u003eWith inter-class clarification (GPT 3.5)\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c2\"\u003e \u003cp\u003e0.631\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c3\"\u003e \u003cp\u003e0.44\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c4\"\u003e \u003cp\u003e0.541\u003c/p\u003e \u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd align=\"left\" colname=\"c1\"\u003e \u003cp\u003eWith inter-class clarification (GPT 4)\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c2\"\u003e \u003cp\u003e\u003cb\u003e0.781\u003c/b\u003e\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c3\"\u003e \u003cp\u003e\u003cb\u003e0.64\u003c/b\u003e\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c4\"\u003e \u003cp\u003e\u003cb\u003e0.730\u003c/b\u003e\u003c/p\u003e \u003c/td\u003e \u003c/tr\u003e \u003c/tbody\u003e \u003c/colgroup\u003e \u003c/table\u003e\u003c/div\u003e \u003c/p\u003e \u003cp\u003eTo address the mislabeling issue, we utilized GPT-4 to generate clarification for the label definitions, leveraging its outstanding performance in language comprehension and generation. It is trained on a larger dataset that incorporates more diverse training data, enhancing its ability to understand language constructs and generate context-aware text (OpenAI, \u003cspan citationid=\"CR38\" class=\"CitationRef\"\u003e2023\u003c/span\u003e). To clarify the prompt for the \u0026ldquo;Command\u0026rdquo; label, we created a separate prompt improvement request incorporating mislabeled examples and the existing coding definition as context. The clarification generated by the GPT-4 model regarding the \u0026ldquo;Command\u0026rdquo; coding definition was used to improve the original prompt. This clarification, along with the new \u0026ldquo;Context\u0026rdquo; section detailing the training scenario and team roles, and including the speaker's role in the utterances, notably enhanced the accuracy to 0.6 and raised the \u0026ldquo;Command\u0026rdquo; label\u0026rsquo;s F1 score to 0.75 for GPT-3.5.\u003c/p\u003e \u003cp\u003eFurther refinement (Stage 4 in Table\u0026nbsp;\u003cspan refid=\"Tab3\" class=\"InternalRef\"\u003e4\u003c/span\u003e) was achieved by examining the confusion matrix of the classifier and instructing GPT-4 to create a clarification prompt to distinguish between the two most conflated classes: \u0026ldquo;Provide-Info\u0026rdquo; and \u0026ldquo;Command.\u0026rdquo; Adding this clarification to the prompt boosted the accuracy to 0.631 when utilizing the GPT-3.5 model for classification. Transitioning the base inference model to GPT-4, we observed a significant accuracy increase to 0.781 for the training dataset (Table\u0026nbsp;\u003cspan refid=\"Tab4\" class=\"InternalRef\"\u003e5\u003c/span\u003e).\u003c/p\u003e \u003cp\u003e \u003cdiv class=\"gridtable\"\u003e\u003ctable float=\"Yes\" id=\"Tab5\" border=\"1\"\u003e \u003ccaption language=\"En\"\u003e \u003cdiv class=\"CaptionNumber\"\u003eTable 6\u003c/div\u003e \u003cdiv class=\"CaptionContent\"\u003e \u003cp\u003eThe dialogue act recognition performance of the highest-performing GPT-3.5 and GPT-4 models on the test set.\u003c/p\u003e \u003c/div\u003e \u003c/caption\u003e \u003ccolgroup cols=\"4\"\u003e \u003cdiv align=\"left\" class=\"colspec\" colname=\"c1\" colnum=\"1\"\u003e\u003c/div\u003e \u003cdiv align=\"left\" class=\"colspec\" colname=\"c2\" colnum=\"2\"\u003e\u003c/div\u003e \u003cdiv align=\"left\" class=\"colspec\" colname=\"c3\" colnum=\"3\"\u003e\u003c/div\u003e \u003cdiv align=\"left\" class=\"colspec\" colname=\"c4\" colnum=\"4\"\u003e\u003c/div\u003e \u003cthead\u003e \u003ctr\u003e \u003cth align=\"left\" colname=\"c1\"\u003e \u003cp\u003ePrompt Content and Details\u003c/p\u003e \u003c/th\u003e \u003cth align=\"left\" colname=\"c2\"\u003e \u003cp\u003eAccuracy\u003c/p\u003e \u003c/th\u003e \u003cth align=\"left\" colname=\"c3\"\u003e \u003cp\u003eF1 Score (macro)\u003c/p\u003e \u003c/th\u003e \u003cth align=\"left\" colname=\"c4\"\u003e \u003cp\u003eMathews Correlation Coefficient (MCC)\u003c/p\u003e \u003c/th\u003e \u003c/tr\u003e \u003c/thead\u003e \u003ctbody\u003e \u003ctr\u003e \u003ctd align=\"left\" colname=\"c1\"\u003e \u003cp\u003eWith inter-class clarification (GPT 3.5)\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c2\"\u003e \u003cp\u003e0.669\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c3\"\u003e \u003cp\u003e0.47\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c4\"\u003e \u003cp\u003e0.600\u003c/p\u003e \u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd align=\"left\" colname=\"c1\"\u003e \u003cp\u003eWith inter-class clarification (GPT 4)\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c2\"\u003e \u003cp\u003e\u003cb\u003e0.773\u003c/b\u003e\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c3\"\u003e \u003cp\u003e\u003cb\u003e0.62\u003c/b\u003e\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c4\"\u003e \u003cp\u003e\u003cb\u003e0.727\u003c/b\u003e\u003c/p\u003e \u003c/td\u003e \u003c/tr\u003e \u003c/tbody\u003e \u003c/colgroup\u003e \u003c/table\u003e\u003c/div\u003e \u003c/p\u003e \u003cp\u003e \u003cb\u003eRQ2.\u003c/b\u003e What is the accuracy of the top-performing GPT models for dialogue act recognition compared to the previous state-of-the-art approach based on T5, as well as other competitive baselines?\u003c/p\u003e \u003cp\u003eAs Stage 4 of the prompt engineering process achieved the highest dialogue act recognition results, we further evaluated the generalization performance of both GPT-3.5 and GPT-4 models using this prompt with the held-out test set. As demonstrated in Table\u0026nbsp;\u003cspan refid=\"Tab5\" class=\"InternalRef\"\u003e6\u003c/span\u003e, the Stage 4 prompt-based GPT models achieved classification accuracy of 0.669 and 0.773 for GPT-3.5 and GPT-4, respectively. Table\u0026nbsp;\u003cspan refid=\"Tab6\" class=\"InternalRef\"\u003e7\u003c/span\u003e shows the accuracy rates of the dialogue act recognition models, including CRF, BLSTM, T5, and GPT-4, on the same held-out test set. GPT-4, augmented with inter-class clarification based on zero-shot learning, achieved the highest dialogue act recognition accuracy at 77.30%, surpassing three competitive baselines. Notably, the GPT-4 model outperformed the previous state-of-the-art models using T5 (Pande et al., \u003cspan citationid=\"CR41\" class=\"CitationRef\"\u003e2023\u003c/span\u003e) with a normalized accuracy improvement of 5.73%. It surpassed the performance of the CRF and BLSTM baselines (Min et al., 2021) as well, and did not require fine-tuning with training data or extensive hyperparameter optimization.\u003c/p\u003e \u003cp\u003e \u003cdiv class=\"gridtable\"\u003e\u003ctable float=\"Yes\" id=\"Tab6\" border=\"1\"\u003e \u003ccaption language=\"En\"\u003e \u003cdiv class=\"CaptionNumber\"\u003eTable 7\u003c/div\u003e \u003cdiv class=\"CaptionContent\"\u003e \u003cp\u003eTest accuracy rates (%) for best performing CRF, BLSTM, T5, and GPT models.\u003c/p\u003e \u003c/div\u003e \u003c/caption\u003e \u003ccolgroup cols=\"2\"\u003e \u003cdiv align=\"left\" class=\"colspec\" colname=\"c1\" colnum=\"1\"\u003e\u003c/div\u003e \u003cdiv align=\"left\" class=\"colspec\" colname=\"c2\" colnum=\"2\"\u003e\u003c/div\u003e \u003cthead\u003e \u003ctr\u003e \u003cth align=\"left\" colname=\"c1\"\u003e \u003cp\u003eModel\u003c/p\u003e \u003c/th\u003e \u003cth align=\"left\" colname=\"c2\"\u003e \u003cp\u003eDialogue Act Recognition\u003c/p\u003e \u003c/th\u003e \u003c/tr\u003e \u003c/thead\u003e \u003ctbody\u003e \u003ctr\u003e \u003ctd align=\"left\" colname=\"c1\"\u003e \u003cp\u003e\u003cb\u003eCRF\u003c/b\u003e\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c2\"\u003e \u003cp\u003e69.42\u003c/p\u003e \u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd align=\"left\" colname=\"c1\"\u003e \u003cp\u003e\u003cb\u003eBLSTM\u003c/b\u003e\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c2\"\u003e \u003cp\u003e64.61\u003c/p\u003e \u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd align=\"left\" colname=\"c1\"\u003e \u003cp\u003e\u003cb\u003eT5\u003c/b\u003e\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c2\"\u003e \u003cp\u003e75.92\u003c/p\u003e \u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd align=\"left\" colname=\"c1\"\u003e \u003cp\u003e\u003cb\u003eGPT-4\u003c/b\u003e\u003c/p\u003e \u003c/td\u003e \u003ctd align=\"left\" colname=\"c2\"\u003e \u003cp\u003e77.30\u003c/p\u003e \u003c/td\u003e \u003c/tr\u003e \u003c/tbody\u003e \u003c/colgroup\u003e \u003c/table\u003e\u003c/div\u003e \u003c/p\u003e \u003cp\u003e \u003cb\u003eRQ3.\u003c/b\u003e What types of classification errors are most prominent in the LLMs\u0026rsquo; inferences, and how can the performance of these NLP models be enhanced based on error analysis findings?\u003c/p\u003e \u003cp\u003eBased on the dialogue act recognition accuracy results, we delved deeper into two highest-performing models (i.e., GPT-4, T5) through an error analysis to scrutinize the labels recognized by our framework in finer detail. To achieve this, we constructed confusion matrices based on the models\u0026rsquo; classification results, aiming to delineate the discrepancies between the inferred labels and the true labels within our test dataset. In Fig.\u0026nbsp;\u003cspan refid=\"Fig2\" class=\"InternalRef\"\u003e2\u003c/span\u003e, the heatmaps of the confusion matrices for the GPT-4 and T5 dialogue act recognition models are shown left and right, respectively, across the nine dialogue acts. The element at position (\u003cem\u003ei\u003c/em\u003e, \u003cem\u003ej\u003c/em\u003e) within each matrix denotes the count of instances where the true label corresponds to row \u003cem\u003ei\u003c/em\u003e, while the classifier's inferred label corresponds to column \u003cem\u003ej\u003c/em\u003e. Correct inferences by the classifier are represented by the counts along the matrix\u0026rsquo;s principal diagonal, where the inferred label matches the true label. Conversely, all other matrix entries represent instances of misclassification.\u003c/p\u003e \u003cp\u003eIn Fig.\u0026nbsp;\u003cspan refid=\"Fig2\" class=\"InternalRef\"\u003e2\u003c/span\u003e, the support (i.e., the number of actual instances of each category) for each true label in the dataset is used to scale the values (i.e., normalization per row). Notably, the mislabeling patterns are similar for both classifiers. \u0026ldquo;Provide-Info,\u0026rdquo; \u0026ldquo;Request-Info,\u0026rdquo; and \u0026ldquo;Other\u0026rdquo; are the labels most commonly confused by both classifiers. We suspect that the short sentences used in the communication during training challenge the classifiers\u0026rsquo; ability to distinguish nuances among these labels. Further investigation of the \u0026ldquo;Other\u0026rdquo; label reveals that both classifiers had difficulty recognizing it correctly; however, the fine-tuned T5 classifier, with a recall rate of 0.54, performed slightly worse than GPT-4, which had a recall rate of 0.67. A potential explanation for this underperformance is that both models encounter challenges learning the characteristics of utterances with the \u0026ldquo;Other\u0026rdquo; label, as this label acts as a catch-all default lacking a precise definition. It is also interesting to note that while fine-tuned T5 often confuses \u0026ldquo;Action-Request\u0026rdquo; with various labels including \u0026ldquo;Command,\u0026rdquo; GPT-4 predominantly misclassifies \u0026ldquo;Action-Request\u0026rdquo; as \u0026ldquo;Request-Info\u0026rdquo; only. In addition, \u0026ldquo;Action-Statement\u0026rdquo; is another label with which fine-tuned T5 has more difficulty than GPT-4 (recall rates: 0.46 vs. 0.73). The lower recall for fine-tuned T5 may indicate a difference in approach between fine-tuning and zero-shot learning. The labels associated with \u0026ldquo;Action,\u0026rdquo; which are not very common in the dataset, benefit from zero-shot learning. Unlike fine-tuned models, GPT-4 with zero-shot learning remains effective despite imbalanced training data because its performance on the label does not depend on the label\u0026rsquo;s frequency in the data; rather performance is driven by the ability to semantically map the prompt instruction to the utterance, utilizing various contextual clues. However, fine-tuned models improve as they are trained on more examples of each label, and it is challenging for these models to adapt to accurately recognize imbalanced labels that rarely appear in the training data.\u003c/p\u003e \u003cp\u003e  \u003c/p\u003e \u003cp\u003eOverall, these results show that using zero-shot learning and iterative prompt refinement with GPT-4 achieved the highest dialogue act recognition accuracy. However, T5 remains a viable option because it is open-source, whereas GPT-4 is proprietary. This distinction becomes more critical as the implementation of our team communication analysis framework scales up, particularly in training settings with multiple sessions where the costs of model deployment are significant. Conversely, in a training environment where T5 models have not been trained with data from that specific training, GPT-4 with zero-shot or few-shot learning might significantly outperform T5, making it potentially more suitable for team communication analytics despite a disparity in cost. Weighing practical benefits, scalability, and cost considerations will be crucial when deploying a runtime version of our team communication analysis framework.\u003c/p\u003e"},{"header":"Discussion","content":"\u003cp\u003eTeam dialogue offers a rich source of data for investigating the processes and behaviors that impact team performance.  Recent advances in deep learning-driven NLP show significant promise for automatically analyzing team communication data while simultaneously providing insights to guide adaptive team training. In this work, we introduce a dialogue act recognition framework that leverages the advanced capabilities of LLMs to support team communication analysis with limited training data, ultimately aiming to provide adaptive team training experiences. Our evaluation of GPT-based models within our dialogue act recognition framework highlighted their superior performance. GPT-4 achieved a recognition accuracy of 77.30% on the held-out test set, surpassing T5, the previous state-of-the-art model, as well as other competitive baseline models including CRF and BLSTM. Particularly, the use of zero-shot learning for GPT models along with iterative prompt enhancements\u0026mdash;inspired by human coding practices\u0026mdash;proved effective in addressing complex dialogue scenarios with minimal data. Error analysis revealed areas for further refinement, particularly in differentiating subtle dialogue acts, and GPT-4 demonstrated stronger recall compared to other models in these cases. This highlights the potential of LLMs to significantly enhance team communication analytics for team training by accurately interpreting and adaptively responding to team communication behaviors, utilizing real-world data from live training exercises.\u003c/p\u003e \u003cp\u003eRegarding our first research question, we adopted an interactive prompt engineering process to optimize the prompt for GPT models.  Our methodology mirrors human coders\u0026rsquo; iterative labeling processes, where they review and discuss the utterances causing the most disagreement post-labeling, refining their coding guidelines. This strategy is intuitive and effective, as demonstrated by our results. With a limited training dataset, our approach made the classifier human-readable and reliable, significantly outperforming the baseline. This iterative process of refining the prompt based on specific areas of confusion, much like iterative discussions among human coders, proves to be a potent strategy for enhancing classifier performance with constrained datasets. For the second research question, we compared the accuracy of the highest-performing LLMs against our framework\u0026rsquo;s baseline models. Specifically, our GPT-4 approach, utilizing the last iteration of prompt engineering, attained the highest accuracy rates overall, as well as higher recall rates on confusing dialogue act labels, relative to T5, the previous state-of-the-art approach. Lastly, our error analysis showed that GPT-4 outperformed T5 in recognizing complex categories like \u0026ldquo;Other\u0026rdquo; and less frequent dialogue act labels like \u0026ldquo;Action-Statement\u0026rdquo;, utilizing zero-shot learning with refined prompts containing high-quality contextual clues, thereby effectively handling nuances and imbalances in training data.\u003c/p\u003e \u003cp\u003eResults from this study encourage the continued exploration of Transformer-based models such as GPT-4 and T5 for developing advanced computational approaches to facilitate team communication analytics. Future work should focus on strategies that: (1) further refine the prompt used for GPT-4 to more effectively represent team communication tasks and clarify properties associated with each label, (2) allow past utterances to provide additional context, enhancing both GPT-4 and T5 model performance in classifying subsequent utterances, and (3) identify characteristics of frequently misclassified utterances based on error analysis results, and use these characteristics to improve NLP models by refining label definitions to enhance label distinction and clarity. Additionally, it will be important to explore other LLMs such as Llama 3 and FLAN-T5 from the transformer language model family to augment the capacity of the team communication analysis framework. Lastly, examining other fundamental dialogue analysis tasks, such as information flow classification, alongside dialogue act recognition, could expand the framework\u0026rsquo;s ability to provide adaptive training experiences that effectively enhance individual trainees\u0026rsquo; learning outcomes. These investigations will deepen our understanding of how team communication influences team performance and guide the development of increasingly effective adaptive team training environments.\u003c/p\u003e"},{"header":"Conclusion","content":"\u003cp\u003eAdaptive training environments capable of analyzing team communication hold significant promise for improving team training outcomes. Developing computational models that can perform robust team communication analytics based on small datasets is a critical challenge. LLMs offer significant potential to address these challenges and enhance dialogue act classification performance. Results from this study show that zero-shot LLMs, refined through prompt engineering, offer a promising means for supporting dialogue act recognition and that prompt refinements, particularly those addressing confusion between dialogue acts, can lead to superior recall rates for challenging labels. Moving forward, it will be important to incorporate these capabilities into future training systems to provide learners and instructors with tailored feedback to support and improve team communication behaviors. These capabilities will allow instructors and researchers to gain insights into the team actions and processes that impact team coordination.\u003c/p\u003e"},{"header":"Declarations","content":"\u003cp\u003e\u003cstrong\u003eFunding.\u0026nbsp;\u003c/strong\u003eThis research was supported by funding from the U.S. Army Combat Capabilities Development Command, Soldier Center under cooperative agreement W912CG-19-2-0001.\u0026nbsp;\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eAuthors\u0026rsquo; contributions.\u0026nbsp;\u003c/strong\u003eAll authors contributed to the study conception and design. Data analysis was performed by Vikram Kumaran, Jay Pande, and Wookhee Min. The first draft of the manuscript was written by Randall Spain and Wookhee Min, and all authors commented on previous versions of the manuscript. All authors read and approved the final manuscript.\u003cstrong\u003e\u0026nbsp;\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eAcknowledgements\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe research described herein has been sponsored by the U.S. Army Combat Capabilities Development Command under cooperative agreement W912CG-19-2-0001. The statements and opinions expressed in this article do not necessarily reflect the position or the policy of the United States Government, and no official endorsement should be inferred.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eConflicts of interest/Competing interests.\u0026nbsp;\u003c/strong\u003eNo potential conflicts of interest.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eAvailability of data and material.\u0026nbsp;\u003c/strong\u003eData and materials created for this research are available upon request. Please direct all inquiries to the corresponding author.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eCode availability.\u0026nbsp;\u003c/strong\u003eCode created for this research is available upon request. Please direct all inquiries to the corresponding author.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eStatements on Open Data, Ethics and Conflict of Interest.\u0026nbsp;\u003c/strong\u003eThis study was conducted with the IRB approval of North Carolina State University. There is no potential conflict of interest in this work.\u0026nbsp;\u003c/p\u003e"},{"header":"References","content":"\u003col\u003e\n\u003cli\u003eAlnasyan, B., Basheri, M., \u0026amp; Alassafi, M. (2024). The Power of Deep Learning Techniques for Predicting Student Performance in Virtual Learning Environments: A Systematic Literature Review. \u003cem\u003eComputers and Education: Artificial Intelligence\u003c/em\u003e, 100231.\u003c/li\u003e\n\u003cli\u003eBaber, C., Leggett, A., Attfield, S., \u0026amp; Elliott, E. (2022). Using speech act theory to apply automated communications analysis to distributed sensemaking. In \u003cem\u003eProceedings of the Human Factors and Ergonomics Society Annual Meeting\u003c/em\u003e (Vol. 66, No. 1, pp. 55\u0026ndash;59). SAGE Publications.\u003c/li\u003e\n\u003cli\u003eBadrinath, S., \u0026amp; Balakrishnan, H. (2022). Automatic speech recognition for air traffic control communications. \u003cem\u003eTransportation research record, 2676\u003c/em\u003e(1), 798\u0026ndash;810.\u003c/li\u003e\n\u003cli\u003eBothe, C., Weber, C., Magg, S., \u0026amp; Wermter, S. (2018). A context-based approach for dialogue act recognition using simple recurrent neural networks. \u003cem\u003earXiv preprint arXiv:1805.06280\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eBowers, C. A., Jentsch, F., Salas, E., \u0026amp; Braun, C. C. (1998). Analyzing communication sequences for team training needs assessment. \u003cem\u003eHuman Factors, 40\u003c/em\u003e(4), 672\u0026ndash;679.\u003c/li\u003e\n\u003cli\u003eButchibabu, A., Sparano-Huiban, C., Sonenberg, L., \u0026amp; Shah, J. (2016). Implicit coordination strategies for effective team communication.\u003cem\u003e Human Factors, 58\u003c/em\u003e(4), 595\u0026ndash;610.\u003c/li\u003e\n\u003cli\u003eCannon-Bowers, J. A., Salas, E., \u0026amp; Converse, S. (1993). Shared mental models in expert team decision making. \u003cem\u003eCurrent issues in individual and group decision making. \u003c/em\u003eLawrence Erlbaum, 221\u0026ndash;246.\u003c/li\u003e\n\u003cli\u003eCarpenter, D., Emerson, A., Mott, B. W., Saleh, A., Glazewski, K. D., Hmelo-Silver, C. E., \u0026amp; Lester, J. C. (2020). Detecting off-task behavior from student dialogue in game-based collaborative learning. In \u003cem\u003eArtificial Intelligence in Education: 21st International Conference, AIED 2020, Ifrane, Morocco, July 6\u0026ndash;10, 2020, Proceedings, Part I 21\u003c/em\u003e (pp. 55\u0026ndash;66). Springer International Publishing.\u003c/li\u003e\n\u003cli\u003eCarpenter, D., Min, W., Lee, S., Ozogul, G., Zheng, X., \u0026amp; Lester, J. (in press). Assessing Student Explanations with Large Language Models Using Fine-Tuning and Few-Shot Learning. To appear in \u003cem\u003eProceedings of the Nineteenth Workshop on Innovative Use of NLP for Building Educational Applications\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eChen, Z., Yang, R., Zhao, Z., Cai, D., \u0026amp; He, X. (2018, June). Dialogue act recognition via crf-attentive structured network.. In \u003cem\u003eThe 41st International ACM SIGIR Conference on Research \u0026amp; Development in Information Retrieval (SIGIR \u0026apos;18)\u003c/em\u003e. Association for Computing Machinery, 225\u0026ndash;234. https://doi.org/10.1145/3209978.3209997\u003c/li\u003e\n\u003cli\u003eDillenbourg, P., J\u0026auml;rvel\u0026auml;, S., \u0026amp; Fischer, F. (2009). \u003cem\u003eThe evolution of research on computer-supported collaborative learning: From design to orchestration\u003c/em\u003e (pp. 3\u0026ndash;19). Springer Netherlands.\u003c/li\u003e\n\u003cli\u003eEga\u0026ntilde;a, A., Aldabe, I., \u0026amp; de Lacalle, O. L. (2023, June). Exploration of Annotation Strategies for Automatic Short Answer Grading. In \u003cem\u003eInternational Conference on Artificial Intelligence in Education\u003c/em\u003e (pp. 377-388). Cham: Springer Nature Switzerland.\u003c/li\u003e\n\u003cli\u003eEmerson, A., Min, W., Azevedo, R., \u0026amp; Lester, J. (2022). Early prediction of student knowledge in game‐based learning with distributed representations of assessment questions. \u003cem\u003eBritish Journal of Educational Technology 54\u003c/em\u003e(1), 40\u0026ndash;57.\u003c/li\u003e\n\u003cli\u003eEntin, E. E., \u0026amp; Serfaty, D. (1999). Adaptive team coordination. \u003cem\u003eHuman Factors, 41\u003c/em\u003e(2), 312\u0026ndash;325.\u003c/li\u003e\n\u003cli\u003eFirdaus, M., Golchha, H., Ekbal, A., \u0026amp; Bhattacharyya, P. (2021). A deep multi-task model for dialogue act classification, intent detection and slot filling. \u003cem\u003eCognitive Computation,\u003c/em\u003e \u003cem\u003e13\u003c/em\u003e, 626\u0026ndash;645. https://doi.org/10.1007/s12559-020-09718-4\u003c/li\u003e\n\u003cli\u003eGanesh, A., Palmer, M., \u0026amp; Kann, K. (2023, July). A survey of challenges and methods in the computational modeling of multi-party dialog. In \u003cem\u003eProceedings of the 5th Workshop on NLP for Conversational AI (NLP4ConvAI 2023)\u003c/em\u003e, pages 140\u0026ndash;154. Association for Computational Linguistics.\u003c/li\u003e\n\u003cli\u003eGarosi, E., Kalantari, R., Zanjirani Farahani, A., Zuaktafi, M., Hosseinzadeh Roknabadi, E., \u0026amp; Bakhshi, E. (2020). Concerns about verbal communication in the operating room: A field study. \u003cem\u003eHuman Factors, 62\u003c/em\u003e(6), 940\u0026ndash;953.\u003c/li\u003e\n\u003cli\u003eGorman, J. C., Cooke, N. J., \u0026amp; Winner, J. L. (2017). Measuring team situation awareness in decentralized command and control environments. In Sals, E. (Ed.) \u003cem\u003eSituational awareness\u003c/em\u003e (pp. 183\u0026ndash;196). Routledge.\u003c/li\u003e\n\u003cli\u003eGorman, J. C., Grimm, D. A., Stevens, R. H., Galloway, T., Willemsen-Dunlap, A. M., \u0026amp; Halpin, D. J. (2020). Measuring real-time team cognition during team training. \u003cem\u003eHuman Factors, 62\u003c/em\u003e(5), 825\u0026ndash;860.\u003c/li\u003e\n\u003cli\u003eGrau, S., Sanchis, E., Castro, M. J., \u0026amp; Vilar, D. (2004). Dialogue act classification using a Bayesian approach. In \u003cem\u003e9th Conference Speech and Computer\u003c/em\u003e. Retrieved from: https://www.isca-archive.org/specom_2004/grau04_specom.pdf\u003c/li\u003e\n\u003cli\u003eGraves, A., \u0026amp; Schmidhuber, J. (2005). Framewise phoneme classification with bidirectional LSTM and other neural network architectures. \u003cem\u003eNeural networks\u003c/em\u003e, \u003cem\u003e18\u003c/em\u003e(5-6), 602\u0026ndash;610.\u003c/li\u003e\n\u003cli\u003eGrimm, D. A., Gorman, J. C., Robinson, E., \u0026amp; Winner, J. (2022, September). Measuring adaptive team coordination in an enroute care training scenario. In \u003cem\u003eProceedings of the Human Factors and Ergonomics Society Annual Meeting\u003c/em\u003e (Vol. 66, No. 1, pp. 50\u0026ndash;54). SAGE Publications.\u003c/li\u003e\n\u003cli\u003eGupta, A., Carpenter, D., Min, W., Rowe, J., Azevedo, R., \u0026amp; Lester, J. (2022). Enhancing multimodal goal recognition in open-world games with natural language player reflections. In S. G. Ware, \u0026amp; M. Eger (Eds.), \u003cem\u003eProceedings of the Eighteenth Annual AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment \u003c/em\u003e(pp. 37\u0026ndash;44). AAAI Press.\u003c/li\u003e\n\u003cli\u003eIshizaki, M., \u0026amp; Kato, T. (1998, August). Exploring the characteristics of multi-party dialogues. In \u003cem\u003e36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1\u003c/em\u003e (pp. 583\u0026ndash;589).\u003c/li\u003e\n\u003cli\u003eJensen, E., L. Pugh, S., \u0026amp; K. D\u0026apos;Mello, S. (2021, April). A deep transfer learning approach to modeling teacher discourse in the classroom. In LAK21: \u003cem\u003e11th international learning analytics and knowledge conference\u003c/em\u003e (pp. 302-312).\u003c/li\u003e\n\u003cli\u003eJeong, H., Hmelo-Silver, C. E., \u0026amp; Jo, K. (2019). Ten years of computer-supported collaborative learning: A meta-analysis of CSCL in STEM education during 2005\u0026ndash;2014. \u003cem\u003eEducational Research Review\u003c/em\u003e, \u003cem\u003e28\u003c/em\u003e, 100284.\u003c/li\u003e\n\u003cli\u003eJohn, P., Brooks, B., \u0026amp; Schriever, U. (2019). Speech acts in professional maritime discourse: A pragmatic risk analysis of bridge team communication directives and commissives in full-mission simulation. \u003cem\u003eJournal of Pragmatics, 140\u003c/em\u003e, 12\u0026ndash;21.\u003c/li\u003e\n\u003cli\u003eJohnston, J. H., Phillips, H. L., Milham, L. M., Riddle, D. L., Townsend, L. N., DeCostanza, A. H., Patton, D. J., Cox, K. R., \u0026amp; Fitzhugh, S. M. (2019). A team training field research study: extending a theory of team development. \u003cem\u003eFrontiers in Psychology\u003c/em\u003e, 10, 1480.\u003c/li\u003e\n\u003cli\u003eJohnston, J., Sottilare, R., Sinatra, A. M., \u0026amp; Burke, C. S. (Eds.). (2018). \u003cem\u003eBuilding intelligent tutoring systems for teams: What matters.\u003c/em\u003e Emerald Group Publishing.\u003c/li\u003e\n\u003cli\u003eKingma, D.P., Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.\u003c/li\u003e\n\u003cli\u003eKumaran, V., Rowe, J., Mott, B., \u0026amp; Lester, J. (2023). SCENECRAFT: Automating interactive narrative scene generation in digital games with large language models. In\u003cem\u003e Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment\u003c/em\u003e (Vol. 19, No. 1, pp. 86\u0026ndash;96).\u003c/li\u003e\n\u003cli\u003eMarlow, S. L., Lacerenza, C. N., Paoletti, J., Burke, C. S., \u0026amp; Salas, E. (2018). Does team communication represent a one-size-fits-all approach?: A meta-analysis of team communication and performance. \u003cem\u003eOrganizational Behavior and Human Decision Processes, 144\u003c/em\u003e, 145\u0026ndash;170.\u003c/li\u003e\n\u003cli\u003eMayfield, E., Adamson, D., \u0026amp; Rose, C. (2012, July). Hierarchical conversation structure prediction in multi-party chat. In \u003cem\u003eProceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue\u003c/em\u003e (pp. 60\u0026ndash;69).\u003c/li\u003e\n\u003cli\u003eMin, W., Spain, R., Saville, J. D., Mott, B., Brawner, K., Johnston, J., \u0026amp; Lester, J. (2021, June). Multidimensional team communication modeling for adaptive team training: A hybrid deep learning and graphical modeling framework. In I. Roll, D. McNamara, S. Sosnovsky, R. Luckin, \u0026amp; V. Dimitrova (Eds.), \u003cem\u003eLecture notes in computer science: Vol. 12748. Artificial intelligence in education\u003c/em\u003e (pp. 293\u0026ndash;305). Springer, Cham.\u003c/li\u003e\n\u003cli\u003eMoore, S., Tong, R., Singh, A., Liu, Z., Hu, X., Lu, Y., Liang, J., Cao, C., Khosravi, H., Denny, P., Brooks, C., \u0026amp; Stamper, J. (2023, June). Empowering education with LLMs-the next-gen interface and content generation. In\u003cem\u003e International Conference on Artificial Intelligence in Education\u003c/em\u003e (pp. 32\u0026ndash;37). Cham: Springer Nature Switzerland.\u003c/li\u003e\n\u003cli\u003eMorris, W., Crossley, S., Holmes, L., Ou, C., Dascalu, M., \u0026amp; McNamara, D. (2024). Formative feedback on student-authored summaries in intelligent textbooks using large language models. \u003cem\u003eInternational Journal of Artificial Intelligence in Education\u003c/em\u003e. https://doi.org/10.1007/s40593-024-00395-0\u003c/li\u003e\n\u003cli\u003eM\u0026uuml;ller, A. C., \u0026amp; Behnke, S. (2014). PyStruct: learning structured prediction in python. \u003cem\u003eJournal of Machine Learning Research\u003c/em\u003e, \u003cem\u003e15\u003c/em\u003e(1), 2055\u0026ndash;2060.\u003c/li\u003e\n\u003cli\u003eOpenAI. (2023). GPT-4 Technical Report. Retrieved from: https://cdn.openai.com/papers/gpt-4.pdf\u003c/li\u003e\n\u003cli\u003eOuahrani, L., \u0026amp; Bennouar, D. (2024). Paraphrase Generation and Supervised Learning for Improved Automatic Short Answer Grading. \u003cem\u003eInternational Journal of Artificial Intelligence in Education\u003c/em\u003e, 1-44.\u003c/li\u003e\n\u003cli\u003eOwan, V. J., Abang, K. B., Idika, D. O., Etta, E. O., \u0026amp; Bassey, B. A. (2023). Exploring the potential of artificial intelligence tools in educational measurement and assessment.\u003cem\u003e EURASIA Journal of Mathematics, Science and Technology Education, 19\u003c/em\u003e(8), em2307.\u003c/li\u003e\n\u003cli\u003ePande, J., Min, W., Spain, R. D., Saville, J. D., \u0026amp; Lester, J. (2023). Robust team communication analytics with transformer-based dialogue modeling. In \u003cem\u003eInternational Conference on Artificial Intelligence in Education\u003c/em\u003e (pp. 639\u0026ndash;650). Cham: Springer Nature Switzerland.\u003c/li\u003e\n\u003cli\u003ePark, K., Sohn, H., Mott, B., Min, W., Saleh, A., Glazewski, K., Hmelo-Silver, C. E., \u0026amp; Lester, J. (2021, April). Detecting disruptive talk in student chat-based discussion within collaborative game-based learning environments. In \u003cem\u003eLAK21: 11th International Learning Analytics and Knowledge Conference\u003c/em\u003e (pp. 405\u0026ndash;415). Society for Learning Analytics Research.\u003c/li\u003e\n\u003cli\u003ePark, K., Sohn, H., Min, W., Mott, B., Glazewski, K., Hmelo-Silver, C., \u0026amp; Lester, J. (2022). Disruptive talk detection in multi-party dialogue within collaborative learning environments with a regularized user-aware network. In O. Lemon, D. Hakkani-Tur, J. J. Li, A. Ashrafzadeh, D. Hern\u0026aacute;ndez Garcia, M. Alikhani, D. Vandyke, \u0026amp; O. Du\u0026scaron;ek, (Eds.), \u003cem\u003eProceedings of the 23rd Annual Meeting of the Special Interest Group on Discourse and Dialogue\u003c/em\u003e (pp. 490\u0026ndash;499). Association for Computational Linguistics.\u003c/li\u003e\n\u003cli\u003ePennington, J., Socher, R., \u0026amp; Manning, C. D. (2014). Glove: Global vectors for word representation. In \u003cem\u003eProceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)\u003c/em\u003e (pp. 1532\u0026ndash;1543).\u003c/li\u003e\n\u003cli\u003ePeters, M.E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., Zettlemoyer, L. (2018).Deep contextualized word representations. arXiv preprint arXiv:1802.05365.\u003c/li\u003e\n\u003cli\u003eRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., \u0026amp; Liu, P. J. (2020). Exploring the limits of transfer learning with a unified text-to-text transformer. \u003cem\u003eThe Journal of Machine Learning Research 21\u003c/em\u003e(1), 5485\u0026ndash;5551.\u003c/li\u003e\n\u003cli\u003eRahimi, Z., \u0026amp; Litman, D. (2018). Weighting model based on group dynamics to measure convergence in multi-party dialogue. In \u003cem\u003eProceedings of the 19th annual SIGdial meeting on discourse and dialogue\u003c/em\u003e (pp. 385\u0026ndash;390).\u003c/li\u003e\n\u003cli\u003eRamesh, D., \u0026amp; Sanampudi, S. K. (2022). An automated essay scoring systems: A systematic literature review. \u003cem\u003eArtificial Intelligence Review 55\u003c/em\u003e(3), 2495\u0026ndash;2527.\u003c/li\u003e\n\u003cli\u003eRobinson, F. E., Huffman, L. C. S., Bevington, L. C. D., French, D., Rothwell, C., Stucky, L. C., Tharp, M., \u0026amp; Hughies, A. (2023). Team coordination style is an adaptive, emergent property of interactions between critical care air transport team personnel. \u003cem\u003eAir Medical Journal, 42\u003c/em\u003e(3), 174\u0026ndash;183.\u003c/li\u003e\n\u003cli\u003eRuseti, S., Paraschiv, I., Dascalu, M., \u0026amp; McNamara, D. S. (2024). Automated Pipeline for Multi-lingual Automated Essay Scoring with ReaderBench. \u003cem\u003eInternational Journal of Artificial Intelligence in Education\u003c/em\u003e, 1-22.\u003c/li\u003e\n\u003cli\u003eSaville, J. D., Spain, R. D., Johnston, J. H., \u0026amp; Lester, J. C. (2021, June). Exploration of team communication behaviors from a live training event. In \u003cem\u003eInternational Conference on Applied Human Factors and Ergonomics\u003c/em\u003e (pp. 101\u0026ndash;108). Cham: Springer International Publishing.\u003c/li\u003e\n\u003cli\u003eSaville, J., Spain, R., Johnston, J., \u0026amp; Lester, J. (2022). An analysis of squad communication behaviors during a field-training exercise to support tactical decision making. In J. Wright and D. Barber (Eds.)\u003cem\u003e Human Factors and Simulation. Vol. 30 \u003c/em\u003e(pp. 109\u0026ndash;116)\u003cem\u003e. \u003c/em\u003eAHFE Open Access.\u003c/li\u003e\n\u003cli\u003eSawatzki, J., Schlippe, T., \u0026amp; Benner-Wickner, M. (2022). Deep learning techniques for automatic short answer grading: Predicting scores for English and German answers. In E. C. K. Cheng, R. B. Koul, T. Wang, \u0026amp; Y. Xinguo (Eds.), \u003cem\u003eArtificial Intelligence in Education: Emerging Technologies, Models and Applications, LNDECT, vol. 104\u003c/em\u003e (pp. 65\u0026ndash;75). Springer.\u003c/li\u003e\n\u003cli\u003eShahriar, T., Matsuda, N., \u0026amp; Ramos, K. (2023). Assertion enhanced few-shot learning: Instructive technique for large language models to generate educational explanations. \u003cem\u003earXiv preprint arXiv\u003c/em\u003e:2312.03122.\u003c/li\u003e\n\u003cli\u003eShamekhi, A., Liao, Q. V., Wang, D., Bellamy, R. K., \u0026amp; Erickson, T. (2018, April). Face Value? Exploring the effects of embodiment for a group facilitation agent. In \u003cem\u003eProceedings of the 2018 CHI conference on human factors in computing systems\u003c/em\u003e (pp. 1\u0026ndash;13).\u003c/li\u003e\n\u003cli\u003eSottilare, R. A., Burke, C., Salas, E., Sinatra, A. M., Johnston, J. H., \u0026amp; Gilbert, S. B. (2018). Designing adaptive instruction for teams: A meta-analysis. \u003cem\u003eInternational Journal of Artificial Intelligence in Education\u003c/em\u003e, \u003cem\u003e28\u003c/em\u003e, 225\u0026ndash;264. \u003c/li\u003e\n\u003cli\u003eStolcke, A., Ries, K., Coccaro, N., Shriberg, E., Bates, R., Jurafsky, D., ... \u0026amp; Meteer, M. (2000). Dialogue act modeling for automatic tagging and recognition of conversational speech.\u003cem\u003e Computational linguistics, 26\u003c/em\u003e(3), 339\u0026ndash;373.\u003c/li\u003e\n\u003cli\u003eStucky, C. H., De Jong, M. J., \u0026amp; Kabo, F. W. (2020). Military surgical team communication: Implications for safety. \u003cem\u003eMilitary Medicine, 185\u003c/em\u003e(3-4), e448\u0026ndash;e456.\u003c/li\u003e\n\u003cli\u003eSu, L., Kaplan, S., Burd, R., Winslow, C., Hargrove, A., \u0026amp; Waller, M. (2017). Trauma resuscitation: Can team behaviours in the prearrival period predict resuscitation performance?. \u003cem\u003eBMJ Simulation \u0026amp; Technology Enhanced Learning\u003c/em\u003e, \u003cem\u003e3\u003c/em\u003e(3), 106.\u003c/li\u003e\n\u003cli\u003eSurendran, D., \u0026amp; Levow, G. A. (2006). Dialog act tagging with support vector machines and hidden Markov models. In \u003cem\u003eInterspeech, ICSLP \u003c/em\u003e(pp. 1950\u0026ndash;1953). Retrieved from: https://faculty.washington.edu/levow/papers/IS06_da.pdf\u003c/li\u003e\n\u003cli\u003eTan, M., Wang, D., Gao, Y., Wang, H., Potdar, S., Guo, X., Chang, S., \u0026amp; Yu, M. (2019, November). Context-aware conversation thread detection in multi-party chat. In \u003cem\u003eProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)\u003c/em\u003e (pp. 6456\u0026ndash;6461).\u003c/li\u003e\n\u003cli\u003eWahlster, W. (2023). Understanding computational dialogue understanding. \u003cem\u003ePhilosophical Transactions of the Royal Society A, 381\u003c/em\u003e(2251), 20220049.\u003c/li\u003e\n\u003cli\u003eWang, D., Shan, D., Zheng, Y., Guo, K., Chen, G., \u0026amp; Lu, Y. (2023, July). Can chatgpt detect student talk moves in classroom discourse? a preliminary comparison with bert. In \u003cem\u003eProceedings of the 16th International Conference on Educational Data Mining\u003c/em\u003e (pp. 515-519). International Educational Data Mining Society.\u003c/li\u003e\n\u003cli\u003eWillms, C., Houy, C., Rehse, J. R., Fettke, P., \u0026amp; Kruijff-Korbayov\u0026aacute;, I. (2019). Team communication processing and process analytics for supporting robot-assisted emergency response. In \u003cem\u003e2019 IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR)\u003c/em\u003e (pp. 216\u0026ndash;221). IEEE.\u003c/li\u003e\n\u003cli\u003evan den Oever, F., \u0026amp; Schraagen, J. M. (2021). Team communication patterns in critical situations. \u003cem\u003eJournal of Cognitive Engineering and Decision Making, 15\u003c/em\u003e(1), 28\u0026ndash;51.\u003c/li\u003e\n\u003cli\u003eYamaura, M., Fukuda, I., \u0026amp; Uto, M. (2023, June). Neural Automated Essay Scoring Considering Logical Structure. In \u003cem\u003eInternational Conference on Artificial Intelligence in Education\u003c/em\u003e (pp. 267-278). Cham: Springer Nature Switzerland.\u003c/li\u003e\n\u003c/ol\u003e"}],"fulltextSource":"","fullText":"","funders":[],"hasAdminPriorityOnWorkflow":false,"hasManuscriptDocX":true,"hasOptedInToPreprint":true,"hasPassedJournalQc":"","hasAnyPriority":false,"hideJournal":false,"highlight":"","institution":"","isAcceptedByJournal":true,"isAuthorSuppliedPdf":false,"isDeskRejected":"","isHiddenFromSearch":false,"isInQc":false,"isInWorkflow":false,"isPdf":false,"isPdfUpToDate":true,"isWithdrawnOrRetracted":false,"journal":{"display":true,"email":"info@researchsquare.com","identity":"international-journal-of-artificial-intelligence-in-education","isNatureJournal":false,"hasQc":true,"allowDirectSubmit":false,"externalIdentity":"aied","sideBox":"Learn more about [International Journal of Artificial Intelligence in Education](http://link.springer.com/journal/40593)","snPcode":"40593","submissionUrl":"https://submission.nature.com/new-submission/40593/3","title":"International Journal of Artificial Intelligence in Education","twitterHandle":"","acdcEnabled":true,"dfaEnabled":true,"editorialSystem":"stoa","reportingPortfolio":"Springer Hybrid","inReviewEnabled":true,"inReviewRevisionsEnabled":false},"keywords":"Natural language processing, Large language models, Team communication analysis, Dialogue act recognition","lastPublishedDoi":"10.21203/rs.3.rs-4565500/v1","lastPublishedDoiUrl":"https://doi.org/10.21203/rs.3.rs-4565500/v1","license":{"name":"CC BY 4.0","url":"https://creativecommons.org/licenses/by/4.0/"},"manuscriptAbstract":"\u003cp\u003eAdaptive training environments that can analyze team communication content and provide remediation to facilitate team coordination offer great potential for enhancing adaptive training systems for teams. Developing computational models that can perform robust team communication analytics based on small datasets is challenging. Large language models (LLMs) offer significant potential to address these challenges and enhance dialogue act classification performance using zero-shot and few-shot learning. This paper evaluates LLMs against previous state-of-the-art methods, with an emphasis on dialogue act recognition performance and error analysis for identifying frequently misclassified instances. Results from a small team communication dataset indicate that zero-shot LLMs, particularly those utilizing GPT-4 and refined through robust prompt engineering, achieve significant classification performance improvements in dialogue act recognition compared to previous state-of-the-art transformer-based models fine-tuned with team communication data. Error analysis shows that the prompt refinements, especially those aimed at clarifying confusion between dialogue acts, result in superior recall rates for challenging dialogue act labels by effectively handling complex dialogue scenarios and ambiguities within communication data. Our transformer-based framework demonstrates its effectiveness in achieving high accuracy rates in dialogue act recognition with minimal training data, underscoring its potential to enhance team training programs by providing adaptive feedback. This approach paves the way for developing AI-enabled training systems that can adapt to the dynamic communication styles of different teams.\u003c/p\u003e","manuscriptTitle":"Applying Large Language Models to Enhance Dialogue and Communication Analysis for Adaptive Team Training","msid":"","msnumber":"","nonDraftVersions":[{"code":1,"date":"2024-07-01 17:33:49","doi":"10.21203/rs.3.rs-4565500/v1","editorialEvents":[{"type":"communityComments","content":0},{"type":"decision","content":"Revision requested","date":"2024-10-14T03:10:27+00:00","index":"","fulltext":""},{"type":"editorInvitedReview","content":"","date":"2024-09-17T09:11:24+00:00","index":"hide","fulltext":""},{"type":"editorInvitedReview","content":"","date":"2024-08-26T01:21:11+00:00","index":"hide","fulltext":""},{"type":"reviewerAgreed","content":"47604957734840285083741710727149405056","date":"2024-07-31T10:07:57+00:00","index":"hide","fulltext":""},{"type":"editorInvitedReview","content":"","date":"2024-07-15T15:01:29+00:00","index":"hide","fulltext":""},{"type":"reviewerAgreed","content":"231371493531617912476130651585688064933","date":"2024-06-24T19:18:42+00:00","index":"hide","fulltext":""},{"type":"reviewerAgreed","content":"31257132488333274098697057670027123779","date":"2024-06-23T17:52:17+00:00","index":"hide","fulltext":""},{"type":"reviewersInvited","content":"","date":"2024-06-23T15:12:08+00:00","index":"","fulltext":""},{"type":"editorAssigned","content":"","date":"2024-06-14T10:27:43+00:00","index":"","fulltext":""},{"type":"checksComplete","content":"","date":"2024-06-14T10:26:57+00:00","index":"","fulltext":""},{"type":"submitted","content":"International Journal of Artificial Intelligence in Education","date":"2024-06-11T16:24:24+00:00","index":"","fulltext":""}],"status":"published","journal":{"display":true,"email":"info@researchsquare.com","identity":"international-journal-of-artificial-intelligence-in-education","isNatureJournal":false,"hasQc":true,"allowDirectSubmit":false,"externalIdentity":"aied","sideBox":"Learn more about [International Journal of Artificial Intelligence in Education](http://link.springer.com/journal/40593)","snPcode":"40593","submissionUrl":"https://submission.nature.com/new-submission/40593/3","title":"International Journal of Artificial Intelligence in Education","twitterHandle":"","acdcEnabled":true,"dfaEnabled":true,"editorialSystem":"stoa","reportingPortfolio":"Springer Hybrid","inReviewEnabled":true,"inReviewRevisionsEnabled":false}}],"origin":"","ownerIdentity":"b63dd8a1-dee0-4422-8399-d77c5363ed31","owner":[],"postedDate":"July 1st, 2024","published":true,"recentEditorialEvents":[],"rejectedJournal":[],"revision":"","amendment":"","status":"published-in-journal","subjectAreas":[],"tags":[],"updatedAt":"2025-05-19T16:04:53+00:00","versionOfRecord":{"articleIdentity":"rs-4565500","link":"https://doi.org/10.1007/s40593-025-00479-5","journal":{"identity":"international-journal-of-artificial-intelligence-in-education","isVorOnly":false,"title":"International Journal of Artificial Intelligence in Education"},"publishedOn":"2025-05-14 15:57:57","publishedOnDateReadable":"May 14th, 2025"},"versionCreatedAt":"2024-07-01 17:33:49","video":"","vorDoi":"10.1007/s40593-025-00479-5","vorDoiUrl":"https://doi.org/10.1007/s40593-025-00479-5","workflowStages":[]},"version":"v1","identity":"rs-4565500","journalConfig":"researchsquare"},"__N_SSP":true},"page":"/article/[identity]/[[...version]]","query":{"identity":"rs-4565500","version":["v1"]},"buildId":"f22rFx76zg5YRmNXsU5vb","isFallback":false,"dynamicIds":[87228],"gssp":true,"scriptLoader":[]}</script><div hidden="" id="snipcart" data-api-key="ODA3NTlmNDUtYjRjNy00MzE2LTgzNmYtYjUwY2Y4MWFkOWRlNjM3MTQwMjI1MDM0NDU2MjEx"></div></body></html>